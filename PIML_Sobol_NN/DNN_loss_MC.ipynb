{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, Adadelta, Adagrad, Adam, Nadam, SGD\n",
    "from keras.callbacks import EarlyStopping, TerminateOnNaN\n",
    "from keras import backend as K\n",
    "from keras.losses import mean_squared_error\n",
    "import tensorflow as tf\n",
    "\n",
    "# Normalize the data.\n",
    "from sklearn import preprocessing\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "import random\n",
    "\n",
    "def pass_arg(nsim, tr_size, dropoutrate):\n",
    "    print(\"Tr_size:\", tr_size)\n",
    "    def fix_seeds(seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "        sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    #     K.set_session(sess)\n",
    "        tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "    ss = 1\n",
    "    fix_seeds(ss)\n",
    "\n",
    "    # MC dropout\n",
    "    class MCDropout(Dropout):\n",
    "        def call(self, inputs, training=None):\n",
    "            return super(MCDropout, self).call(inputs, training=True)\n",
    "\n",
    "    # import pickle\n",
    "\n",
    "    # def save_obj(obj, name):\n",
    "    #     with open(name, 'wb') as f:\n",
    "    #         pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "    # Compute the RMSE given the ground truth (y_true) and the predictions(y_pred)\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "            return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "\n",
    "    # Making sure final porosity is less than initial\n",
    "    def poros(poroi, porof):\n",
    "        return K.relu(tf.negative(porof)) + K.relu(porof-poroi)\n",
    "\n",
    "    def phy_loss_mean(params):\n",
    "        # useful for cross-checking training\n",
    "        loss1, lam1 = params\n",
    "        def loss(y_true,y_pred):\n",
    "            return lam1*K.mean(K.relu(loss1))\n",
    "        return loss\n",
    "\n",
    "    #function to calculate the combined loss = sum of rmse and phy based loss\n",
    "    def combined_loss(params):\n",
    "        loss1, lam1 = params\n",
    "        def loss(y_true,y_pred):\n",
    "            return mean_squared_error(y_true, y_pred) + lam1 * K.mean(K.relu(loss1))\n",
    "        return loss\n",
    "\n",
    "    def PGNN_train_test(optimizer_name, optimizer_val, drop_frac, use_YPhy, iteration, n_layers, n_nodes, tr_size, lamda, reg, samp):\n",
    "\n",
    "    #     fix_seeds(ss)\n",
    "\n",
    "        # Hyper-parameters of the training process\n",
    "    #     batch_size = tr_size\n",
    "        batch_size = 10\n",
    "        num_epochs = 300\n",
    "        val_frac = 0.25\n",
    "        patience_val = 80\n",
    "\n",
    "        # Initializing results filename\n",
    "        exp_name = \"DNN_loss\" + optimizer_name + '_drop' + str(drop_frac) + '_usePhy' + str(use_YPhy) +  '_nL' + str(n_layers) + '_nN' + str(n_nodes) + '_trsize' + str(tr_size) + '_lamda' + str(lamda) + '_iter' + str(iteration)\n",
    "        exp_name = exp_name.replace('.','pt')\n",
    "        results_dir = '../results/'\n",
    "        model_name = results_dir + exp_name + '_model.h5' # storing the trained model\n",
    "\n",
    "        if reg==True and samp==25:\n",
    "            results_name = results_dir + exp_name + '_results_25_regularizer.dat' # storing the results of the model\n",
    "        elif reg==False and samp==25:\n",
    "            results_name = results_dir + exp_name + '_results_25.dat' # storing the results of the model\n",
    "        elif reg==True and samp==1519:\n",
    "            results_name = results_dir + exp_name + '_results_1519_regularizer.dat' # storing the results of the model\n",
    "        elif reg==False and samp==1519:\n",
    "            results_name = results_dir + exp_name + '_results_1519.dat' # storing the results of the model\n",
    "\n",
    "        # Load labeled data\n",
    "        data = np.loadtxt('../data/labeled_data.dat')\n",
    "    #     data = np.loadtxt('../data/labeled_data_BK_constw_unique.dat')\n",
    "    #     data = np.loadtxt('../data/labeled_data_BK_constw_v2.dat')\n",
    "        x_labeled = data[:, :2] # -2 because we do not need porosity predictions\n",
    "        y_labeled = data[:, -2:-1] # dimensionless bond length and porosity measurements\n",
    "        if samp==25:\n",
    "            data = np.loadtxt('../data/unlabeled_data_BK_constw_v2_25.dat')\n",
    "            x_unlabeled = data[:, :]\n",
    "        elif samp==1519:\n",
    "            data = np.loadtxt('../data/unlabeled_data_BK_constw_v2_1525.dat')\n",
    "            x_unlabeled = data[:, :]\n",
    "\n",
    "        x_unlabeled1 = x_unlabeled[:1303, :]\n",
    "        x_unlabeled2 = x_unlabeled[-6:, :]\n",
    "        x_unlabeled = np.vstack((x_unlabeled1,x_unlabeled2))\n",
    "\n",
    "        # initial porosity\n",
    "        init_poro = x_unlabeled[:, -1]\n",
    "        x_unlabeled = x_unlabeled[:, :2]\n",
    "\n",
    "    #     data = np.loadtxt('../data/unlabeled_data_BK_constw_v2_1519.dat')\n",
    "    #     x_unlabeled = data[:1303, :] # 1303 last regular sample: 260, 46\n",
    "    #     x_unlabeled_non = x_unlabeled\n",
    "\n",
    "\n",
    "\n",
    "        # normalize dataset with MinMaxScaler\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "    #     scaler = preprocessing.StandardScaler()\n",
    "        x_labeled = scaler.fit_transform(x_labeled)\n",
    "        # y_labeled = scaler.fit_transform(y_labeled)\n",
    "        x_unlabeled = scaler.fit_transform(x_unlabeled)\n",
    "\n",
    "    #     # initial porosity & physics outputs are removed\n",
    "    #     x_unlabeled = x_unlabeled[:, :-3]\n",
    "\n",
    "        # train and test data\n",
    "        trainX, trainY = x_labeled[:tr_size,:], y_labeled[:tr_size]\n",
    "    #     testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]   \n",
    "        testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]\n",
    "\n",
    "        if use_YPhy == 0:\n",
    "            # Removing the last column from x_unlabeled (corresponding to Y_PHY)\n",
    "            x_unlabeled = x_unlabeled[:,:-1]\n",
    "\n",
    "        # Creating the model\n",
    "        model = Sequential()\n",
    "        for layer in np.arange(n_layers):\n",
    "            if layer == 0:\n",
    "                model.add(Dense(n_nodes, activation='relu', input_shape=(np.shape(trainX)[1],)))\n",
    "            else:\n",
    "                if reg:\n",
    "                    model.add(Dense(n_nodes, activation='relu', kernel_regularizer=l1_l2(l1=.001, l2=.001)))\n",
    "                else:\n",
    "                    model.add(Dense(n_nodes, activation='relu'))\n",
    "            # model.add(Dropout(rate=drop_frac))\n",
    "            model.add(MCDropout(rate=drop_frac))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "\n",
    "        # physics-based regularization\n",
    "        uinp_sc = K.constant(value=x_unlabeled) # unlabeled input data\n",
    "        lam1 = K.constant(value=lamda) # regularization hyper-parameter\n",
    "        predictions = model(uinp_sc) # model output at depth i\n",
    "        phyloss2 = poros(init_poro, predictions) # physics loss 1\n",
    "        totloss = combined_loss([phyloss2, lam1])\n",
    "        phyloss = phy_loss_mean([phyloss2, lam1])\n",
    "\t\t\n",
    "        model.compile(loss=totloss,\n",
    "                      optimizer=optimizer_val,\n",
    "                      metrics=[phyloss, root_mean_squared_error])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience_val, verbose=1)\n",
    "\n",
    "    #     print('Running...' + optimizer_name)\n",
    "        history = model.fit(trainX, trainY,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=num_epochs,\n",
    "                            verbose=0,\n",
    "                            validation_split=val_frac, callbacks=[early_stopping, TerminateOnNaN()])\n",
    "    \n",
    "        test_score = model.evaluate(testX, testY, verbose=1)\n",
    "        print(test_score)\n",
    "\n",
    "        test_scores = []\n",
    "        for i in range(int(nsim)):\n",
    "#             print(\"simulation num:\",i)\n",
    "#             predictions = model.predict(testX)\n",
    "#             samples.append(predictions)\n",
    "            test_score = model.evaluate(testX, testY, verbose=0)\n",
    "            test_scores.append(test_score[2])\n",
    "        return np.array(test_scores)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Main Function\n",
    "    if __name__ == '__main__':\n",
    "\n",
    "        fix_seeds(1)\n",
    "\n",
    "        # List of optimizers to choose from    \n",
    "        optimizer_names = ['Adagrad', 'Adadelta', 'Adam', 'Nadam', 'RMSprop', 'SGD', 'NSGD']\n",
    "        optimizer_vals = [Adagrad(clipnorm=1), Adadelta(clipnorm=1), Adam(clipnorm=1), Nadam(clipnorm=1), RMSprop(clipnorm=1), SGD(clipnorm=1.), SGD(clipnorm=1, nesterov=True)]\n",
    "\n",
    "        # selecting the optimizer\n",
    "        optimizer_num = 1\n",
    "        optimizer_name = optimizer_names[optimizer_num]\n",
    "        optimizer_val = optimizer_vals[optimizer_num]\n",
    "\n",
    "        # Selecting Other Hyper-parameters\n",
    "        drop_frac = dropoutrate # Fraction of nodes to be dropped out\n",
    "        use_YPhy = 1 # Whether YPhy is used as another feature in the NN model or not\n",
    "        n_layers = 2 # Number of hidden layers\n",
    "        n_nodes = 5 # Number of nodes per hidden layer\n",
    "\n",
    "        #set lamda\n",
    "        lamda = 0.01 # Physics-based regularization constant  \n",
    "\n",
    "#         # Iterating over different training fractions and splitting indices for train-test splits\n",
    "#         trsize_range = [4,6,8,10,20]\n",
    "\n",
    "#         #default training size = 5000\n",
    "#         tr_size = trsize_range[4]\n",
    "        \n",
    "        tr_size = int(tr_size)\n",
    "\n",
    "        # use regularizer\n",
    "        reg = True\n",
    "\n",
    "        # sample size used\n",
    "        samp = 1519\n",
    "    #     samp = 25\n",
    "\n",
    "        # total number of runs\n",
    "        iter_range = np.arange(1)\n",
    "        testrmse=[]\n",
    "        # iterating through all possible params\n",
    "        for iteration in iter_range:\n",
    "#             results, result_file, pred, obs, rmse, obs_train = PGNN_train_test(optimizer_name, optimizer_val, drop_frac, use_YPhy, \n",
    "#                             iteration, n_layers, n_nodes, tr_size, lamda, reg, samp)\n",
    "#             testrmse.append(rmse)\n",
    "            pred = PGNN_train_test(optimizer_name, optimizer_val, drop_frac, use_YPhy, \n",
    "                            iteration, n_layers, n_nodes, tr_size, lamda, reg, samp)\n",
    "            \n",
    "\n",
    "    return np.squeeze(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005\n",
      "Tr_size: 30\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "[0.0008603056194260716, 2.2439391500483907e-07, 0.011477804742753506]\n",
      "0.01\n",
      "Tr_size: 30\n",
      "9/9 [==============================] - 0s 996us/step\n",
      "[0.0009400751441717148, 9.857116083367146e-07, 0.012173284776508808]\n",
      "0.02\n",
      "Tr_size: 30\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "[0.0008915316429920495, 5.434878858068259e-07, 0.013674764893949032]\n",
      "0.05\n",
      "Tr_size: 30\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "[0.0009491285891272128, 2.781316155164859e-08, 0.017200473695993423]\n",
      "0.1\n",
      "Tr_size: 30\n",
      "9/9 [==============================] - 0s 997us/step\n",
      "[0.0008630902739241719, 0.0, 0.019408006221055984]\n",
      "0.15\n",
      "Tr_size: 30\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "[0.0008798272465355694, 0.0, 0.02063882350921631]\n",
      "0.2\n",
      "Tr_size: 30\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "[0.0007985556731000543, 0.0, 0.020240701735019684]\n",
      "0.25\n",
      "Tr_size: 30\n",
      "9/9 [==============================] - 0s 941us/step\n",
      "[0.0008417761418968439, 0.0, 0.019106492400169373]\n",
      "0.3\n",
      "Tr_size: 30\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "[0.0008735223091207445, 0.0, 0.01862216368317604]\n",
      "0.5\n",
      "Tr_size: 30\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "[0.0017722140764817595, 1.1736038686649408e-05, 0.020278707146644592]\n"
     ]
    }
   ],
   "source": [
    "mean_rmses=[]\n",
    "std_rmses=[]\n",
    "for ii in ([.005,.01,.02,.05,.1,.15,.2,.25,.3,0.5]):\n",
    "    print(ii)\n",
    "    test_rmse = pass_arg(50, 30, ii)\n",
    "    mean_rmse = np.mean(test_rmse)\n",
    "    std_rmse = np.std(test_rmse)\n",
    "    mean_rmses.append(mean_rmse)\n",
    "    std_rmses.append(std_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.011755541395395995,\n",
       " 0.012724407650530338,\n",
       " 0.013105153050273656,\n",
       " 0.013390020728111266,\n",
       " 0.019391486309468745,\n",
       " 0.019796954058110713,\n",
       " 0.019507023394107818,\n",
       " 0.019551680684089662,\n",
       " 0.01884949453175068,\n",
       " 0.02036983359605074]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0014014646467065755,\n",
       " 0.0015668682968109182,\n",
       " 0.0020780043839842,\n",
       " 0.002428422070285528,\n",
       " 0.0014661866938951835,\n",
       " 0.001362591369341849,\n",
       " 0.0011978184337197064,\n",
       " 0.0011109530050942684,\n",
       " 0.0005887063283113887,\n",
       " 0.00353718129746394]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "save_obj(mean_rmses, \"../mean_rmse_dnn_loss_MC.dat\")\n",
    "save_obj(std_rmses, \"../std_rmse_dnn_loss_MC.dat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05279518, 0.03283933, 0.0537447 , 0.03974609, 0.03457924,\n",
       "       0.03742982, 0.04465229, 0.05489689, 0.0322278 , 0.04860931,\n",
       "       0.05708254, 0.04503478, 0.06214603, 0.04120987, 0.04285658,\n",
       "       0.03374553, 0.04266082, 0.04460517, 0.04174339, 0.0369988 ,\n",
       "       0.06102522, 0.04720526, 0.03547252, 0.04057915, 0.06028698,\n",
       "       0.04594869, 0.03679036, 0.04504748, 0.05434823, 0.05183214,\n",
       "       0.04711368, 0.03764421, 0.03771775, 0.05239221, 0.04771913,\n",
       "       0.04199286, 0.05519148, 0.04541095, 0.06952468, 0.04033285,\n",
       "       0.04858945, 0.03479638, 0.03056115, 0.03366404, 0.05692469,\n",
       "       0.05566785, 0.0399124 , 0.04820377, 0.04281671, 0.03658003,\n",
       "       0.05378899, 0.04206469, 0.04401018, 0.03655737, 0.06820614,\n",
       "       0.05063169, 0.04610386, 0.05112227, 0.0458006 , 0.04457092,\n",
       "       0.03266823, 0.04351811, 0.03674877, 0.03626641, 0.06036571,\n",
       "       0.04281345, 0.04486926, 0.05853532, 0.0412476 , 0.03061781,\n",
       "       0.03981644, 0.04545145, 0.04714443, 0.0347507 , 0.0435339 ,\n",
       "       0.0563973 , 0.0462813 , 0.03278266, 0.03832164, 0.04817164,\n",
       "       0.04458795, 0.03405588, 0.04257753, 0.04751948, 0.03065004,\n",
       "       0.03623127, 0.02619934, 0.03702716, 0.02890221, 0.06691784,\n",
       "       0.04177232, 0.04288093, 0.04231226, 0.03492444, 0.0536108 ,\n",
       "       0.03288045, 0.04017974, 0.05028878, 0.02899871, 0.04921668,\n",
       "       0.0544173 , 0.06278119, 0.03588118, 0.05201441, 0.04280652,\n",
       "       0.06454953, 0.04821771, 0.04457631, 0.05032665, 0.04444049,\n",
       "       0.04416846, 0.03295628, 0.06199717, 0.06398957, 0.03572344,\n",
       "       0.04308823, 0.03726914, 0.03696414, 0.03961926, 0.05957818,\n",
       "       0.04563537, 0.05378069, 0.01528705, 0.04214678, 0.0434939 ,\n",
       "       0.05372976, 0.05624967, 0.0261834 , 0.03281919, 0.04216101,\n",
       "       0.03712658, 0.06280787, 0.07287747, 0.06073531, 0.0448016 ,\n",
       "       0.03773594, 0.03362345, 0.04283446, 0.03972682, 0.05943469,\n",
       "       0.04580568, 0.03576661, 0.04975818, 0.04071469, 0.0422328 ,\n",
       "       0.05092936, 0.05575986, 0.05577863, 0.04989756, 0.02099567,\n",
       "       0.03364211, 0.04367697, 0.0414076 , 0.04126591, 0.04021514,\n",
       "       0.06240829, 0.06253503, 0.05431565, 0.04770659, 0.05804824,\n",
       "       0.05336291, 0.03884013, 0.04068544, 0.04030986, 0.0343623 ,\n",
       "       0.05537937, 0.03943855, 0.0437831 , 0.03251111, 0.04641122,\n",
       "       0.04650781, 0.06336901, 0.03897109, 0.0525963 , 0.04402711,\n",
       "       0.04272104, 0.05419434, 0.04758772, 0.01059438, 0.0397981 ,\n",
       "       0.04009419, 0.03988982, 0.04115646, 0.06074953, 0.03537698,\n",
       "       0.05409338, 0.03621498, 0.04268951, 0.02742414, 0.06314985,\n",
       "       0.04056774, 0.03371869, 0.05680367, 0.04027347, 0.0423556 ,\n",
       "       0.04359945, 0.03329067, 0.06083277, 0.03186129, 0.04327793,\n",
       "       0.04347276, 0.06906273, 0.06251105, 0.03914563, 0.04627613,\n",
       "       0.04470402, 0.04327693, 0.03847009, 0.03701867, 0.03641728,\n",
       "       0.04423818, 0.04275543, 0.04751433, 0.01598257, 0.0361139 ,\n",
       "       0.06344987, 0.04440669, 0.0590071 , 0.03750511, 0.04802949,\n",
       "       0.04319545, 0.03292385, 0.05683002, 0.04306199, 0.07553553,\n",
       "       0.04664924, 0.02615323, 0.04164355, 0.04017032, 0.04845427,\n",
       "       0.01524437, 0.04560505, 0.04755528, 0.04621894, 0.035671  ,\n",
       "       0.05214545, 0.04782487, 0.03974194, 0.04686124, 0.03011703,\n",
       "       0.04219105, 0.00512406, 0.04616782, 0.04027798, 0.03635201,\n",
       "       0.04006006, 0.04698586, 0.04916538, 0.04942863, 0.04301912,\n",
       "       0.01242082, 0.04586679, 0.04452428, 0.05462679, 0.04546325,\n",
       "       0.04317021, 0.04421661, 0.04328338, 0.03609335, 0.01322834,\n",
       "       0.06298792, 0.05936319, 0.05422922, 0.02276836, 0.04393116,\n",
       "       0.03929581, 0.03899974, 0.04848171, 0.06487098, 0.03819473,\n",
       "       0.04477736, 0.05434734, 0.02750718, 0.04081893, 0.0329066 ,\n",
       "       0.06147749, 0.0481715 , 0.04027211, 0.03971713, 0.02977636,\n",
       "       0.04933265, 0.04041059, 0.00676761, 0.03140671, 0.06092018,\n",
       "       0.03260735, 0.04498848, 0.04263134, 0.03973242, 0.06383008,\n",
       "       0.04699862, 0.04485212, 0.04147894, 0.0351189 , 0.03459029,\n",
       "       0.03605019, 0.05538899, 0.03960966, 0.06022389, 0.06221807,\n",
       "       0.05112661, 0.04046448, 0.03787848, 0.03318763, 0.04231551,\n",
       "       0.03408996, 0.04793409, 0.04374145, 0.0370138 , 0.03402049,\n",
       "       0.04871874, 0.03599902, 0.04268842, 0.031595  , 0.06650469,\n",
       "       0.04995964, 0.04491393, 0.04104722, 0.03849293, 0.03155924,\n",
       "       0.03873638, 0.03353048, 0.06421298, 0.05130164, 0.02741468,\n",
       "       0.04650908, 0.03832591, 0.03874923, 0.04615496, 0.04358387,\n",
       "       0.04240591, 0.03957541, 0.04253527, 0.05690234, 0.04045956,\n",
       "       0.04065699, 0.0403286 , 0.03424413, 0.03766483, 0.03705497,\n",
       "       0.05626259, 0.04910798, 0.01901662, 0.04778707, 0.04089962,\n",
       "       0.04454983, 0.03830842, 0.04658324, 0.03315055, 0.04952156,\n",
       "       0.03471559, 0.0379632 , 0.05008176, 0.04855555, 0.03418293,\n",
       "       0.04798516, 0.0462708 , 0.03557218, 0.05841213, 0.06195722,\n",
       "       0.06544988, 0.05256044, 0.03281475, 0.05309714, 0.03220435,\n",
       "       0.03524032, 0.05978776, 0.01114547, 0.0403653 , 0.01870047,\n",
       "       0.04483945, 0.0600082 , 0.06017193, 0.04687081, 0.03984557,\n",
       "       0.03926663, 0.03725995, 0.04146082, 0.03977628, 0.04412157,\n",
       "       0.05626578, 0.04534624, 0.05506472, 0.04392665, 0.03638872,\n",
       "       0.0433717 , 0.02737629, 0.04461788, 0.03761269, 0.05160951,\n",
       "       0.0459027 , 0.039365  , 0.04209209, 0.03908138, 0.0412695 ,\n",
       "       0.03313401, 0.03775529, 0.04635129, 0.05896781, 0.05973145,\n",
       "       0.04243883, 0.05048671, 0.01785599, 0.06609472, 0.05757917,\n",
       "       0.0508724 , 0.03826016, 0.0431794 , 0.03648675, 0.04202123,\n",
       "       0.04486038, 0.04739558, 0.04292233, 0.01210807, 0.06012275,\n",
       "       0.04378906, 0.04868803, 0.06150983, 0.04017437, 0.04108255,\n",
       "       0.03956662, 0.03133844, 0.02841418, 0.05882077, 0.03911506,\n",
       "       0.01570129, 0.05334627, 0.03734942, 0.05917521, 0.04017762,\n",
       "       0.03722735, 0.0487099 , 0.04183091, 0.03963765, 0.0420479 ,\n",
       "       0.04516809, 0.04795982, 0.04286966, 0.05817571, 0.04632949,\n",
       "       0.04989766, 0.06223575, 0.04001621, 0.03295584, 0.03740323,\n",
       "       0.03808199, 0.05160694, 0.0491569 , 0.04337936, 0.048905  ,\n",
       "       0.04065185, 0.04902779, 0.04377836, 0.04476486, 0.03879192,\n",
       "       0.03586128, 0.04934675, 0.0444076 , 0.03212309, 0.04081418,\n",
       "       0.03653471, 0.04613219, 0.05204809, 0.04459298, 0.02870271,\n",
       "       0.04963794, 0.03938055, 0.03699584, 0.03089659, 0.03601137,\n",
       "       0.05182813, 0.04269172, 0.05679964, 0.04807729, 0.03759655,\n",
       "       0.03311763, 0.04822993, 0.06901485, 0.03831674, 0.05005316,\n",
       "       0.03735617, 0.03801285, 0.0400143 , 0.03984103, 0.03762729,\n",
       "       0.04846573, 0.05011744, 0.04316266, 0.04659165, 0.03932131,\n",
       "       0.04832724, 0.05914959, 0.04246675, 0.05139846, 0.0508204 ,\n",
       "       0.04245749, 0.03920192, 0.04374741, 0.04440278, 0.07043009,\n",
       "       0.04326815, 0.04529073, 0.03765638, 0.0441573 , 0.03938232,\n",
       "       0.01724157, 0.03735636, 0.03937669, 0.04155016, 0.02828331,\n",
       "       0.04484601, 0.05684888, 0.02101439, 0.04529534, 0.06719849,\n",
       "       0.00785868, 0.04776655, 0.04360517, 0.03004374, 0.03659462,\n",
       "       0.04283959, 0.03819267, 0.03446211, 0.04613439, 0.05960507,\n",
       "       0.04953236, 0.04579959, 0.05572011, 0.08103574, 0.04239484,\n",
       "       0.0478289 , 0.03910539, 0.03254723, 0.051312  , 0.04611661,\n",
       "       0.06050571, 0.0377232 , 0.03930637, 0.03020538, 0.05295208,\n",
       "       0.04577718, 0.04675239, 0.05126189, 0.0391922 , 0.04722361,\n",
       "       0.05681045, 0.0632087 , 0.04919884, 0.05349575, 0.03658286,\n",
       "       0.03607224, 0.0319424 , 0.03775658, 0.05663596, 0.05217598,\n",
       "       0.05512119, 0.04675249, 0.05234021, 0.02559543, 0.03197379,\n",
       "       0.03413805, 0.05056763, 0.03869798, 0.03434749, 0.04637082,\n",
       "       0.04871831, 0.04047071, 0.03855848, 0.03460875, 0.03146775,\n",
       "       0.05392698, 0.03979561, 0.03449735, 0.03743624, 0.0364854 ,\n",
       "       0.0420774 , 0.04710102, 0.03939492, 0.07279739, 0.03991805,\n",
       "       0.06620637, 0.03318482, 0.03911456, 0.06113617, 0.03583329,\n",
       "       0.05212792, 0.00909505, 0.01897203, 0.03747234, 0.04254362,\n",
       "       0.05099336, 0.03262505, 0.03730283, 0.03660676, 0.06245321,\n",
       "       0.04067576, 0.0513083 , 0.04261226, 0.05343415, 0.03869383,\n",
       "       0.04666749, 0.0410488 , 0.04059319, 0.05083061, 0.05239444,\n",
       "       0.03725563, 0.0416708 , 0.02188714, 0.04207311, 0.04458124,\n",
       "       0.03172497, 0.03064107, 0.04012884, 0.0435473 , 0.02927088,\n",
       "       0.03498768, 0.03759187, 0.05255836, 0.02694392, 0.02856888,\n",
       "       0.04275145, 0.04285615, 0.03719417, 0.01546929, 0.04376385,\n",
       "       0.04912224, 0.05249702, 0.030741  , 0.04435243, 0.03981445,\n",
       "       0.0488332 , 0.04108545, 0.04184182, 0.05161147, 0.03938922,\n",
       "       0.04480792, 0.04779361, 0.04829352, 0.03413004, 0.04666568,\n",
       "       0.05765747, 0.04353295, 0.03942567, 0.04229992, 0.03991237,\n",
       "       0.04353385, 0.04595402, 0.05245533, 0.06009521, 0.02747586,\n",
       "       0.02660296, 0.04335519, 0.03618021, 0.04261728, 0.07055166,\n",
       "       0.04877911, 0.05935969, 0.06776973, 0.03014638, 0.06124297,\n",
       "       0.03810485, 0.05287892, 0.05246849, 0.03665496, 0.0514911 ,\n",
       "       0.04859405, 0.0357625 , 0.05218095, 0.04716004, 0.04407423,\n",
       "       0.04240528, 0.0384133 , 0.0276382 , 0.03420068, 0.04132637,\n",
       "       0.04130692, 0.06216715, 0.04646367, 0.03099644, 0.04031827,\n",
       "       0.0433968 , 0.04563094, 0.04900436, 0.04065592, 0.04600475,\n",
       "       0.03992416, 0.04139792, 0.04306282, 0.0426866 , 0.05377676,\n",
       "       0.04957552, 0.04282311, 0.0593105 , 0.04953005, 0.04462589,\n",
       "       0.05209666, 0.04784837, 0.04749036, 0.04169641, 0.03935406,\n",
       "       0.03148273, 0.0292948 , 0.03290548, 0.03081997, 0.04000999,\n",
       "       0.03713587, 0.04765454, 0.03433583, 0.0277455 , 0.04272005,\n",
       "       0.03749274, 0.01784158, 0.04547046, 0.0454343 , 0.0381241 ,\n",
       "       0.04488645, 0.0302513 , 0.04575409, 0.04263332, 0.04567531,\n",
       "       0.07201058, 0.04110647, 0.04458228, 0.05615763, 0.03986882,\n",
       "       0.04677866, 0.03729148, 0.03993796, 0.03930625, 0.05375721,\n",
       "       0.04512943, 0.0502216 , 0.0370604 , 0.05051342, 0.043832  ,\n",
       "       0.04091546, 0.0488181 , 0.04731522, 0.05351252, 0.04101642,\n",
       "       0.06737964, 0.03662672, 0.06605085, 0.03913338, 0.04276843,\n",
       "       0.03077391, 0.02763186, 0.04508662, 0.04627687, 0.04350668,\n",
       "       0.03212296, 0.03580047, 0.04155901, 0.06370017, 0.06135947,\n",
       "       0.05175262, 0.03688934, 0.04580327, 0.04490193, 0.0435379 ,\n",
       "       0.04474186, 0.03925879, 0.03720258, 0.06179819, 0.03544012,\n",
       "       0.05358851, 0.04013669, 0.0515479 , 0.04348711, 0.05909012,\n",
       "       0.05236348, 0.04427191, 0.03218457, 0.04462882, 0.04067846,\n",
       "       0.04115688, 0.0473131 , 0.04852907, 0.05232359, 0.04305536,\n",
       "       0.04975556, 0.0515929 , 0.05626136, 0.03924873, 0.02953034,\n",
       "       0.04186771, 0.06266486, 0.0329607 , 0.0326166 , 0.04566088,\n",
       "       0.03255178, 0.05521293, 0.04981787, 0.01767337, 0.0505938 ,\n",
       "       0.0427512 , 0.0383315 , 0.04246178, 0.04451006, 0.04170784,\n",
       "       0.04483588, 0.05869438, 0.02208606, 0.04019042, 0.07423872,\n",
       "       0.03747218, 0.06630869, 0.04457509, 0.03722416, 0.04440615,\n",
       "       0.02937616, 0.01594665, 0.06227311, 0.05477206, 0.02566287,\n",
       "       0.04144973, 0.05330642, 0.04925497, 0.0083856 , 0.06281556,\n",
       "       0.026724  , 0.04038114, 0.05707906, 0.04809815, 0.0438485 ,\n",
       "       0.04062607, 0.05219135, 0.04211576, 0.03868686, 0.04587659,\n",
       "       0.05822401, 0.05003511, 0.05675057, 0.04219415, 0.05703018,\n",
       "       0.04310456, 0.04202121, 0.0387982 , 0.01830204, 0.07487979,\n",
       "       0.03625273, 0.04076526, 0.04786075, 0.03864377, 0.0316664 ,\n",
       "       0.04531031, 0.05207087, 0.04461835, 0.0414838 , 0.03798683,\n",
       "       0.04492886, 0.04319296, 0.01454918, 0.0674394 , 0.03927596,\n",
       "       0.04223438, 0.03697844, 0.04586577, 0.06195136, 0.0375639 ,\n",
       "       0.05623294, 0.05637901, 0.05116555, 0.04249465, 0.04175164,\n",
       "       0.04177836, 0.04401038, 0.05125599, 0.04363798, 0.04488043,\n",
       "       0.0354461 , 0.0502258 , 0.03325381, 0.06194891, 0.04361294,\n",
       "       0.04793674, 0.03304671, 0.04730592, 0.04986428, 0.04312812,\n",
       "       0.04228811, 0.04614924, 0.07385568, 0.04078123, 0.05538186,\n",
       "       0.0466128 , 0.05417765, 0.06136123, 0.05777773, 0.04320841,\n",
       "       0.0390791 , 0.03683809, 0.04249642, 0.03677712, 0.04103402,\n",
       "       0.03523289, 0.03799768, 0.03447225, 0.03944195, 0.04412178,\n",
       "       0.05269638, 0.06503177, 0.03954517, 0.06083582, 0.04179038,\n",
       "       0.03806328, 0.06386146, 0.04112491, 0.0315623 , 0.06591168,\n",
       "       0.03369149, 0.04154384, 0.04696532, 0.03912373, 0.0422404 ,\n",
       "       0.04908294, 0.04998066, 0.04156994, 0.03341593, 0.02907764,\n",
       "       0.06051515, 0.04634394, 0.05049663, 0.0365489 , 0.04277272,\n",
       "       0.0234484 , 0.05364086, 0.03429641, 0.01467825, 0.04606941,\n",
       "       0.02708082, 0.04125452, 0.06629837, 0.04426799, 0.04452875,\n",
       "       0.04056318, 0.05406957, 0.04711315, 0.03340158, 0.0308817 ,\n",
       "       0.05007011, 0.0412897 , 0.0628581 , 0.02801358, 0.04091408,\n",
       "       0.0463423 , 0.02876052, 0.03848444, 0.0427662 , 0.04813664,\n",
       "       0.03248874, 0.04402324, 0.06565332, 0.06107773, 0.04572601,\n",
       "       0.03911167, 0.02988927, 0.0439564 , 0.01421903, 0.04449343,\n",
       "       0.03843656, 0.03675603, 0.04277168, 0.03808434, 0.03302428,\n",
       "       0.04509344, 0.02819183, 0.04512571, 0.06677135, 0.0513861 ,\n",
       "       0.06597431, 0.03706232, 0.04766389, 0.05875278, 0.04985196,\n",
       "       0.06338655, 0.05396913, 0.04378147, 0.05825808, 0.05275422,\n",
       "       0.04775405, 0.06275615, 0.05154007, 0.04484907, 0.06624148,\n",
       "       0.01763078, 0.0381368 , 0.04106071, 0.05132953, 0.05279565,\n",
       "       0.05026697, 0.04257726, 0.04233714, 0.04048957, 0.02463468,\n",
       "       0.04597154, 0.04659113, 0.04622385, 0.05537616, 0.04542978],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_pred=np.mean(pred,axis=0)\n",
    "mc_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "save_obj(mc_pred, \"../pred_loss_MC_Xx.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
