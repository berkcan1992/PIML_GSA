{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, Adadelta, Adagrad, Adam, Nadam, SGD\n",
    "from keras.callbacks import EarlyStopping, TerminateOnNaN\n",
    "from keras import backend as K\n",
    "from keras.losses import mean_squared_error\n",
    "import tensorflow as tf\n",
    "\n",
    "# Normalize the data.\n",
    "from sklearn import preprocessing\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "import random\n",
    "\n",
    "def pass_arg(nsim, tr_size):\n",
    "    print(\"Tr_size:\", tr_size)\n",
    "    def fix_seeds(seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "        sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    #     K.set_session(sess)\n",
    "        tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "    ss = 1\n",
    "    fix_seeds(ss)\n",
    "\n",
    "    # MC dropout\n",
    "    class MCDropout(Dropout):\n",
    "        def call(self, inputs, training=None):\n",
    "            return super(MCDropout, self).call(inputs, training=True)\n",
    "\n",
    "\n",
    "    # import pickle\n",
    "\n",
    "    # def save_obj(obj, name):\n",
    "    #     with open(name, 'wb') as f:\n",
    "    #         pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "    # Compute the RMSE given the ground truth (y_true) and the predictions(y_pred)\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "            return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "    # Making sure dimensionless bond length is less than 1\n",
    "    def bond(bl):\n",
    "        return tf.add(K.relu(tf.negative(bl)), K.relu(bl-1.0))\n",
    "\n",
    "    # Making sure final porosity is less than initial\n",
    "    def poros(poroi, porof):\n",
    "        return K.relu(tf.negative(porof)) + K.relu(porof-poroi)\n",
    "\n",
    "    def strength1(bl, porof, nlayer=6):\n",
    "        sigma01, sigma02, C1s = 6, 31, 21\n",
    "        sigma_long = sigma01*(K.exp((1.0-porof)**(C1s*nlayer))-porof) + sigma02*(1.0-porof)\n",
    "        n = K.shape(sigma_long)[0]  \n",
    "        sorted_strength, sortedIndices = tf.math.top_k(sigma_long, n, True)\n",
    "        sorted_bl = K.gather(bl, sortedIndices)\n",
    "        sorted_porof = K.gather(porof, sortedIndices)\n",
    "        argg = tf.argsort(sorted_bl,axis=-1,direction='DESCENDING',stable=False,name=None)\n",
    "        sorted_bl_corr = K.gather(sorted_bl, argg)\n",
    "        return sorted_bl_corr-sorted_bl\n",
    "\n",
    "\n",
    "    def strength2(bl, porof, nlayer=6):\n",
    "        sigma01, sigma02, C1s = 6, 31, 21\n",
    "        sigma_long = sigma01*(K.exp((1.0-porof)**(C1s*nlayer))-porof) + sigma02*(1.0-porof)\n",
    "        n = K.shape(sigma_long)[0]  \n",
    "        sorted_strength, sortedIndices = tf.math.top_k(sigma_long, n, True)\n",
    "        sorted_bl = K.gather(bl, sortedIndices)\n",
    "        n = K.cast(n, tf.float32)\n",
    "        rel = K.relu(sorted_bl[1:]-sorted_bl[0:-1])\n",
    "        num_vio = K.cast(tf.math.count_nonzero(rel), tf.float32)\n",
    "        return num_vio/n\n",
    "\n",
    "\n",
    "    def phy_loss_mean(params):\n",
    "        # useful for cross-checking training\n",
    "        loss1, loss2, loss3, loss4, lam1, lam2, lam3, lam4 = params\n",
    "        def loss(y_true,y_pred):\n",
    "    #         return lam1*K.mean(K.relu(loss1)) + lam2*K.mean(K.relu(loss2)) + lam2*K.mean(K.relu(loss3))\n",
    "            return lam1*K.mean(K.relu(loss1)) + lam2*K.mean(K.relu(loss2)) + lam3*K.mean(K.relu(loss3)) + lam4*loss4\n",
    "        return loss\n",
    "\n",
    "    #function to calculate the combined loss = sum of rmse and phy based loss\n",
    "    def combined_loss(params):\n",
    "        loss1, loss2, loss3, loss4, lam1, lam2, lam3, lam4 = params\n",
    "        def loss(y_true,y_pred):\n",
    "    #         return mean_squared_error(y_true, y_pred) + lam1 * K.mean(K.relu(loss1)) + lam2 * K.mean(K.relu(loss2)) + lam2 * K.mean(K.relu(loss3))\n",
    "            return mean_squared_error(y_true, y_pred) + lam1 * K.mean(K.relu(loss1)) + lam2 * K.mean(K.relu(loss2)) + lam3*K.mean(K.relu(loss3)) + lam4 * loss4\n",
    "        return loss\n",
    "\n",
    "    def PGNN_train_test(optimizer_name, optimizer_val, drop_frac, use_YPhy, iteration, n_layers, n_nodes, tr_size, lamda, reg, samp):\n",
    "\n",
    "    #     fix_seeds(ss)\n",
    "\n",
    "        # Hyper-parameters of the training process\n",
    "    #     batch_size = tr_size\n",
    "        batch_size = 1\n",
    "        num_epochs = 50\n",
    "        val_frac = 0.2\n",
    "        patience_val = 50\n",
    "\n",
    "        # Initializing results filename\n",
    "        exp_name = optimizer_name + '_drop' + str(drop_frac) + '_usePhy' + str(use_YPhy) +  '_nL' + str(n_layers) + '_nN' + str(n_nodes) + '_trsize' + str(tr_size) + '_lamda' + str(lamda) + '_iter' + str(iteration)\n",
    "        exp_name = exp_name.replace('.','pt')\n",
    "        results_dir = '../results/'\n",
    "        model_name = results_dir + exp_name + '_model.h5' # storing the trained model\n",
    "\n",
    "        if reg==True and samp==25:\n",
    "            results_name = results_dir + exp_name + '_results_25_regularizer.dat' # storing the results of the model\n",
    "        elif reg==False and samp==25:\n",
    "            results_name = results_dir + exp_name + '_results_25.dat' # storing the results of the model\n",
    "        elif reg==True and samp==1519:\n",
    "            results_name = results_dir + exp_name + '_results_1519_regularizer.dat' # storing the results of the model\n",
    "        elif reg==False and samp==1519:\n",
    "            results_name = results_dir + exp_name + '_results_1519.dat' # storing the results of the model\n",
    "\n",
    "        # Load labeled data\n",
    "        data = np.loadtxt('../data/labeled_data.dat')\n",
    "    #     data = np.loadtxt('../data/labeled_data_BK_constw_unique.dat')\n",
    "    #     data = np.loadtxt('../data/labeled_data_BK_constw_v2.dat')\n",
    "#         x_labeled = data[:, :2] # -2 because we do not need porosity predictions\n",
    "        x_label = data[:, :-3] # -2 because we do not need porosity predictions\n",
    "        x_labeled = np.hstack((x_label[:,:2],x_label[:,-2:]))\n",
    "        y_labeled = data[:, -3:-1] # dimensionless bond length and porosity measurements\n",
    "        if samp==25:\n",
    "            data = np.loadtxt('../data/unlabeled_data_BK_constw_v2_25.dat')\n",
    "            x_unlabeled = data[:, :]\n",
    "        elif samp==1519:\n",
    "            data = np.loadtxt('../data/unlabeled_data_BK_constw_v2_1525.dat')\n",
    "            x_unlabeled = data[:, :]\n",
    "\n",
    "        x_unlabeled1 = x_unlabeled[:1303, :]\n",
    "        x_unlabeled2 = x_unlabeled[-6:, :]\n",
    "        x_unlabeled = np.vstack((x_unlabeled1,x_unlabeled2))\n",
    "\n",
    "        # initial porosity\n",
    "        init_poro = x_unlabeled[:, -1]\n",
    "        x_unlabeled = np.hstack((x_unlabeled[:,:2],x_unlabeled[:,-3:-1]))\n",
    "#         x_unlabeled = x_unlabeled[:, :2]\n",
    "\n",
    "\n",
    "        # normalize dataset with MinMaxScaler\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "    #     scaler = preprocessing.StandardScaler()\n",
    "        x_labeled = scaler.fit_transform(x_labeled)\n",
    "#         y_labeled = scaler.fit_transform(y_labeled)\n",
    "        x_unlabeled = scaler.fit_transform(x_unlabeled)\n",
    "\n",
    "    #     # initial porosity & physics outputs are removed\n",
    "    #     x_unlabeled = x_unlabeled[:, :-3]\n",
    "\n",
    "        # train and test data\n",
    "        trainX, trainY = x_labeled[:tr_size,:], y_labeled[:tr_size]\n",
    "    #     testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]   \n",
    "        testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]\n",
    "\n",
    "        if use_YPhy == 0:\n",
    "            # Removing the last column from x_unlabeled (corresponding to Y_PHY)\n",
    "            x_unlabeled = x_unlabeled[:,:-1]\n",
    "\n",
    "        # Creating the model\n",
    "        model = Sequential()\n",
    "        for layer in np.arange(n_layers):\n",
    "            if layer == 0:\n",
    "                model.add(Dense(n_nodes, activation='relu', input_shape=(np.shape(trainX)[1],)))\n",
    "            else:\n",
    "                if reg:\n",
    "                    model.add(Dense(n_nodes, activation='relu', kernel_regularizer=l1_l2(l1=.001, l2=.001)))\n",
    "                else:\n",
    "                    model.add(Dense(n_nodes, activation='relu'))\n",
    "            # model.add(Dropout(rate=drop_frac))\n",
    "            model.add(MCDropout(rate=drop_frac))\n",
    "        model.add(Dense(2, activation='linear'))\n",
    "\n",
    "        # physics-based regularization\n",
    "        uinp_sc = K.constant(value=x_unlabeled) # unlabeled input data\n",
    "        lam1 = K.constant(value=lamda[0]) # regularization hyper-parameter\n",
    "        lam2 = K.constant(value=lamda[1]) # regularization hyper-parameter\n",
    "        lam3 = K.constant(value=lamda[2]) # regularization hyper-parameter\n",
    "        lam4 = K.constant(value=lamda[3]) # regularization hyper-parameter\n",
    "        predictions = model(uinp_sc) # model output at depth i\n",
    "    #     porosity = K.relu(predictions[:,1])\n",
    "        phyloss1 = bond(predictions[:,0]) # physics loss 1\n",
    "    #     uinp = K.constant(value=x_unlabeled_non) # unlabeled input data\n",
    "        phyloss2 = poros(init_poro, predictions[:,1]) # physics loss 1\n",
    "        phyloss3 = strength1(predictions[:,0], predictions[:,1])\n",
    "        phyloss4 = strength2(predictions[:,0], predictions[:,1])\n",
    "        totloss = combined_loss([phyloss1, phyloss2, phyloss3, phyloss4, lam1, lam2, lam3, lam4])\n",
    "        phyloss = phy_loss_mean([phyloss1, phyloss2, phyloss3, phyloss4, lam1, lam2, lam3, lam4])\n",
    "\n",
    "\n",
    "        model.compile(loss=totloss,\n",
    "                      optimizer=optimizer_val,\n",
    "                      metrics=[phyloss, root_mean_squared_error])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience_val, verbose=1)\n",
    "\n",
    "    #     print('Running...' + optimizer_name)\n",
    "        history = model.fit(trainX, trainY,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=num_epochs,\n",
    "                            verbose=0,\n",
    "                            validation_split=val_frac, callbacks=[early_stopping, TerminateOnNaN()])\n",
    "    \n",
    "#     early_stopping = EarlyStopping(monitor='loss', patience=patience_val, verbose=1)\n",
    "#     history = model.fit(trainX, trainY,\n",
    "#                         batch_size=batch_size,\n",
    "#                         epochs=num_epochs,\n",
    "#                         verbose=1,\n",
    "#                         callbacks=[early_stopping, TerminateOnNaN()])\n",
    "\n",
    "#     test_score = model.evaluate(testX, testY, verbose=0)\n",
    "#     predictions = model.predict(x_labeled) # model output at depth i\n",
    "#     print(np.sort(predictions[:,0], axis=0))\n",
    "    \n",
    "#     predictions = model.predict(x_unlabeled) # model output at depth i\n",
    "#     print(np.sort(predictions[:,0], axis=0))\n",
    "    \n",
    "#     print('iter: ' + str(iteration) + ' useYPhy: ' + str(use_YPhy) + \n",
    "#           ' nL: ' + str(n_layers) + ' nN: ' + str(n_nodes) + \n",
    "#           ' lamda1: ' + str(lamda[0]) + ' lamda2: ' + str(lamda[1]) + ' trsize: ' + str(tr_size) + \n",
    "#           ' TestRMSE: ' + str(test_score[2]) + ' PhyLoss: ' + str(test_score[1]), ' TestLoss: ' + str(test_score[0]), \"\\n\")\n",
    "\n",
    "# #     print('iter: ' + str(iteration) + ' TestRMSE: ' + str(test_score[2]) + ' PhyLoss: ' + str(test_score[1]), \"\\n\")\n",
    "\n",
    "    \n",
    "# #     model.save(model_name)\n",
    "    \n",
    "#     # save results\n",
    "#     results = {'train_loss_1':history.history['loss_1'], \n",
    "#                                 'val_loss_1':history.history['val_loss_1'], \n",
    "#                                 'train_rmse':history.history['root_mean_squared_error'], \n",
    "#                                 'val_rmse':history.history['val_root_mean_squared_error'],\n",
    "#                                 'test_rmse':test_score[2],\n",
    "#                                 'PhyLoss':test_score[1]}\n",
    "\n",
    "#     results = {'train_loss_1':history.history['loss_1'], \n",
    "#                                 'train_rmse':history.history['root_mean_squared_error'], \n",
    "#                                 'test_rmse':test_score[2],\n",
    "#                                 'PhyLoss':test_score[1]}\n",
    "\n",
    "    \n",
    "\n",
    "#     save_obj(results, results_name)\n",
    "\n",
    "#     predictions = model.predict(testX)\n",
    "#     return results, results_name, predictions, testY, test_score[2], trainY\n",
    "    \n",
    "        test_score = model.evaluate(testX, testY, verbose=1)\n",
    "        print(test_score)\n",
    "\n",
    "        Xx = np.random.uniform(0,1,(1000,2))\n",
    "        xx1 = np.ones((1000,2))\n",
    "        Xx = np.hstack((Xx,xx1))\n",
    "        \n",
    "        samples = []\n",
    "        for i in range(int(nsim)):\n",
    "#             print(\"simulation num:\",i)\n",
    "            predictions = model.predict(Xx)\n",
    "            predictions = predictions[:,1]\n",
    "            samples.append(predictions)\n",
    "        return np.array(samples)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Main Function\n",
    "    if __name__ == '__main__':\n",
    "\n",
    "        fix_seeds(1)\n",
    "\n",
    "        # List of optimizers to choose from    \n",
    "        optimizer_names = ['Adagrad', 'Adadelta', 'Adam', 'Nadam', 'RMSprop', 'SGD', 'NSGD']\n",
    "        optimizer_vals = [Adagrad(clipnorm=1), Adadelta(clipnorm=1), Adam(clipnorm=1), Nadam(clipnorm=1), RMSprop(clipnorm=1), SGD(clipnorm=1.), SGD(clipnorm=1, nesterov=True)]\n",
    "\n",
    "        # selecting the optimizer\n",
    "        optimizer_num = 1\n",
    "        optimizer_name = optimizer_names[optimizer_num]\n",
    "        optimizer_val = optimizer_vals[optimizer_num]\n",
    "\n",
    "        # Selecting Other Hyper-parameters\n",
    "        drop_frac = 0.1 # Fraction of nodes to be dropped out\n",
    "        use_YPhy = 1 # Whether YPhy is used as another feature in the NN model or not\n",
    "        n_layers = 2 # Number of hidden layers\n",
    "        n_nodes = 5 # Number of nodes per hidden layer\n",
    "\n",
    "        #set lamda\n",
    "        lamda = [0.3, 0.15, 0.008, 0] # Physics-based regularization constant  \n",
    "\n",
    "#         # Iterating over different training fractions and splitting indices for train-test splits\n",
    "#         trsize_range = [4,6,8,10,20]\n",
    "\n",
    "#         #default training size = 5000\n",
    "#         tr_size = trsize_range[4]\n",
    "        \n",
    "        tr_size = int(tr_size)\n",
    "\n",
    "        # use regularizer\n",
    "        reg = True\n",
    "\n",
    "        # sample size used\n",
    "        samp = 1519\n",
    "    #     samp = 25\n",
    "\n",
    "        # total number of runs\n",
    "        iter_range = np.arange(1)\n",
    "        testrmse=[]\n",
    "        # iterating through all possible params\n",
    "        for iteration in iter_range:\n",
    "#             results, result_file, pred, obs, rmse, obs_train = PGNN_train_test(optimizer_name, optimizer_val, drop_frac, use_YPhy, \n",
    "#                             iteration, n_layers, n_nodes, tr_size, lamda, reg, samp)\n",
    "#             testrmse.append(rmse)\n",
    "            pred = PGNN_train_test(optimizer_name, optimizer_val, drop_frac, use_YPhy, \n",
    "                            iteration, n_layers, n_nodes, tr_size, lamda, reg, samp)\n",
    "            \n",
    "\n",
    "    return np.squeeze(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr_size: 20\n",
      "19/19 [==============================] - 0s 53us/step\n",
      "[0.015868138521909714, 0.000551834877114743, 0.07156015932559967]\n"
     ]
    }
   ],
   "source": [
    "pred = pass_arg(50, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.42983960e-02,  3.10231782e-02,  2.30593011e-02,  1.03699237e-01,\n",
       "        8.02772492e-02, -2.38511078e-02,  1.52287617e-01,  1.21249348e-01,\n",
       "        1.91829316e-02,  3.51462997e-02,  7.02235997e-02,  1.63126737e-01,\n",
       "        1.46971390e-01,  9.72257406e-02,  1.69197172e-01,  3.29563878e-02,\n",
       "        1.38348728e-01,  1.90577835e-01,  1.35779217e-01,  1.71404623e-04,\n",
       "        7.47894496e-02,  1.11002102e-01, -2.93192570e-03,  1.06759921e-01,\n",
       "        3.57108265e-02,  2.97774319e-02,  3.12048346e-02,  1.86630443e-01,\n",
       "        1.67250514e-01,  1.40376166e-02,  1.29431576e-01,  6.60574213e-02,\n",
       "        7.16345087e-02,  1.29814506e-01,  1.00107357e-01,  1.27915051e-02,\n",
       "        1.28592879e-01,  1.12155497e-01,  4.72722277e-02,  6.87448010e-02,\n",
       "        8.85853078e-03,  3.83499749e-02,  1.94043834e-02, -4.28455975e-03,\n",
       "        3.69736925e-02,  1.12075433e-01,  1.92998368e-02,  1.50864804e-02,\n",
       "        8.40060934e-02,  5.52906469e-03,  2.21362650e-01,  1.33668199e-01,\n",
       "        1.10932849e-01,  6.44747689e-02,  3.95788848e-02,  1.14243422e-02,\n",
       "        1.81132853e-01,  5.01939505e-02,  3.77964526e-02,  1.23178005e-01,\n",
       "       -1.80858411e-02,  1.29377067e-01,  1.40760662e-02,  5.40301055e-02,\n",
       "        5.58148921e-02,  2.99217012e-02,  2.00756758e-01,  9.15753618e-02,\n",
       "        7.72521943e-02,  8.49184021e-03,  8.78665894e-02,  1.91221759e-01,\n",
       "       -1.48223108e-03, -2.36490220e-02,  9.33276117e-02,  1.16159275e-01,\n",
       "        1.89247191e-01,  3.75074483e-02, -3.71180149e-03,  1.66678786e-01,\n",
       "        1.95185408e-01, -1.76172908e-02,  3.63193639e-02,  2.65481733e-02,\n",
       "        1.62058626e-03,  2.05133832e-03, -1.97661351e-02, -9.12414957e-03,\n",
       "       -1.17671974e-02,  1.41271576e-01,  1.88807435e-02,  1.14794940e-01,\n",
       "        2.88008824e-02, -2.14639455e-02,  1.86530184e-02,  2.43172720e-02,\n",
       "        1.29239177e-02,  2.78160740e-02,  1.88365895e-02,  1.25612423e-01,\n",
       "        3.13959010e-02,  1.98960990e-01,  4.07988988e-02,  3.05103920e-02,\n",
       "        1.06271200e-01,  4.38735671e-02,  2.00885218e-02,  1.67495266e-01,\n",
       "        1.59733053e-02,  1.51665002e-01,  1.96498200e-01,  9.65830695e-05,\n",
       "        5.90006150e-02,  5.04455045e-02,  6.97154254e-02,  1.21853381e-01,\n",
       "        6.88397959e-02,  4.17969450e-02,  5.51607050e-02,  3.30656804e-02,\n",
       "        1.83477551e-01,  3.01082060e-02, -4.53325287e-02,  5.37052155e-02,\n",
       "       -4.72752936e-03,  1.34668410e-01,  3.50421369e-02, -1.74722914e-02,\n",
       "       -2.96187308e-02,  1.08785637e-01,  1.61677040e-02,  8.00188854e-02,\n",
       "        1.68378174e-01,  7.45594054e-02,  1.72088351e-02,  3.79569158e-02,\n",
       "       -3.70677523e-02,  1.38521045e-01,  2.37946883e-02,  8.67336467e-02,\n",
       "        5.67392167e-03,  7.59161115e-02,  1.71091482e-01,  9.77688953e-02,\n",
       "        2.01848313e-01,  2.14452986e-02,  1.14320032e-01,  4.05719429e-02,\n",
       "        1.16467722e-01, -2.15242747e-02,  8.08734447e-03,  1.86597750e-01,\n",
       "        6.54478520e-02,  1.31145254e-01,  8.79909769e-02,  1.59561887e-01,\n",
       "        1.86899051e-01,  1.31262362e-01,  1.52804807e-01,  1.48289666e-01,\n",
       "        2.71201897e-02,  8.04587677e-02,  2.27111485e-02,  5.24814017e-02,\n",
       "        3.27715352e-02,  1.29019916e-01,  1.44352898e-01,  1.15241762e-03,\n",
       "       -7.34441401e-03,  1.72325343e-01,  9.85255372e-03,  5.21655940e-02,\n",
       "        2.85587683e-02,  4.10681851e-02,  1.57720178e-01,  5.25224023e-02,\n",
       "        3.98331210e-02,  1.06990211e-01, -3.35368440e-02,  6.95554167e-02,\n",
       "       -1.73195228e-02, -6.63721329e-03,  2.81562041e-02,  1.94263577e-01,\n",
       "        2.57999655e-02,  3.03172823e-02, -2.33510304e-02,  2.31913969e-01,\n",
       "        1.58120617e-02,  5.03645316e-02,  4.18335833e-02, -4.38550562e-02,\n",
       "        1.05561569e-01,  8.40435252e-02,  5.50208315e-02,  1.91839784e-01,\n",
       "       -2.44345590e-02,  4.68363129e-02, -2.87015103e-02,  1.25211567e-01,\n",
       "        1.89998135e-01,  4.10309806e-02,  3.80621105e-02,  5.45987338e-02,\n",
       "        2.00635850e-01,  2.07884252e-01,  1.02875069e-01,  6.30822629e-02,\n",
       "        5.87444054e-03, -1.31357983e-02,  8.20844173e-02, -1.15401600e-03,\n",
       "        1.93830892e-01, -3.83833423e-02, -1.12015577e-02,  1.52117133e-01,\n",
       "        1.19075753e-01,  3.78628373e-02,  6.03388399e-02,  6.00735396e-02,\n",
       "        1.07923403e-01,  4.91192788e-02,  9.38857272e-02,  6.61384407e-03,\n",
       "        1.68022752e-01,  1.58226918e-02, -2.03172248e-02,  7.31526911e-02,\n",
       "        1.36886820e-01,  2.11002618e-01, -2.11988017e-02,  1.02517173e-01,\n",
       "        1.30209967e-01,  3.06330342e-02,  7.50304982e-02,  1.75556298e-02,\n",
       "        3.55022699e-02,  1.23941928e-01,  2.35881787e-02,  1.37829781e-03,\n",
       "        1.39821336e-01, -4.73831668e-02,  3.07244901e-03,  8.53577107e-02,\n",
       "        5.66372685e-02,  3.36271822e-02,  1.93301871e-01,  1.48630098e-01,\n",
       "        2.24670935e-02,  6.57835230e-02, -6.65163342e-03,  2.04852670e-01,\n",
       "        8.97955671e-02,  3.19187194e-02,  1.73458979e-01,  9.31039304e-02,\n",
       "        1.01875579e-02,  1.24534905e-01,  6.35494962e-02, -3.43738422e-02,\n",
       "        4.77475300e-02,  6.71300590e-02,  3.87224331e-02, -2.16611139e-02,\n",
       "        5.55270948e-02,  8.33800063e-02,  1.19092487e-01,  1.85929388e-02,\n",
       "        4.13544215e-02, -3.00168269e-03,  1.19077871e-02,  4.38394845e-02,\n",
       "       -3.83075066e-02,  1.03215940e-01,  6.86846077e-02,  5.99556938e-02,\n",
       "        1.43347993e-01,  1.20502144e-01,  8.01165774e-02,  1.43114780e-03,\n",
       "        1.09910905e-01,  7.42124617e-02, -4.00993004e-02, -1.27902329e-02,\n",
       "        7.87013024e-02, -3.69424075e-02,  9.66517255e-02,  6.58080122e-03,\n",
       "        3.49179842e-02,  4.84447591e-02,  2.15166714e-02,  7.18022045e-03,\n",
       "        2.28327932e-03,  8.94047506e-03,  3.97286862e-02,  2.51205787e-02,\n",
       "        1.21567771e-01, -8.99665616e-03,  9.91580710e-02,  4.46718298e-02,\n",
       "        3.17528732e-02,  1.17176287e-01, -1.48551092e-02, -1.59651209e-02,\n",
       "        4.07174937e-02, -3.39744687e-02,  3.40872929e-02,  4.17544618e-02,\n",
       "        1.00451924e-01,  1.78911742e-02,  3.85274664e-02,  7.07655549e-02,\n",
       "        1.62384167e-01, -9.99584142e-03,  4.46187072e-02,  1.29893748e-02,\n",
       "        2.07357481e-01,  4.83619428e-04,  7.98073709e-02, -9.94715467e-03,\n",
       "       -1.31904539e-02,  3.09797246e-02,  4.88679260e-02,  2.60720495e-02,\n",
       "       -1.26144560e-02, -6.40942855e-03,  7.57619813e-02,  8.34149346e-02,\n",
       "       -2.30991701e-03,  1.51973918e-01,  4.83471975e-02,  3.54082733e-02,\n",
       "        8.69023800e-02,  4.21098098e-02,  1.31013736e-01,  9.46407765e-02,\n",
       "       -2.55654985e-03,  4.40779468e-03,  2.39375373e-03,  9.05973539e-02,\n",
       "        1.73297763e-01,  5.02730114e-03, -2.37651821e-02,  3.03745363e-02,\n",
       "        1.19941823e-01,  1.22789107e-01,  8.73433053e-02, -5.45163918e-03,\n",
       "        1.51900174e-02,  3.39345299e-02,  3.53182517e-02,  6.14455566e-02,\n",
       "        1.20352879e-01,  2.02970323e-03, -2.46639680e-02,  1.80761125e-02,\n",
       "        3.66266607e-03,  5.33869639e-02,  5.65130711e-02,  3.54384892e-02,\n",
       "        3.88692282e-02,  1.64778158e-02, -6.86666230e-04,  2.84027699e-02,\n",
       "       -1.67386215e-02, -2.97264606e-02,  7.12260902e-02, -3.56761590e-02,\n",
       "        1.09137364e-01, -3.46745029e-02,  1.76589489e-01,  6.63815141e-02,\n",
       "        4.14599217e-02,  1.80567786e-01,  1.71591230e-02,  9.10511464e-02,\n",
       "        6.53409511e-02,  1.16395906e-01,  7.54182339e-02,  1.75629690e-01,\n",
       "        1.04350880e-01,  4.16593114e-03,  3.13088186e-02,  1.94092184e-01,\n",
       "       -2.72509223e-03,  1.34445891e-01, -4.23122458e-02,  2.42915362e-01,\n",
       "        8.37499499e-02,  7.78279230e-02,  1.63421974e-01, -7.08043808e-03,\n",
       "        1.04222499e-01, -1.89328985e-03,  1.33356646e-01,  2.08815392e-02,\n",
       "       -1.06273973e-02,  9.11670029e-02,  4.61487770e-02,  2.02006832e-01,\n",
       "        1.46775663e-01,  1.58151239e-02, -2.17026826e-02,  4.64045964e-02,\n",
       "        9.90368724e-02,  1.44046366e-01,  9.56544951e-02,  1.31855868e-02,\n",
       "        9.16980058e-02,  6.82754591e-02,  1.74754828e-01,  4.12079431e-02,\n",
       "        1.67419374e-01, -3.57939377e-02,  4.07269746e-02,  2.13195473e-01,\n",
       "        2.84162499e-02,  3.57505381e-02,  1.43807560e-01,  4.94116172e-02,\n",
       "        7.57735968e-02, -1.92206148e-02, -8.79966188e-04,  9.06124860e-02,\n",
       "        6.74629658e-02, -1.66912656e-02,  2.24289820e-02,  7.13288337e-02,\n",
       "        1.05938770e-01,  2.57857982e-02,  6.55769035e-02,  2.17779100e-01,\n",
       "        8.13104957e-03,  9.92379524e-03,  1.60777923e-02,  1.71399370e-01,\n",
       "        1.58510700e-01,  7.74074048e-02,  4.16255668e-02,  1.75451499e-03,\n",
       "        2.56512910e-02,  4.44110744e-02,  1.08119078e-01,  3.71952951e-02,\n",
       "       -1.06230946e-02, -2.53657326e-02,  1.63197406e-02,  1.68288842e-01,\n",
       "        1.45245701e-01,  5.50314970e-03,  1.18725814e-01,  1.62919790e-01,\n",
       "       -1.25113968e-02,  2.10948005e-01, -6.15979312e-03,  5.33267707e-02,\n",
       "        2.76220609e-02,  2.08223835e-01,  5.31307003e-03,  8.15367624e-02,\n",
       "        3.39390486e-02,  3.24407853e-02,  3.71622220e-02, -9.31991171e-03,\n",
       "       -8.59737117e-03,  2.54786704e-02,  7.68886134e-02,  8.21293294e-02,\n",
       "       -2.81663556e-02,  9.23591573e-03,  1.81598105e-02,  8.31585526e-02,\n",
       "        3.10773514e-02,  1.27675489e-01,  5.28042838e-02,  3.14096957e-02,\n",
       "        1.71141718e-02,  4.11954485e-02,  5.17138354e-02,  1.06191948e-01,\n",
       "        5.92980534e-02,  3.81961241e-02, -1.04656657e-02,  9.97815877e-02,\n",
       "       -8.53275822e-04,  1.81725428e-01,  1.07826754e-01, -1.98081369e-03,\n",
       "        8.61997344e-03,  6.18109070e-02,  3.72857563e-02,  3.77658829e-02,\n",
       "        1.23472147e-01,  2.76659988e-02,  1.14168577e-01,  8.49860460e-02,\n",
       "        8.42290595e-02,  5.65615147e-02,  3.67146693e-02,  2.11688370e-01,\n",
       "        1.49811149e-01,  1.54243588e-01, -6.18671533e-03,  8.98470953e-02,\n",
       "        8.68480951e-02, -2.13853810e-02, -1.41898345e-03,  1.17133610e-01,\n",
       "        2.82771001e-03, -2.77041420e-02,  1.55527309e-01,  3.13660726e-02,\n",
       "       -2.53815483e-02,  1.86315119e-01,  4.10002097e-02, -5.14216796e-02,\n",
       "        1.64146081e-01,  1.73101485e-01, -1.08254794e-02,  4.64321710e-02,\n",
       "        5.73755540e-02,  7.10263252e-02, -5.37816733e-02, -9.95487557e-04,\n",
       "        3.80855314e-02,  8.42513368e-02,  1.28177926e-01,  8.81120935e-02,\n",
       "        1.66966841e-01,  7.98716918e-02,  2.34789904e-02,  9.80582461e-02,\n",
       "       -2.23081112e-02,  1.11023812e-02,  7.90224448e-02,  7.51217604e-02,\n",
       "        1.08738624e-01,  9.61645320e-02, -4.38401401e-02,  6.61032423e-02,\n",
       "        2.17414394e-01,  1.23577498e-01,  9.21657681e-02,  4.53164801e-02,\n",
       "        8.83141905e-03,  1.18278101e-01,  6.05174415e-02,  2.43762117e-02,\n",
       "        6.23966232e-02, -4.19938266e-02,  2.07031686e-02, -1.50606520e-02,\n",
       "        3.01800054e-02,  3.33415717e-02,  2.03463882e-02,  4.26765718e-02,\n",
       "        2.59666257e-02,  2.38322720e-01, -6.73497934e-03, -2.45089121e-02,\n",
       "       -1.98597815e-02,  1.46708205e-01,  6.45233467e-02, -1.57091133e-02,\n",
       "        5.06765069e-03, -9.73176153e-04, -5.50746778e-03,  1.78362146e-01,\n",
       "        3.85660529e-02, -3.70462723e-02,  1.88382834e-01,  8.72145779e-03,\n",
       "        6.76231235e-02,  7.47498944e-02,  5.79023361e-02,  1.75143424e-02,\n",
       "        1.46112880e-02,  1.20780014e-01,  1.37596488e-01,  1.22733854e-01,\n",
       "        4.74709161e-02,  8.11739638e-03,  5.36369979e-02,  4.38992605e-02,\n",
       "        5.31117665e-03,  1.11838721e-01, -1.92580000e-02, -1.65269990e-02,\n",
       "        2.57207379e-02,  4.12029214e-02,  1.49453655e-01, -1.65003650e-02,\n",
       "        3.74394096e-02,  9.19145793e-02,  2.07309246e-01,  4.75471020e-02,\n",
       "        1.08010121e-01,  1.36421263e-01,  1.93931870e-02,  1.00325607e-01,\n",
       "        3.07032540e-02,  1.33999735e-01, -1.30316708e-02,  3.72599401e-02,\n",
       "        1.05608180e-01,  8.01571310e-02,  8.55637565e-02, -2.30406243e-02,\n",
       "        5.49319163e-02,  2.34546453e-01,  1.25599150e-02,  3.52049596e-03,\n",
       "        3.29399556e-02,  2.01963875e-02, -4.84726876e-02, -6.01560110e-03,\n",
       "       -1.91566069e-02,  2.78065745e-02, -8.62575136e-03,  2.01386306e-02,\n",
       "        9.20661613e-02,  5.17324321e-02,  5.41178808e-02, -1.78922936e-02,\n",
       "       -1.54452829e-03,  7.36518800e-02,  1.32865384e-02, -2.66874749e-02,\n",
       "        2.46143341e-02,  6.11445531e-02,  3.33259143e-02,  1.21617075e-02,\n",
       "        1.37534678e-01,  8.75507668e-03,  7.65780583e-02, -1.08813811e-02,\n",
       "        4.53965440e-02,  2.79181320e-02,  3.64820212e-02,  6.14880547e-02,\n",
       "        9.52807367e-02,  1.33550152e-01,  1.39431804e-01,  1.17353566e-01,\n",
       "        7.24386200e-02,  2.30087668e-01,  1.94121376e-02,  3.61929499e-02,\n",
       "        3.31207402e-02, -1.50569761e-02, -1.99794695e-02,  1.55227944e-01,\n",
       "        3.58321369e-02,  1.54629061e-02,  1.81574479e-01,  1.98206857e-01,\n",
       "        3.58647741e-02,  5.27984723e-02, -2.88907643e-02,  3.51464301e-02,\n",
       "       -2.79020425e-02,  1.27420500e-02,  3.75807397e-02,  3.88793051e-02,\n",
       "        1.49374818e-02,  1.82612222e-02, -1.18623553e-02,  2.15369109e-02,\n",
       "        4.08871509e-02,  1.51157051e-01,  3.62120904e-02,  1.50111958e-01,\n",
       "       -2.76109949e-02,  2.46079583e-02,  1.43873453e-01, -1.18055046e-02,\n",
       "        5.73952310e-02,  1.23171248e-02,  4.34052497e-02,  6.30652830e-02,\n",
       "        1.34422228e-01,  1.76167548e-01,  1.73284754e-01,  7.77999684e-03,\n",
       "        2.49191761e-01,  8.43713582e-02,  9.84498709e-02,  2.43969485e-02,\n",
       "        1.71265811e-01,  9.76922587e-02,  1.91689193e-01,  1.35880843e-01,\n",
       "        8.04441571e-02,  1.69224981e-02,  1.75653324e-01,  3.16057988e-02,\n",
       "        9.93396621e-04,  1.18990280e-01,  1.33085430e-01,  1.14676304e-01,\n",
       "        1.90155953e-02, -2.02625394e-02, -2.13859398e-02, -1.67447664e-02,\n",
       "       -9.29137412e-03,  4.26347405e-02,  2.11454332e-01,  2.48431675e-02,\n",
       "        3.45928827e-03,  2.95829847e-02,  6.61234483e-02, -2.32984908e-02,\n",
       "        1.44840907e-02,  2.35830620e-02,  3.34323533e-02, -3.46962269e-03,\n",
       "       -2.99753919e-02,  1.36764357e-02,  1.10205352e-01,  2.59549711e-02,\n",
       "        3.97177301e-02,  1.24458864e-01,  1.41056314e-01,  3.12486440e-02,\n",
       "        8.81448016e-02,  3.75930332e-02, -6.88412460e-04,  6.16391227e-02,\n",
       "        5.47177494e-02,  1.50528625e-01,  2.93882942e-04,  3.27977911e-02,\n",
       "        6.90302923e-02,  5.00640646e-02,  1.09680921e-01,  1.08513363e-01,\n",
       "        3.76344696e-02,  8.39266367e-03,  2.05789998e-01, -4.02770005e-03,\n",
       "        1.71298355e-01,  5.39024547e-02,  4.42179218e-02,  9.88861024e-02,\n",
       "        2.19274359e-03, -4.72213048e-03, -2.08873209e-02,  1.78064868e-01,\n",
       "       -1.77046505e-03,  9.77657512e-02,  1.11061065e-02, -2.98106689e-02,\n",
       "        9.68258977e-02,  5.26004210e-02,  1.73925310e-01,  1.81673646e-01,\n",
       "        1.68112498e-02,  1.21598445e-01,  8.25502947e-02, -6.12333044e-03,\n",
       "        1.45973172e-02,  1.02787316e-01,  3.79161574e-02,  3.56175713e-02,\n",
       "       -3.19671109e-02,  2.59130858e-02,  3.60520445e-02,  2.47876551e-02,\n",
       "        1.22580960e-01,  9.89504755e-02,  2.22835653e-02, -4.30512941e-03,\n",
       "        3.61114554e-02,  2.14922562e-01,  9.99582410e-02,  1.51426777e-01,\n",
       "        1.02388766e-02,  1.55843794e-02,  1.54241472e-01,  2.09200159e-01,\n",
       "        6.38430342e-02,  2.87425071e-02,  6.88749328e-02, -1.80776101e-02,\n",
       "        3.57690151e-03,  1.17897943e-01,  3.39901820e-02,  8.13864451e-03,\n",
       "        2.27010064e-03,  1.08115472e-01, -5.35977073e-03,  3.55919711e-02,\n",
       "        7.59808067e-03, -2.10718196e-02,  3.86399701e-02,  1.07832678e-01,\n",
       "        1.41796216e-01,  1.21760229e-02,  2.10960165e-01,  7.67846406e-02,\n",
       "        1.10591829e-01,  1.54068932e-01, -1.65675916e-02,  1.11894630e-01,\n",
       "        1.50944710e-01, -2.38217954e-02,  4.45537604e-02,  1.27370730e-01,\n",
       "        8.94046873e-02,  1.09760739e-01, -3.29404022e-03, -1.36624798e-02,\n",
       "        1.29690796e-01,  2.21614957e-01, -1.75486878e-02,  5.65896649e-03,\n",
       "        2.31743548e-02,  1.00597203e-01, -3.89686748e-02,  4.23399881e-02,\n",
       "       -1.98960584e-02,  8.84819999e-02,  3.11936419e-02,  1.54884383e-01,\n",
       "        1.31688058e-01,  8.19215551e-02,  1.92940459e-02,  1.56580925e-01,\n",
       "        5.50334938e-02,  7.30380043e-02,  4.02492322e-02,  1.48961008e-01,\n",
       "        2.73116939e-02,  9.77226272e-02,  4.36360799e-02,  1.89758182e-01,\n",
       "        1.78880617e-01,  3.40598673e-02, -1.41349779e-02,  1.71649948e-01,\n",
       "        3.18942731e-03,  6.58428371e-02,  2.38321349e-02,  4.04079817e-02,\n",
       "        2.95580346e-02,  8.66365731e-02,  3.00980695e-02,  2.63715666e-02,\n",
       "        9.87192094e-02, -1.06278065e-04,  1.57559112e-01,  1.08173430e-01,\n",
       "       -5.03321439e-02,  4.41414788e-02,  8.58383626e-02,  3.46479542e-03,\n",
       "        5.95442951e-02,  9.82011575e-03,  4.06372920e-02,  1.06862567e-01,\n",
       "        3.06429919e-02,  1.64181277e-01,  1.66508872e-02,  1.64398402e-01,\n",
       "        1.41938299e-01,  3.42409201e-02,  9.33969468e-02,  6.27240911e-03,\n",
       "        4.19541728e-03,  1.92842215e-01, -2.47302819e-02,  9.27094184e-03,\n",
       "        3.09638921e-02,  3.38749252e-02,  1.44984290e-01,  2.91042812e-02,\n",
       "        3.59165706e-02,  8.78457725e-02,  1.37371263e-02,  1.37975469e-01,\n",
       "        1.07330590e-01,  1.94585416e-02,  1.99807897e-01,  1.23688489e-01,\n",
       "        1.89499184e-01,  2.38758642e-02,  1.71866149e-01,  4.05380353e-02,\n",
       "        1.39220461e-01,  1.94159105e-01,  1.11609519e-01, -2.66515706e-02,\n",
       "        6.25740644e-03, -1.55930519e-02,  2.99391896e-02,  5.06769717e-02,\n",
       "        5.41142672e-02, -2.41217595e-02,  6.91021010e-02,  2.02904835e-01,\n",
       "        3.09013315e-02,  3.75242457e-02,  1.50922403e-01,  8.56564268e-02,\n",
       "        6.73598275e-02,  1.26735754e-02,  1.29594862e-01,  1.05076854e-03,\n",
       "        4.58607357e-03,  3.67731489e-02,  4.91165631e-02,  1.40351206e-01,\n",
       "        3.64340357e-02,  2.29211506e-02,  5.00795245e-03,  1.10319466e-03,\n",
       "        2.49700788e-02,  1.55717462e-01,  5.21081798e-02, -4.77892172e-05,\n",
       "        7.39581510e-02,  1.20028906e-01,  1.69542581e-02,  6.16986379e-02,\n",
       "       -7.53079634e-03, -1.24739492e-02,  2.64240280e-02, -9.64254513e-03,\n",
       "       -1.46291880e-02,  1.97521538e-01, -9.45014320e-03,  8.78174752e-02,\n",
       "        1.46624982e-01,  1.21987097e-01,  1.48510650e-01, -1.79946458e-03,\n",
       "        3.34504098e-02,  2.62605604e-02,  7.32650310e-02, -1.35699566e-02,\n",
       "        9.35192406e-03,  9.34464559e-02,  4.99404483e-02, -2.35440880e-02,\n",
       "        1.52279921e-02,  1.49302199e-01, -1.64130386e-02,  5.13836509e-03,\n",
       "        9.30903926e-02,  2.42491737e-02, -2.23362017e-02,  1.62996411e-01,\n",
       "        1.87704921e-01,  3.88414040e-02,  1.88073829e-01,  9.87814814e-02,\n",
       "       -1.97905954e-02,  2.13351101e-02, -4.30516005e-02,  8.84436816e-02,\n",
       "        7.61967450e-02,  6.74132705e-02,  1.91648737e-01,  5.78611642e-02,\n",
       "       -4.79912572e-03,  1.73949465e-01, -2.47716717e-02,  1.34180024e-01,\n",
       "        3.99643444e-02,  1.20360730e-02,  4.62971888e-02,  6.44250885e-02,\n",
       "        2.81830169e-02,  3.63754891e-02,  7.81251211e-03,  1.82835311e-01,\n",
       "        3.90633978e-02,  1.54325575e-01,  3.34807672e-02,  1.79315135e-02,\n",
       "        1.16192989e-01,  2.10928038e-01,  5.16509824e-02,  2.42166251e-01,\n",
       "        3.86681780e-02, -1.01936720e-02,  6.23199716e-02,  7.07816035e-02,\n",
       "        1.88616678e-01,  1.60561521e-02,  2.11149022e-01,  1.21252544e-01,\n",
       "        9.61018354e-02,  5.92883639e-02, -1.88485309e-02,  1.19027093e-01,\n",
       "        9.20016691e-03,  1.97338969e-01,  4.16590683e-02,  1.75855175e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_pred=np.mean(pred,axis=0)\n",
    "mc_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "save_obj(mc_pred, \"../pred_loss_hyb_MC_Xx.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
