{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.spatial.distance as spdist\n",
    "\n",
    "# Normalize the data.\n",
    "from sklearn import preprocessing\n",
    "from numpy.linalg import cholesky, det, lstsq\n",
    "from scipy.optimize import minimize\n",
    "from numpy.linalg import inv\n",
    "\n",
    "def pass_arg(Xx, nsim, tr_size):\n",
    "    \n",
    "    # Compute the RMSE\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "        return np.sqrt(np.mean((y_pred-y_true)**2))\n",
    "\n",
    "    print(\"tr_Size:\",tr_size)\n",
    "    # Making sure final porosity is less than initial\n",
    "    def poros(poroi, porof):\n",
    "        porofn = -porof*(porof<0)\n",
    "        porofp = porof*(porof>=poroi) - poroi*(porof>=poroi)\n",
    "        return porofp+porofn\n",
    "\n",
    "\n",
    "    # Load labeled data\n",
    "    data = np.loadtxt('../data/labeled_data.dat')\n",
    "    x_labeled = data[:, :2].astype(np.float64) # -2 because we do not need porosity predictions\n",
    "    y_labeled = data[:, -2:-1].astype(np.float64) # dimensionless bond length and porosity measurements\n",
    "\n",
    "    # normalize dataset with MinMaxScaler\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "    x_labeled = scaler.fit_transform(x_labeled)\n",
    "    # y_labeled = scaler.fit_transform(y_labeled)\n",
    "\n",
    "    tr_size = int(tr_size)\n",
    "\n",
    "    # train and test data\n",
    "    trainX, trainY = x_labeled[:tr_size,:], y_labeled[:tr_size]\n",
    "    testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]\n",
    "\n",
    "\n",
    "    # trainY = np.transpose(trainY)\n",
    "    # testY = np.transpose(testY)\n",
    "\n",
    "    data_phyloss = np.loadtxt('../data/unlabeled_data_BK_constw_v2_1525.dat')\n",
    "    x_unlabeled = data_phyloss[:, :]\n",
    "\n",
    "    # initial porosity\n",
    "    initporo = x_unlabeled[:, -1]\n",
    "\n",
    "    x_unlabeled1 = x_unlabeled[:1303, :2]\n",
    "    x_unlabeled2 = x_unlabeled[-6:, :2]\n",
    "    x_unlabeled = np.vstack((x_unlabeled1,x_unlabeled2))\n",
    "\n",
    "    x_unlabeled = scaler.fit_transform(x_unlabeled)\n",
    "    init_poro1 = initporo[:1303]\n",
    "    init_poro2 = initporo[-6:]\n",
    "    init_poro = np.hstack((init_poro1,init_poro2))\n",
    "    \n",
    "    \n",
    "    def covSEard(hyp=None, x=None, z=None):\n",
    "        ''' Squared Exponential covariance function with Automatic Relevance Detemination\n",
    "         (ARD) distance measure. The covariance function is parameterized as:\n",
    "\n",
    "         k(x^p,x^q) = sf2 * exp(-(x^p - x^q)' * inv(P) * (x^p - x^q)/2)\n",
    "\n",
    "         where the P matrix is diagonal with ARD parameters ell_1^2,...,ell_D^2, where\n",
    "         D is the dimension of the input space and sf2 is the signal variance.\n",
    "\n",
    "         The hyperparameters are:\n",
    "\n",
    "         hyp = [ log(ell_1)\n",
    "                 log(ell_2)\n",
    "                 ...\n",
    "                 log(ell_D)\n",
    "                 log(sqrt(sf2)) ]\n",
    "        '''\n",
    "\n",
    "        [n, D] = x.shape\n",
    "        ell = 1/np.array(hyp[0:D])        # characteristic length scale\n",
    "        \n",
    "        \n",
    "        sf2 = np.array(hyp[D])**2         # signal variance\n",
    "        tmp = np.dot(np.diag(ell),x.T).T\n",
    "        A = spdist.cdist(np.dot(np.diag(ell),x.T).T, np.dot(np.diag(ell),z.T).T, 'sqeuclidean') # cross covariances\n",
    "        A = sf2*np.exp(-0.5*A)  \n",
    "\n",
    "        return A\n",
    "\n",
    "\n",
    "    def posterior_predictive(X_s, X_train, Y_train, l1=.1, l2=.1, sigma_f=.1, sigma_y=1e-5):\n",
    "        '''  \n",
    "        Computes the suffifient statistics of the GP posterior predictive distribution \n",
    "        from m training data X_train and Y_train and n new inputs X_s.\n",
    "\n",
    "        Args:\n",
    "            X_s: New input locations (n x d).\n",
    "            X_train: Training locations (m x d).\n",
    "            Y_train: Training targets (m x 1).\n",
    "            l: Kernel length parameter.\n",
    "            sigma_f: Kernel vertical variation parameter.\n",
    "            sigma_y: Noise parameter.\n",
    "\n",
    "        Returns:\n",
    "            Posterior mean vector (n x d) and covariance matrix (n x n).\n",
    "        '''\n",
    "\n",
    "\n",
    "        K = covSEard(hyp=[l1,l2,sigma_f], x=X_train, z=X_train) + sigma_y**2 * np.eye(len(X_train))\n",
    "        K_s = covSEard(hyp=[l1,l2,sigma_f], x=X_train, z=X_s)\n",
    "        K_ss = covSEard(hyp=[l1,l2,sigma_f], x=X_s, z=X_s)  + 1e-8 * np.eye(len(X_s))\n",
    "#         K_inv = inv(K)\n",
    "        K_inv = np.linalg.pinv(K)\n",
    "    \n",
    "        # Equation (4)\n",
    "        mu_s = K_s.T.dot(K_inv).dot(Y_train)\n",
    "\n",
    "        # Equation (5)\n",
    "        cov_s = K_ss - K_s.T.dot(K_inv).dot(K_s)\n",
    "        \n",
    "        return mu_s, cov_s\n",
    "\n",
    "\n",
    "    def nll_fn(X_train, Y_train, x_unlabeled, init_poro, naive=False):\n",
    "        '''\n",
    "        Returns a function that computes the negative log marginal\n",
    "        likelihood for training data X_train and Y_train and given \n",
    "        noise level.\n",
    "\n",
    "        Args:\n",
    "            X_train: training locations (m x d).\n",
    "            Y_train: training targets (m x 1).\n",
    "            noise: known noise level of Y_train.\n",
    "            naive: if True use a naive implementation of Eq. (7), if \n",
    "                   False use a numerically more stable implementation. \n",
    "\n",
    "        Returns:\n",
    "            Minimization objective.\n",
    "        '''\n",
    "\n",
    "        def nll_stable(theta):\n",
    "            # Numerically more stable implementation of Eq. (7) as described\n",
    "            # in http://www.gaussianprocess.org/gpml/chapters/RW2.pdf, Section\n",
    "            # 2.2, Algorithm 2.1.\n",
    "            K = covSEard(hyp=[theta[0],theta[1],theta[2]], x=X_train, z=X_train) + \\\n",
    "                theta[3]**2 * np.eye(len(X_train))\n",
    "            \n",
    "            \n",
    "            K += 1e-6 * np.eye(*K.shape)\n",
    "            L = cholesky(K)\n",
    "        \n",
    "\n",
    "            mu_un, _ = posterior_predictive(x_unlabeled, X_train, Y_train, l1=theta[0], l2=theta[1], sigma_f=theta[2], sigma_y=theta[3])\n",
    "            phyloss_poro = np.mean(poros(init_poro, mu_un))\n",
    "\n",
    "            log_loss = np.sum(np.log(np.diagonal(L))) + \\\n",
    "                   0.5 * Y_train.T.dot(lstsq(L.T, lstsq(L, Y_train)[0])[0]) + \\\n",
    "                   0.5 * len(X_train) * np.log(2*np.pi)\n",
    "        \n",
    "            # print(500000*phyloss_poro,log_loss, theta)\n",
    "            return 500000*phyloss_poro + log_loss\n",
    "\n",
    "        if naive:\n",
    "            return nll_naive\n",
    "        else:\n",
    "            return nll_stable\n",
    "\n",
    "    \n",
    "    # Optimization\n",
    "    res = minimize(nll_fn(trainX, trainY, x_unlabeled, init_poro), x0 = [.1, .1, .1, 1e-3], \n",
    "                   bounds=((1e-5, None), (1e-5, None), (1e-5, None),(1e-7, None)),\n",
    "                    method='L-BFGS-B')\n",
    "    \n",
    "#     print(f'After parameter optimization: l1={res.x[0]:.5f} l2={res.x[1]:.5f} sigma_f={res.x[2]:.5f}')\n",
    "#     print(np.exp(res.x[0]),np.exp(res.x[1]), np.exp(res.x[2]))\n",
    "    mu_s, cov_s = posterior_predictive(testX, trainX, trainY, *res.x)\n",
    "    \n",
    "    RMSE = []\n",
    "    for ii in range(int(nsim)):\n",
    "        samples = np.random.multivariate_normal(mu_s.ravel(), cov_s, 1)\n",
    "        RMSE.append(root_mean_squared_error(testY, samples))\n",
    "        \n",
    "        print(\"RMSE:\", root_mean_squared_error(testY, samples))\n",
    "\n",
    "\n",
    "    return samples, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_Size: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berkc\\Miniconda3\\envs\\R\\lib\\site-packages\\ipykernel_launcher.py:160: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.01887178119891527\n",
      "RMSE: 0.022422125472662455\n",
      "RMSE: 0.01709485051420006\n",
      "RMSE: 0.01894643997926025\n",
      "RMSE: 0.024926161122790655\n",
      "RMSE: 0.02088521855032978\n",
      "RMSE: 0.02123508813025256\n",
      "RMSE: 0.017469613535328905\n",
      "RMSE: 0.02082993830179254\n",
      "RMSE: 0.019368945066724246\n",
      "RMSE: 0.021151389805499684\n",
      "RMSE: 0.016658314991575485\n",
      "RMSE: 0.021130084199330715\n",
      "RMSE: 0.01908215000508387\n",
      "RMSE: 0.01939684132909302\n",
      "RMSE: 0.020203666011849638\n",
      "RMSE: 0.01970623097466492\n",
      "RMSE: 0.0190532637895618\n",
      "RMSE: 0.018216483685318384\n",
      "RMSE: 0.01974181719310583\n",
      "RMSE: 0.02034516659833612\n",
      "RMSE: 0.018584352201264587\n",
      "RMSE: 0.02271928636080908\n",
      "RMSE: 0.017779981884677682\n",
      "RMSE: 0.024036591140256997\n",
      "RMSE: 0.019111905320128877\n",
      "RMSE: 0.02280856459857227\n",
      "RMSE: 0.017998203325819247\n",
      "RMSE: 0.018034947546228416\n",
      "RMSE: 0.019466857744585567\n",
      "RMSE: 0.021510992271884966\n",
      "RMSE: 0.01890676622351696\n",
      "RMSE: 0.020457298453895458\n",
      "RMSE: 0.01941486616834205\n",
      "RMSE: 0.020304294711812124\n",
      "RMSE: 0.019659313038087448\n",
      "RMSE: 0.017278456134758792\n",
      "RMSE: 0.018787267351328487\n",
      "RMSE: 0.01907569663746207\n",
      "RMSE: 0.017728250244809313\n",
      "RMSE: 0.020989539925798655\n",
      "RMSE: 0.018974529133562054\n",
      "RMSE: 0.020296287542742417\n",
      "RMSE: 0.018166108193092095\n",
      "RMSE: 0.01969864994873792\n",
      "RMSE: 0.019068283563889045\n",
      "RMSE: 0.017930347133424895\n",
      "RMSE: 0.020278643832217124\n",
      "RMSE: 0.023801772279813303\n",
      "RMSE: 0.019342482003840346\n",
      "RMSE: 0.02364872300626923\n",
      "RMSE: 0.02050193941227498\n",
      "RMSE: 0.02033122020840356\n",
      "RMSE: 0.019231788646600023\n",
      "RMSE: 0.019531753828251577\n",
      "RMSE: 0.020920897839963662\n",
      "RMSE: 0.017577682055843626\n",
      "RMSE: 0.02244643493902261\n",
      "RMSE: 0.018327826631939027\n",
      "RMSE: 0.02081730579269\n",
      "RMSE: 0.019427396541394457\n",
      "RMSE: 0.018950873438468883\n",
      "RMSE: 0.018868309631465532\n",
      "RMSE: 0.02060083514734719\n",
      "RMSE: 0.021508245421576297\n",
      "RMSE: 0.020488238867502173\n",
      "RMSE: 0.01819914511269821\n",
      "RMSE: 0.02208061906007221\n",
      "RMSE: 0.019566137776715845\n",
      "RMSE: 0.019707148432951953\n",
      "RMSE: 0.020389015181996907\n",
      "RMSE: 0.01788352845479479\n",
      "RMSE: 0.018677819383938227\n",
      "RMSE: 0.016777942935998866\n",
      "RMSE: 0.023032072849746874\n",
      "RMSE: 0.019882318520109618\n",
      "RMSE: 0.019629531789847866\n",
      "RMSE: 0.022500205893223834\n",
      "RMSE: 0.018603065001233862\n",
      "RMSE: 0.018694746576729135\n",
      "RMSE: 0.022526515092215777\n",
      "RMSE: 0.02155993559086671\n",
      "RMSE: 0.019861881204937502\n",
      "RMSE: 0.018986963584533913\n",
      "RMSE: 0.01957716698290589\n",
      "RMSE: 0.020947927619008606\n",
      "RMSE: 0.01921695290389749\n",
      "RMSE: 0.01817386224143254\n",
      "RMSE: 0.02253074573185414\n",
      "RMSE: 0.019860083487148492\n",
      "RMSE: 0.020554817801712918\n",
      "RMSE: 0.022476338273600915\n",
      "RMSE: 0.01914933237767348\n",
      "RMSE: 0.021036478656928804\n",
      "RMSE: 0.018662248640136568\n",
      "RMSE: 0.02253779766891499\n",
      "RMSE: 0.021678000355755466\n",
      "RMSE: 0.02000480843235223\n",
      "RMSE: 0.023579016828811045\n",
      "RMSE: 0.02223854393722379\n"
     ]
    }
   ],
   "source": [
    "Xx = np.random.uniform(size=(3, 2))\n",
    "ss, rmse = pass_arg(Xx, 100, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02002908291162017"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
