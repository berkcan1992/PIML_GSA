{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Normalize the data.\n",
    "from sklearn import preprocessing\n",
    "from numpy.linalg import cholesky, det, lstsq\n",
    "from scipy.optimize import minimize\n",
    "from numpy.linalg import inv\n",
    "\n",
    "def pass_arg(Xx, nsim, tr_size):\n",
    "    \n",
    "    # Compute the RMSE\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "        return np.sqrt(np.mean((y_pred-y_true)**2))\n",
    "\n",
    "    def kernel(X1, X2, l=1.0, sigma_f=1.0):\n",
    "        '''\n",
    "        Isotropic squared exponential kernel. Computes \n",
    "        a covariance matrix from points in X1 and X2.\n",
    "\n",
    "        Args:\n",
    "            X1: Array of m points (m x d).\n",
    "            X2: Array of n points (n x d).\n",
    "\n",
    "        Returns:\n",
    "            Covariance matrix (m x n).\n",
    "        '''\n",
    "        sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "        return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist)\n",
    "\n",
    "    def posterior_predictive(X_s, X_train, Y_train, l=1.0, sigma_f=1.0, sigma_y=1e-8):\n",
    "        '''  \n",
    "        Computes the suffifient statistics of the GP posterior predictive distribution \n",
    "        from m training data X_train and Y_train and n new inputs X_s.\n",
    "\n",
    "        Args:\n",
    "            X_s: New input locations (n x d).\n",
    "            X_train: Training locations (m x d).\n",
    "            Y_train: Training targets (m x 1).\n",
    "            l: Kernel length parameter.\n",
    "            sigma_f: Kernel vertical variation parameter.\n",
    "            sigma_y: Noise parameter.\n",
    "\n",
    "        Returns:\n",
    "            Posterior mean vector (n x d) and covariance matrix (n x n).\n",
    "        '''\n",
    "        K = kernel(X_train, X_train, l, sigma_f) + sigma_y**2 * np.eye(len(X_train))\n",
    "        K_s = kernel(X_train, X_s, l, sigma_f)\n",
    "        K_ss = kernel(X_s, X_s, l, sigma_f) + 1e-8 * np.eye(len(X_s))\n",
    "        K_inv = inv(K)\n",
    "\n",
    "        # Equation (4)\n",
    "        mu_s = K_s.T.dot(K_inv).dot(Y_train)\n",
    "\n",
    "        # Equation (5)\n",
    "        cov_s = K_ss - K_s.T.dot(K_inv).dot(K_s)\n",
    "        \n",
    "        min_eig = np.min(np.real(np.linalg.eigvals(cov_s)))\n",
    "        print(\"eig:\",min_eig)\n",
    "        if min_eig < 0:\n",
    "            cov_s -= 10*min_eig * np.eye(*cov_s.shape)\n",
    "            \n",
    "        cov_s += 1e8*min_eig * np.eye(*cov_s.shape)\n",
    "    \n",
    "        return mu_s, cov_s\n",
    "\n",
    "\n",
    "    def nll_fn(X_train, Y_train, noise=0, naive=False):\n",
    "        '''\n",
    "        Returns a function that computes the negative log marginal\n",
    "        likelihood for training data X_train and Y_train and given \n",
    "        noise level.\n",
    "\n",
    "        Args:\n",
    "            X_train: training locations (m x d).\n",
    "            Y_train: training targets (m x 1).\n",
    "            noise: known noise level of Y_train.\n",
    "            naive: if True use a naive implementation of Eq. (7), if \n",
    "                   False use a numerically more stable implementation. \n",
    "\n",
    "        Returns:\n",
    "            Minimization objective.\n",
    "        '''\n",
    "        def nll_naive(theta):\n",
    "            # Naive implementation of Eq. (7). Works well for the examples \n",
    "            # in this article but is numerically less stable compared to \n",
    "            # the implementation in nll_stable below.\n",
    "            K = kernel(X_train, X_train, l=theta[0], sigma_f=theta[1]) + \\\n",
    "                noise**2 * np.eye(len(X_train))\n",
    "            return 0.5 * np.log(det(K)) + \\\n",
    "                   0.5 * Y_train.T.dot(inv(K).dot(Y_train)) + \\\n",
    "                   0.5 * len(X_train) * np.log(2*np.pi)\n",
    "\n",
    "        def nll_stable(theta):\n",
    "            # Numerically more stable implementation of Eq. (7) as described\n",
    "            # in http://www.gaussianprocess.org/gpml/chapters/RW2.pdf, Section\n",
    "            # 2.2, Algorithm 2.1.\n",
    "            K = kernel(X_train, X_train, l=theta[0], sigma_f=theta[1]) + \\\n",
    "                noise**2 * np.eye(len(X_train)) \n",
    "            K += 1e-6 * np.eye(*K.shape)\n",
    "    #         print(np.linalg.eigvals(K))\n",
    "            L = cholesky(K)\n",
    "\n",
    "            return np.sum(np.log(np.diagonal(L))) + \\\n",
    "                   0.5 * Y_train.T.dot(lstsq(L.T, lstsq(L, Y_train)[0])[0]) + \\\n",
    "                   0.5 * len(X_train) * np.log(2*np.pi)\n",
    "\n",
    "        if naive:\n",
    "            return nll_naive\n",
    "        else:\n",
    "            return nll_stable\n",
    "\n",
    "\n",
    "\n",
    "    # Load labeled data\n",
    "    data = np.loadtxt('../data/labeled_data.dat')\n",
    "    x_labeled = data[:, :2].astype(np.float64) # -2 because we do not need porosity predictions\n",
    "    y_labeled = data[:, -2:-1].astype(np.float64) # dimensionless bond length and porosity measurements\n",
    "\n",
    "    # normalize dataset with MinMaxScaler\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "    x_labeled = scaler.fit_transform(x_labeled)\n",
    "    # y_labeled = scaler.fit_transform(y_labeled)\n",
    "\n",
    "    tr_size = tr_size\n",
    "\n",
    "    # train and test data\n",
    "    trainX, trainY = x_labeled[:tr_size,:], y_labeled[:tr_size]\n",
    "    # testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]\n",
    "#     testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]\n",
    "\n",
    "\n",
    "    # trainY = np.transpose(trainY)\n",
    "    # testY = np.transpose(testY)\n",
    "\n",
    "    # data_phyloss = np.loadtxt('../data/unlabeled_data_BK_constw_v2_1525.dat')\n",
    "    # x_unlabeled = data_phyloss[:, :]\n",
    "\n",
    "    # # initial porosity\n",
    "    # initporo = x_unlabeled[:, -1]\n",
    "\n",
    "    # x_unlabeled1 = x_unlabeled[:1303, :2]\n",
    "    # x_unlabeled2 = x_unlabeled[-6:, :2]\n",
    "    # x_unlabeled = np.vstack((x_unlabeled1,x_unlabeled2))\n",
    "\n",
    "    # x_unlabeled = scaler.fit_transform(x_unlabeled)\n",
    "    # init_poro1 = initporo[:1303]\n",
    "    # init_poro2 = initporo[-6:]\n",
    "    # init_poro = np.hstack((init_poro1,init_poro2))\n",
    "\n",
    "\n",
    "    # Optimization\n",
    "    res = minimize(nll_fn(trainX, trainY), [.35, .45], \n",
    "                   bounds=((1e-5, None), (1e-5, None)),\n",
    "                   method='L-BFGS-B')\n",
    "\n",
    "    mu_s, cov_s = posterior_predictive(Xx, trainX, trainY, *res.x)\n",
    "\n",
    "    # print(f'After parameter optimization: l={res.x[0]:.2f} sigma_f={res.x[1]:.2f}')\n",
    "    print(cov_s)\n",
    "    samples = np.random.multivariate_normal(mu_s.ravel(), cov_s, nsim)\n",
    "#     print(\"RMSE:\", root_mean_squared_error(testY, samples))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eig: 8.771637533611218e-06\n",
      "[[ 8.77163817e+02 -6.15424039e-06 -9.19877974e-06]\n",
      " [-6.26810837e-06  8.77163807e+02 -4.77901277e-06]\n",
      " [-9.25711946e-06 -4.56368086e-06  8.77163764e+02]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berkc\\Miniconda3\\envs\\R\\lib\\site-packages\\ipykernel_launcher.py:108: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "C:\\Users\\berkc\\Miniconda3\\envs\\R\\lib\\site-packages\\ipykernel_launcher.py:164: RuntimeWarning: covariance is not positive-semidefinite.\n"
     ]
    }
   ],
   "source": [
    "Xx = np.random.uniform(size=(3, 2))\n",
    "ss = pass_arg(Xx, 1, 30)\n",
    "# print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.41935484]\n",
      " [1.         0.64516129]\n",
      " [0.9        1.        ]\n",
      " [0.48       0.61290323]\n",
      " [0.26       0.4516129 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.71828183, 2.24046058, 2.20118042, 2.58759721, 2.68417729],\n",
       "       [2.24046058, 2.71828183, 2.65116896, 2.58589401, 2.44094204],\n",
       "       [2.20118042, 2.65116896, 2.71828183, 2.55995783, 2.38534172],\n",
       "       [2.58759721, 2.58589401, 2.55995783, 2.71828183, 2.68132803],\n",
       "       [2.68417729, 2.44094204, 2.38534172, 2.68132803, 2.71828183]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.spatial.distance as spdist\n",
    "\n",
    "def covSEard(hyp=None, x=None, z=None, der=None):\n",
    "    ''' Squared Exponential covariance function with Automatic Relevance Detemination\n",
    "     (ARD) distance measure. The covariance function is parameterized as:\n",
    "    \n",
    "     k(x^p,x^q) = sf2 * exp(-(x^p - x^q)' * inv(P) * (x^p - x^q)/2)\n",
    "    \n",
    "     where the P matrix is diagonal with ARD parameters ell_1^2,...,ell_D^2, where\n",
    "     D is the dimension of the input space and sf2 is the signal variance.\n",
    "    \n",
    "     The hyperparameters are:\n",
    "    \n",
    "     hyp = [ log(ell_1)\n",
    "             log(ell_2)\n",
    "             ...\n",
    "             log(ell_D)\n",
    "             log(sqrt(sf2)) ]\n",
    "    '''\n",
    "#     if hyp == None:                 # report number of parameters\n",
    "#         return ['D + 1']            # USAGE: integer OR D_+_int (spaces are SIGNIFICANT)\n",
    "    \n",
    "    [n, D] = x.shape\n",
    "    ell = 1/np.exp(hyp[0:D])        # characteristic length scale\n",
    "\n",
    "    sf2 = np.exp(2.*hyp[D])         # signal variance\n",
    "    tmp = np.dot(np.diag(ell),x.T).T\n",
    "    A = spdist.cdist(tmp, tmp, 'sqeuclidean')\n",
    "    A = sf2*np.exp(-0.5*A)  \n",
    "        \n",
    "#     if z == 'diag':\n",
    "#         A = np.zeros((n,1))\n",
    "#     elif z == None:\n",
    "#         tmp = np.dot(np.diag(ell),x.T).T\n",
    "#         A = spdist.cdist(tmp, tmp, 'sqeuclidean')\n",
    "#     else:                           # compute covariance between data sets x and z\n",
    "#         A = spdist.cdist(np.dot(np.diag(ell),x.T).T, np.dot(np.diag(ell),z.T).T, 'sqeuclidean') # cross covariances\n",
    " \n",
    "#     A = sf2*np.exp(-0.5*A)\n",
    "#     if not der == None:\n",
    "#         if der < D:                 # compute derivative matrix wrt length scale parameters\n",
    "#             if z == 'diag':\n",
    "#                 A = A*0.\n",
    "#             elif z == None:\n",
    "#                 tmp = np.atleast_2d(x[:,der]).T/ell[der]\n",
    "#                 A = A * spdist.cdist(tmp, tmp, 'sqeuclidean')\n",
    "#             else:\n",
    "#                 A = A * spdist.cdist(np.atleast_2d(x[:,der]).T/ell[der], np.atleast_2d(z[:,der]).T/ell[der], 'sqeuclidean')\n",
    "#         elif der==D:                # compute derivative matrix wrt magnitude parameter\n",
    "#             A = 2.*A\n",
    "#         else:\n",
    "#             raise Exception(\"Wrong derivative index in covSEard\")\n",
    "                \n",
    "    return A\n",
    "\n",
    "covSEard(hyp=[.5,.5,.5], x=trainX, z=trainX, der=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load labeled data\n",
    "# data = np.loadtxt('../data/labeled_data.dat')\n",
    "# x_labeled = data[:, :2].astype(np.float64) # -2 because we do not need porosity predictions\n",
    "# y_labeled = data[:, -2:-1].astype(np.float64) # dimensionless bond length and porosity measurements\n",
    "\n",
    "# # normalize dataset with MinMaxScaler\n",
    "# scaler = preprocessing.MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "# x_labeled = scaler.fit_transform(x_labeled)\n",
    "# # y_labeled = scaler.fit_transform(y_labeled)\n",
    "\n",
    "# tr_size = 5\n",
    "\n",
    "# # train and test data\n",
    "# trainX, trainY = x_labeled[:tr_size,:], y_labeled[:tr_size]\n",
    "\n",
    "\n",
    "# def exponential_cov(x, y, params):\n",
    "#     return params[0]**2 * np.exp( -0.5 * params[1] * np.subtract.outer(x, y)**2)\n",
    "\n",
    "# def kernel(X1, X2, l=1.0, sigma_f=1.0):\n",
    "#     sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "#     return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist)\n",
    "\n",
    "# b1=kernel(trainX, trainX, 1.0,1.0)\n",
    "# b2=exponential_cov(trainX, trainX, [1.0,1.0])\n",
    "# print(b1)\n",
    "# print(b2)\n",
    "\n",
    "\n",
    "# # print(trainX**2)\n",
    "# # a1 = np.sum(trainX**2, 1).reshape(-1, 1)\n",
    "# # a2 = np.sum(trainX**2, 1) \n",
    "# # a3 = -2 * np.dot(trainX, trainX.T)\n",
    "# # print(a1, a2, a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Normalize the data.\n",
    "from sklearn import preprocessing\n",
    "from numpy.linalg import cholesky, det, lstsq\n",
    "from scipy.optimize import minimize\n",
    "import scipy.spatial.distance as spdist\n",
    "\n",
    "def pass_arg(Xx, nsim, tr_size):\n",
    "    \n",
    "    log = np.log\n",
    "    \n",
    "    # Compute the RMSE\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "        return np.sqrt(np.mean((y_pred-y_true)**2))\n",
    "\n",
    "    # Making sure final porosity is less than initial\n",
    "    def poros(poroi, porof):\n",
    "        porofn = -porof*(porof<0)\n",
    "        porofp = porof*(porof>=poroi) - poroi*(porof>=poroi)\n",
    "        return porofp+porofn\n",
    "\n",
    "\n",
    "    # Load labeled data\n",
    "    data = np.loadtxt('../data/labeled_data.dat')\n",
    "    x_labeled = data[:, :2].astype(np.float64) # -2 because we do not need porosity predictions\n",
    "    y_labeled = data[:, -2:-1].astype(np.float64) # dimensionless bond length and porosity measurements\n",
    "\n",
    "    # normalize dataset with MinMaxScaler\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "    x_labeled = scaler.fit_transform(x_labeled)\n",
    "    # y_labeled = scaler.fit_transform(y_labeled)\n",
    "\n",
    "    tr_size = tr_size\n",
    "\n",
    "    # train and test data\n",
    "    trainX, trainY = x_labeled[:tr_size,:], y_labeled[:tr_size]\n",
    "    testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]\n",
    "\n",
    "\n",
    "    # trainY = np.transpose(trainY)\n",
    "    # testY = np.transpose(testY)\n",
    "\n",
    "    data_phyloss = np.loadtxt('../data/unlabeled_data_BK_constw_v2_1525.dat')\n",
    "    x_unlabeled = data_phyloss[:, :]\n",
    "\n",
    "    # initial porosity\n",
    "    initporo = x_unlabeled[:, -1]\n",
    "\n",
    "    x_unlabeled1 = x_unlabeled[:1303, :2]\n",
    "    x_unlabeled2 = x_unlabeled[-6:, :2]\n",
    "    x_unlabeled = np.vstack((x_unlabeled1,x_unlabeled2))\n",
    "\n",
    "    x_unlabeled = scaler.fit_transform(x_unlabeled)\n",
    "    init_poro1 = initporo[:1303]\n",
    "    init_poro2 = initporo[-6:]\n",
    "    init_poro = np.hstack((init_poro1,init_poro2))\n",
    "    \n",
    "    \n",
    "    def covSEard(hyp=None, x=None, z=None):\n",
    "        ''' Squared Exponential covariance function with Automatic Relevance Detemination\n",
    "         (ARD) distance measure. The covariance function is parameterized as:\n",
    "\n",
    "         k(x^p,x^q) = sf2 * exp(-(x^p - x^q)' * inv(P) * (x^p - x^q)/2)\n",
    "\n",
    "         where the P matrix is diagonal with ARD parameters ell_1^2,...,ell_D^2, where\n",
    "         D is the dimension of the input space and sf2 is the signal variance.\n",
    "\n",
    "         The hyperparameters are:\n",
    "\n",
    "         hyp = [ log(ell_1)\n",
    "                 log(ell_2)\n",
    "                 ...\n",
    "                 log(ell_D)\n",
    "                 log(sqrt(sf2)) ]\n",
    "        '''\n",
    "\n",
    "        [n, D] = x.shape\n",
    "        ell = 1/np.array(hyp[0:D])        # characteristic length scale\n",
    "        \n",
    "        \n",
    "        sf2 = np.array(hyp[D])**2         # signal variance\n",
    "        tmp = np.dot(np.diag(ell),x.T).T\n",
    "        \n",
    "        if z == 'itself':\n",
    "            tmp = np.dot(np.diag(ell),x.T).T\n",
    "            A = spdist.cdist(tmp, tmp, 'sqeuclidean')\n",
    "        else:                           # compute covariance between data sets x and z\n",
    "            A = spdist.cdist(np.dot(np.diag(ell),x.T).T, np.dot(np.diag(ell),z.T).T, 'sqeuclidean') # cross covariances\n",
    "\n",
    "        A = sf2*np.exp(-0.5*A)  \n",
    "\n",
    "        return A\n",
    "\n",
    "\n",
    "    def posterior_predictive(X_s, X_train, Y_train, l1=log(.1), l2=log(.1), sigma_f=log(.1), sigma_y=0):\n",
    "        '''  \n",
    "        Computes the suffifient statistics of the GP posterior predictive distribution \n",
    "        from m training data X_train and Y_train and n new inputs X_s.\n",
    "\n",
    "        Args:\n",
    "            X_s: New input locations (n x d).\n",
    "            X_train: Training locations (m x d).\n",
    "            Y_train: Training targets (m x 1).\n",
    "            l: Kernel length parameter.\n",
    "            sigma_f: Kernel vertical variation parameter.\n",
    "            sigma_y: Noise parameter.\n",
    "\n",
    "        Returns:\n",
    "            Posterior mean vector (n x d) and covariance matrix (n x n).\n",
    "        '''\n",
    "\n",
    "\n",
    "        K = covSEard(hyp=[l1,l2,sigma_f], x=X_train, z='itself') + sigma_y**2 * np.eye(len(X_train))\n",
    "        K_s = covSEard(hyp=[l1,l2,sigma_f], x=X_train, z=X_s)\n",
    "        K_ss = covSEard(hyp=[l1,l2,sigma_f], x=X_s, z='itself')  + 1e-8 * np.eye(len(X_s))\n",
    "#         K_inv = inv(K)\n",
    "        K_inv = np.linalg.pinv(K)\n",
    "    \n",
    "        # Equation (4)\n",
    "        mu_s = K_s.T.dot(K_inv).dot(Y_train)\n",
    "\n",
    "        # Equation (5)\n",
    "        cov_s = K_ss - K_s.T.dot(K_inv).dot(K_s)\n",
    "        \n",
    "        return mu_s, cov_s\n",
    "\n",
    "\n",
    "    def nll_fn(X_train, Y_train, x_unlabeled, init_poro, noise=0, naive=False):\n",
    "        '''\n",
    "        Returns a function that computes the negative log marginal\n",
    "        likelihood for training data X_train and Y_train and given \n",
    "        noise level.\n",
    "\n",
    "        Args:\n",
    "            X_train: training locations (m x d).\n",
    "            Y_train: training targets (m x 1).\n",
    "            noise: known noise level of Y_train.\n",
    "            naive: if True use a naive implementation of Eq. (7), if \n",
    "                   False use a numerically more stable implementation. \n",
    "\n",
    "        Returns:\n",
    "            Minimization objective.\n",
    "        '''\n",
    "\n",
    "        def nll_stable(theta):\n",
    "            # Numerically more stable implementation of Eq. (7) as described\n",
    "            # in http://www.gaussianprocess.org/gpml/chapters/RW2.pdf, Section\n",
    "            # 2.2, Algorithm 2.1.\n",
    "            K = covSEard(hyp=[theta[0],theta[1],theta[2]], x=X_train, z=X_train) + \\\n",
    "                noise**2 * np.eye(len(X_train))\n",
    "            \n",
    "            \n",
    "            K += 1e-8 * np.eye(*K.shape)\n",
    "            L = cholesky(K)\n",
    "        \n",
    "\n",
    "            mu_un, _ = posterior_predictive(x_unlabeled, X_train, Y_train, l1=theta[0], l2=theta[1], sigma_f=theta[2])\n",
    "            phyloss_poro = np.mean(poros(init_poro, mu_un))\n",
    "\n",
    "            log_loss = np.sum(np.log(np.diagonal(L))) + \\\n",
    "                   0.5 * Y_train.T.dot(lstsq(L.T, lstsq(L, Y_train)[0])[0]) + \\\n",
    "                   0.5 * len(X_train) * np.log(2*np.pi)\n",
    "        \n",
    "            print(500000*phyloss_poro,log_loss, theta)\n",
    "            return 500000*phyloss_poro + log_loss\n",
    "\n",
    "        if naive:\n",
    "            return nll_naive\n",
    "        else:\n",
    "            return nll_stable\n",
    "\n",
    "    \n",
    "    # Optimization\n",
    "    res = minimize(nll_fn(trainX, trainY, x_unlabeled, init_poro), x0 = [.1, .1, .1], \n",
    "                   bounds=((1e-5, None), (1e-5, None), (1e-5, None)),\n",
    "                    method='L-BFGS-B')\n",
    "    \n",
    "#     print(f'After parameter optimization: l1={res.x[0]:.5f} l2={res.x[1]:.5f} sigma_f={res.x[2]:.5f}')\n",
    "#     print(np.exp(res.x[0]),np.exp(res.x[1]), np.exp(res.x[2]))\n",
    "    mu_s, cov_s = posterior_predictive(Xx, trainX, trainY, *res.x)\n",
    "    \n",
    "    samples = np.random.multivariate_normal(mu_s.ravel(), cov_s, nsim)\n",
    "#     print(\"RMSE:\", root_mean_squared_error(testY, samples))\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Normalize the data.\n",
    "from sklearn import preprocessing\n",
    "from numpy.linalg import cholesky, det, lstsq\n",
    "from scipy.optimize import minimize\n",
    "from numpy.linalg import inv\n",
    "import scipy.spatial.distance as spdist\n",
    "\n",
    "def pass_arg(Xx, nsim, tr_size):\n",
    "    \n",
    "    log = np.log\n",
    "    \n",
    "    # Compute the RMSE\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "        return np.sqrt(np.mean((y_pred-y_true)**2))\n",
    "\n",
    "    # Making sure final porosity is less than initial\n",
    "    def poros(poroi, porof):\n",
    "        porofn = -porof*(porof<0)\n",
    "        porofp = porof*(porof>=poroi) - poroi*(porof>=poroi)\n",
    "        return porofp+porofn\n",
    "\n",
    "\n",
    "    # Load labeled data\n",
    "    data = np.loadtxt('../data/labeled_data.dat')\n",
    "    x_labeled = data[:, :2].astype(np.float64) # -2 because we do not need porosity predictions\n",
    "    y_labeled = data[:, -2:-1].astype(np.float64) # dimensionless bond length and porosity measurements\n",
    "\n",
    "    # normalize dataset with MinMaxScaler\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "    x_labeled = scaler.fit_transform(x_labeled)\n",
    "    # y_labeled = scaler.fit_transform(y_labeled)\n",
    "\n",
    "    tr_size = tr_size\n",
    "\n",
    "    # train and test data\n",
    "    trainX, trainY = x_labeled[:tr_size,:], y_labeled[:tr_size]\n",
    "    testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]\n",
    "\n",
    "\n",
    "    # trainY = np.transpose(trainY)\n",
    "    # testY = np.transpose(testY)\n",
    "\n",
    "    data_phyloss = np.loadtxt('../data/unlabeled_data_BK_constw_v2_1525.dat')\n",
    "    x_unlabeled = data_phyloss[:, :]\n",
    "\n",
    "    # initial porosity\n",
    "    initporo = x_unlabeled[:, -1]\n",
    "\n",
    "    x_unlabeled1 = x_unlabeled[:1303, :2]\n",
    "    x_unlabeled2 = x_unlabeled[-6:, :2]\n",
    "    x_unlabeled = np.vstack((x_unlabeled1,x_unlabeled2))\n",
    "\n",
    "    x_unlabeled = scaler.fit_transform(x_unlabeled)\n",
    "    init_poro1 = initporo[:1303]\n",
    "    init_poro2 = initporo[-6:]\n",
    "    init_poro = np.hstack((init_poro1,init_poro2))\n",
    "\n",
    "    \n",
    "    \n",
    "    def covSEard(hyp=None, x=None, z=None):\n",
    "        ''' Squared Exponential covariance function with Automatic Relevance Detemination\n",
    "         (ARD) distance measure. The covariance function is parameterized as:\n",
    "\n",
    "         k(x^p,x^q) = sf2 * exp(-(x^p - x^q)' * inv(P) * (x^p - x^q)/2)\n",
    "\n",
    "         where the P matrix is diagonal with ARD parameters ell_1^2,...,ell_D^2, where\n",
    "         D is the dimension of the input space and sf2 is the signal variance.\n",
    "\n",
    "         The hyperparameters are:\n",
    "\n",
    "         hyp = [ log(ell_1)\n",
    "                 log(ell_2)\n",
    "                 ...\n",
    "                 log(ell_D)\n",
    "                 log(sqrt(sf2)) ]\n",
    "        '''\n",
    "        #     if hyp == None:                 # report number of parameters\n",
    "        #         return ['D + 1']            # USAGE: integer OR D_+_int (spaces are SIGNIFICANT)\n",
    "\n",
    "        [n, D] = x.shape\n",
    "        ell = 1/np.array(hyp[0:D])        # characteristic length scale\n",
    "        \n",
    "        \n",
    "        sf2 = np.array(hyp[D])**2         # signal variance\n",
    "        tmp = np.dot(np.diag(ell),x.T).T\n",
    "#         A = spdist.cdist(tmp, tmp, 'sqeuclidean')\n",
    "        \n",
    "        if z == 'itself':\n",
    "            tmp = np.dot(np.diag(ell),x.T).T\n",
    "            A = spdist.cdist(tmp, tmp, 'sqeuclidean')\n",
    "        else:                           # compute covariance between data sets x and z\n",
    "            A = spdist.cdist(np.dot(np.diag(ell),x.T).T, np.dot(np.diag(ell),z.T).T, 'sqeuclidean') # cross covariances\n",
    "\n",
    "        A = sf2*np.exp(-0.5*A)  \n",
    "\n",
    "        return A\n",
    "\n",
    "    \n",
    "#     def kernel(X1, X2, l=1.0, sigma_f=1.0):\n",
    "#         '''\n",
    "#         Isotropic squared exponential kernel. Computes \n",
    "#         a covariance matrix from points in X1 and X2.\n",
    "\n",
    "#         Args:\n",
    "#             X1: Array of m points (m x d).\n",
    "#             X2: Array of n points (n x d).\n",
    "\n",
    "#         Returns:\n",
    "#             Covariance matrix (m x n).\n",
    "#         '''\n",
    "#         sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "#         return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist)\n",
    "\n",
    "    def posterior_predictive(X_s, X_train, Y_train, l1=log(.1), l2=log(.1), sigma_f=log(.1), sigma_y=0):\n",
    "        '''  \n",
    "        Computes the suffifient statistics of the GP posterior predictive distribution \n",
    "        from m training data X_train and Y_train and n new inputs X_s.\n",
    "\n",
    "        Args:\n",
    "            X_s: New input locations (n x d).\n",
    "            X_train: Training locations (m x d).\n",
    "            Y_train: Training targets (m x 1).\n",
    "            l: Kernel length parameter.\n",
    "            sigma_f: Kernel vertical variation parameter.\n",
    "            sigma_y: Noise parameter.\n",
    "\n",
    "        Returns:\n",
    "            Posterior mean vector (n x d) and covariance matrix (n x n).\n",
    "        '''\n",
    "        \n",
    "#         K = kernel(X_train, X_train, l, sigma_f) + sigma_y**2 * np.eye(len(X_train))\n",
    "#         K_s = kernel(X_train, X_s, l, sigma_f)\n",
    "#         K_ss = kernel(X_s, X_s, l, sigma_f) + 1e-8 * np.eye(len(X_s))\n",
    "#         K_inv = inv(K)\n",
    "\n",
    "\n",
    "        K = covSEard(hyp=[l1,l2,sigma_f], x=X_train, z='itself') + sigma_y**2 * np.eye(len(X_train))\n",
    "        K_s = covSEard(hyp=[l1,l2,sigma_f], x=X_train, z=X_s)\n",
    "        K_ss = covSEard(hyp=[l1,l2,sigma_f], x=X_s, z='itself')  + 1e-8 * np.eye(len(X_s))\n",
    "#         K_inv = inv(K)\n",
    "        K_inv = np.linalg.pinv(K)\n",
    "    \n",
    "        # Equation (4)\n",
    "        mu_s = K_s.T.dot(K_inv).dot(Y_train)\n",
    "\n",
    "        # Equation (5)\n",
    "#         print(K_s.T.dot(K_inv).shape, K_s.T.dot(K_inv).dot(K_s).shape, K_ss.shape)\n",
    "        cov_s = K_ss - K_s.T.dot(K_inv).dot(K_s)\n",
    "        \n",
    "#         min_eig = np.min(np.real(np.linalg.eigvals(cov_s)))\n",
    "# #         print(\"eig:\",min_eig)\n",
    "#         if min_eig < 0:\n",
    "#             cov_s -= 10*min_eig * np.eye(*cov_s.shape)\n",
    "            \n",
    "#         cov_s += 1e8*min_eig * np.eye(*cov_s.shape)\n",
    "    \n",
    "        return mu_s, cov_s\n",
    "\n",
    "\n",
    "    def nll_fn(X_train, Y_train, x_unlabeled, init_poro, noise=0, naive=False):\n",
    "        '''\n",
    "        Returns a function that computes the negative log marginal\n",
    "        likelihood for training data X_train and Y_train and given \n",
    "        noise level.\n",
    "\n",
    "        Args:\n",
    "            X_train: training locations (m x d).\n",
    "            Y_train: training targets (m x 1).\n",
    "            noise: known noise level of Y_train.\n",
    "            naive: if True use a naive implementation of Eq. (7), if \n",
    "                   False use a numerically more stable implementation. \n",
    "\n",
    "        Returns:\n",
    "            Minimization objective.\n",
    "        '''\n",
    "#         def nll_naive(theta):\n",
    "#             # Naive implementation of Eq. (7). Works well for the examples \n",
    "#             # in this article but is numerically less stable compared to \n",
    "#             # the implementation in nll_stable below.\n",
    "#             K = kernel(X_train, X_train, l=theta[0], sigma_f=theta[1]) + \\\n",
    "#                 noise**2 * np.eye(len(X_train))\n",
    "#             return 0.5 * np.log(det(K)) + \\\n",
    "#                    0.5 * Y_train.T.dot(inv(K).dot(Y_train)) + \\\n",
    "#                    0.5 * len(X_train) * np.log(2*np.pi)\n",
    "\n",
    "        def nll_stable(theta):\n",
    "            # Numerically more stable implementation of Eq. (7) as described\n",
    "            # in http://www.gaussianprocess.org/gpml/chapters/RW2.pdf, Section\n",
    "            # 2.2, Algorithm 2.1.\n",
    "#             K = kernel(X_train, X_train, l=theta[0], sigma_f=theta[1]) + \\\n",
    "#                 noise**2 * np.eye(len(X_train)) \n",
    "            K = covSEard(hyp=[theta[0],theta[1],theta[2]], x=X_train, z=X_train) + \\\n",
    "                noise**2 * np.eye(len(X_train))\n",
    "            \n",
    "            \n",
    "            K += 1e-8 * np.eye(*K.shape)\n",
    "    #         print(np.linalg.eigvals(K))\n",
    "            L = cholesky(K)\n",
    "        \n",
    "\n",
    "            mu_un, _ = posterior_predictive(x_unlabeled, X_train, Y_train, l1=theta[0], l2=theta[1], sigma_f=theta[2])\n",
    "            phyloss_poro = np.mean(poros(init_poro, mu_un))\n",
    "            \n",
    "#             print(\"normal loss:\", np.sum(np.log(np.diagonal(L))) + \\\n",
    "#                    0.5 * Y_train.T.dot(lstsq(L.T, lstsq(L, Y_train)[0])[0]) + \\\n",
    "#                    0.5 * len(X_train) * np.log(2*np.pi))\n",
    "#             \n",
    "#             return np.sum(np.log(np.diagonal(L))) + \\\n",
    "#                    0.5 * Y_train.T.dot(lstsq(L.T, lstsq(L, Y_train)[0])[0]) + \\\n",
    "#                    0.5 * len(X_train) * np.log(2*np.pi)\n",
    "\n",
    "            log_loss = np.sum(np.log(np.diagonal(L))) + \\\n",
    "                   0.5 * Y_train.T.dot(lstsq(L.T, lstsq(L, Y_train)[0])[0]) + \\\n",
    "                   0.5 * len(X_train) * np.log(2*np.pi)\n",
    "            \n",
    "\n",
    "#             print(log_loss, theta)\n",
    "#             return log_loss\n",
    "        \n",
    "            print(500000*phyloss_poro,log_loss, theta)\n",
    "            return 500000*phyloss_poro + log_loss\n",
    "\n",
    "        if naive:\n",
    "            return nll_naive\n",
    "        else:\n",
    "            return nll_stable\n",
    "\n",
    "    \n",
    "    # Optimization\n",
    "    res = minimize(nll_fn(trainX, trainY, x_unlabeled, init_poro), x0 = [.1, .1, .1], \n",
    "                   bounds=((1e-5, None), (1e-5, None), (1e-5, None)),\n",
    "                    method='L-BFGS-B')\n",
    "    \n",
    "    print(f'After parameter optimization: l1={res.x[0]:.5f} l2={res.x[1]:.5f} sigma_f={res.x[2]:.5f}')\n",
    "    print(np.exp(res.x[0]),np.exp(res.x[1]), np.exp(res.x[2]))\n",
    "    mu_s, cov_s = posterior_predictive(Xx, trainX, trainY, *res.x)\n",
    "    \n",
    "    samples = np.random.multivariate_normal(mu_s.ravel(), cov_s, nsim)\n",
    "#     print(\"RMSE:\", root_mean_squared_error(testY, samples))\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berkc\\Miniconda3\\envs\\R\\lib\\site-packages\\ipykernel_launcher.py:94: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "C:\\Users\\berkc\\Miniconda3\\envs\\R\\lib\\site-packages\\ipykernel_launcher.py:219: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 [[26902.83334319]] [0.1 0.1 0.1]\n",
      "0.0 [[26902.83334299]] [0.10000001 0.1        0.1       ]\n",
      "0.0 [[26902.83334263]] [0.1        0.10000001 0.1       ]\n",
      "0.0 [[26902.83334491]] [0.1        0.1        0.10000001]\n",
      "11545.065826339196 [[27871.65239384]] [0.43935192 1.04065801 0.09833507]\n",
      "11545.060611781093 [[27871.65241651]] [0.43935193 1.04065801 0.09833507]\n",
      "11545.069846810109 [[27871.65241024]] [0.43935192 1.04065802 0.09833507]\n",
      "11545.06542184991 [[27871.65233146]] [0.43935192 1.04065801 0.09833508]\n",
      "2276.3439604011874 [[26902.68495692]] [0.20558832 0.39268287 0.09948196]\n",
      "2276.343883240979 [[26902.68495743]] [0.20558833 0.39268287 0.09948196]\n",
      "2276.3441482694934 [[26902.68496024]] [0.20558832 0.39268288 0.09948196]\n",
      "2276.3439604174528 [[26902.684955]] [0.20558832 0.39268287 0.09948197]\n",
      "0.0 [[26902.76473677]] [0.1004541  0.10125873 0.09999777]\n",
      "0.0 [[26902.764736]] [0.10045411 0.10125873 0.09999777]\n",
      "0.0 [[26902.76473531]] [0.1004541  0.10125874 0.09999777]\n",
      "0.0 [[26902.7647379]] [0.1004541  0.10125873 0.09999778]\n",
      "0.0 [[26902.57649896]] [0.101681   0.1046596  0.09999175]\n",
      "0.0 [[26902.57649866]] [0.10168101 0.1046596  0.09999175]\n",
      "0.0 [[26902.57649847]] [0.101681   0.10465961 0.09999175]\n",
      "0.0 [[26902.57650044]] [0.101681   0.1046596  0.09999176]\n",
      "325.0509173381828 [[26893.34140688]] [0.15363466 0.24867124 0.09973686]\n",
      "325.0509586829716 [[26893.34140657]] [0.15363467 0.24867124 0.09973686]\n",
      "325.0509771872538 [[26893.34140633]] [0.15363466 0.24867125 0.09973686]\n",
      "325.0509173379747 [[26893.34140839]] [0.15363466 0.24867124 0.09973687]\n",
      "0.0 [[26901.70407214]] [0.10707543 0.11961256 0.09996529]\n",
      "0.0 [[26901.70407202]] [0.10707544 0.11961256 0.09996529]\n",
      "0.0 [[26901.70407168]] [0.10707543 0.11961257 0.09996529]\n",
      "0.0 [[26901.70407397]] [0.10707543 0.11961256 0.0999653 ]\n",
      "685.4356991410549 [[1641126.49055073]] [1.53721622e-01 2.92804473e-01 1.00000000e-05]\n",
      "685.435772930731 [[1641126.48720962]] [1.53721632e-01 2.92804473e-01 1.00000000e-05]\n",
      "685.4358031045283 [[1641126.48943684]] [1.53721622e-01 2.92804483e-01 1.00000000e-05]\n",
      "685.4356991408001 [[1640930.93033104]] [1.53721622e-01 2.92804473e-01 1.00100000e-05]\n",
      "8.759318792567537 [[26891.53803854]] [0.12261149 0.17729614 0.06667401]\n",
      "8.759329051971172 [[26891.53803807]] [0.1226115  0.17729614 0.06667401]\n",
      "8.759325878485427 [[26891.53803807]] [0.12261149 0.17729615 0.06667401]\n",
      "8.759318792568966 [[26891.53804085]] [0.12261149 0.17729614 0.06667402]\n",
      "0.0 [[26900.20137594]] [0.10961764 0.1290515  0.09451774]\n",
      "0.0 [[26900.20137589]] [0.10961765 0.1290515  0.09451774]\n",
      "0.0 [[26900.20137542]] [0.10961764 0.12905151 0.09451774]\n",
      "0.0 [[26900.20137793]] [0.10961764 0.1290515  0.09451775]\n",
      "0.0 [[26894.9496172]] [0.11777536 0.15934015 0.07703707]\n",
      "0.0 [[26894.9496169]] [0.11777537 0.15934015 0.07703707]\n",
      "0.0 [[26894.94961677]] [0.11777536 0.15934016 0.07703707]\n",
      "0.0 [[26894.94961939]] [0.11777536 0.15934015 0.07703708]\n",
      "0.25433144991273443 [[26894.31032976]] [0.11870515 0.16279236 0.07504467]\n",
      "0.25433362595641723 [[26894.31032935]] [0.11870516 0.16279236 0.07504467]\n",
      "0.25433296257928345 [[26894.31032934]] [0.11870515 0.16279237 0.07504467]\n",
      "0.25433144991286694 [[26894.3103319]] [0.11870515 0.16279236 0.07504468]\n",
      "0.0 [[1669177.14333015]] [1.02492149e-01 2.01764410e-01 1.00000000e-05]\n",
      "0.0 [[1669177.14041475]] [1.02492159e-01 2.01764410e-01 1.00000000e-05]\n",
      "0.0 [[1669177.14184332]] [1.02492149e-01 2.01764420e-01 1.00000000e-05]\n",
      "0.0 [[1669029.77574739]] [1.02492149e-01 2.01764410e-01 1.00100000e-05]\n",
      "0.7657662710945011 [[26887.72171395]] [0.11330883 0.17576378 0.05007021]\n",
      "0.7657705309754647 [[26887.7217136]] [0.11330884 0.17576378 0.05007021]\n",
      "0.7657681981084116 [[26887.72171338]] [0.11330883 0.17576379 0.05007021]\n",
      "0.7657662710949966 [[26887.72171665]] [0.11330883 0.17576378 0.05007022]\n",
      "0.5131375372704248 [[26881.53551432]] [0.10790049 0.18876409 0.0250401 ]\n",
      "0.5131415076548744 [[26881.53551405]] [0.1079005  0.18876409 0.0250401 ]\n",
      "0.5131387036554732 [[26881.535514]] [0.10790049 0.1887641  0.0250401 ]\n",
      "0.513137537269415 [[26881.53551436]] [0.10790049 0.18876409 0.02504011]\n",
      "0.14284460638742305 [[1683828.43144897]] [5.25152099e-02 2.14484937e-01 1.00000000e-05]\n",
      "0.1428432578377217 [[1683828.42828008]] [5.25152199e-02 2.14484937e-01 1.00000000e-05]\n",
      "0.14284593807045062 [[1683828.43069245]] [5.25152099e-02 2.14484947e-01 1.00000000e-05]\n",
      "0.1428446063882624 [[1683707.52808045]] [5.25152099e-02 2.14484937e-01 1.00100000e-05]\n",
      "0.0 [[26885.68604187]] [0.08954077 0.19729032 0.01674285]\n",
      "0.0 [[26885.68604164]] [0.08954078 0.19729032 0.01674285]\n",
      "0.0 [[26885.68604184]] [0.08954077 0.19729033 0.01674285]\n",
      "0.0 [[26885.68602856]] [0.08954077 0.19729032 0.01674286]\n",
      "0.0 [[26881.78487441]] [0.10274649 0.1911576  0.02271087]\n",
      "0.0 [[26881.78487416]] [0.1027465  0.1911576  0.02271087]\n",
      "0.0 [[26881.78487415]] [0.10274649 0.19115761 0.02271087]\n",
      "0.0 [[26881.78487271]] [0.10274649 0.1911576  0.02271088]\n",
      "0.0 [[26889.80468935]] [0.0924386  0.17944608 0.0560389 ]\n",
      "0.0 [[26889.80468918]] [0.09243861 0.17944608 0.0560389 ]\n",
      "0.0 [[26889.80468892]] [0.0924386  0.17944609 0.0560389 ]\n",
      "0.0 [[26889.80469208]] [0.0924386  0.17944608 0.05603891]\n",
      "0.0 [[26881.73522848]] [0.10157778 0.18982974 0.02648963]\n",
      "0.0 [[26881.73522826]] [0.10157779 0.18982974 0.02648963]\n",
      "0.0 [[26881.73522816]] [0.10157778 0.18982975 0.02648963]\n",
      "0.0 [[26881.73522927]] [0.10157778 0.18982974 0.02648964]\n",
      "0.0 [[26881.54028027]] [0.10087758 0.1951979  0.02516471]\n",
      "0.0 [[26881.54027999]] [0.10087759 0.1951979  0.02516471]\n",
      "0.0 [[26881.54027999]] [0.10087758 0.19519791 0.02516471]\n",
      "0.0 [[26881.54028029]] [0.10087758 0.1951979  0.02516472]\n",
      "0.0 [[26881.4083491]] [0.09883827 0.20705904 0.02409179]\n",
      "0.0 [[26881.40834887]] [0.09883828 0.20705904 0.02409179]\n",
      "0.0 [[26881.40834897]] [0.09883827 0.20705905 0.02409179]\n",
      "0.0 [[26881.40834813]] [0.09883827 0.20705904 0.0240918 ]\n",
      "0.0 [[26881.33991805]] [0.09767435 0.21191332 0.0247516 ]\n",
      "0.0 [[26881.33991785]] [0.09767436 0.21191332 0.0247516 ]\n",
      "0.0 [[26881.33991801]] [0.09767435 0.21191333 0.0247516 ]\n",
      "0.0 [[26881.33991741]] [0.09767435 0.21191332 0.02475161]\n",
      "0.0 [[26881.30766732]] [0.0976661  0.21195963 0.02581796]\n",
      "0.0 [[26881.30766715]] [0.09766611 0.21195963 0.02581796]\n",
      "0.0 [[26881.30766723]] [0.0976661  0.21195964 0.02581796]\n",
      "0.0 [[26881.30766731]] [0.0976661  0.21195963 0.02581797]\n",
      "0.0 [[26881.29382823]] [0.09736019 0.21476167 0.02610171]\n",
      "0.0 [[26881.29382804]] [0.0973602  0.21476167 0.02610171]\n",
      "0.0 [[26881.29382817]] [0.09736019 0.21476168 0.02610171]\n",
      "0.0 [[26881.29382829]] [0.09736019 0.21476167 0.02610172]\n",
      "0.0 [[26881.28240892]] [0.09739575 0.21943094 0.026303  ]\n",
      "0.0 [[26881.28240871]] [0.09739576 0.21943094 0.026303  ]\n",
      "0.0 [[26881.28240889]] [0.09739575 0.21943095 0.026303  ]\n",
      "0.0 [[26881.28240888]] [0.09739575 0.21943094 0.02630301]\n",
      "0.0 [[26881.32219693]] [0.09814152 0.22682073 0.02686354]\n",
      "0.0 [[26881.32219677]] [0.09814153 0.22682073 0.02686354]\n",
      "0.0 [[26881.32219707]] [0.09814152 0.22682074 0.02686354]\n",
      "0.0 [[26881.32219692]] [0.09814152 0.22682073 0.02686355]\n",
      "0.0 [[26881.28322733]] [0.09754814 0.22094097 0.02641754]\n",
      "0.0 [[26881.28322714]] [0.09754815 0.22094097 0.02641754]\n",
      "0.0 [[26881.28322734]] [0.09754814 0.22094098 0.02641754]\n",
      "0.0 [[26881.28322732]] [0.09754814 0.22094097 0.02641755]\n",
      "0.0 [[26881.28227526]] [0.09743336 0.21980358 0.02633127]\n",
      "0.0 [[26881.28227505]] [0.09743337 0.21980358 0.02633127]\n",
      "0.0 [[26881.28227525]] [0.09743336 0.21980359 0.02633127]\n",
      "0.0 [[26881.28227522]] [0.09743336 0.21980358 0.02633128]\n",
      "0.0 [[26881.26575999]] [0.09841486 0.22031203 0.02639108]\n",
      "0.0 [[26881.26575982]] [0.09841487 0.22031203 0.02639108]\n",
      "0.0 [[26881.26576001]] [0.09841486 0.22031204 0.02639108]\n",
      "0.0 [[26881.26576001]] [0.09841486 0.22031203 0.02639109]\n",
      "1.5901748764168149 [[26881.20209104]] [0.1024554  0.22056946 0.0262095 ]\n",
      "1.5901839823593953 [[26881.20209089]] [0.10245541 0.22056946 0.0262095 ]\n",
      "1.590176396941076 [[26881.20209109]] [0.1024554  0.22056947 0.0262095 ]\n",
      "1.5901748764158372 [[26881.20209087]] [0.1024554  0.22056946 0.02620951]\n",
      "0.0 [[26881.26388352]] [0.09852198 0.22031885 0.02638627]\n",
      "0.0 [[26881.26388335]] [0.09852199 0.22031885 0.02638627]\n",
      "0.0 [[26881.26388355]] [0.09852198 0.22031886 0.02638627]\n",
      "0.0 [[26881.26388352]] [0.09852198 0.22031885 0.02638628]\n",
      "0.0 [[26881.26192652]] [0.09863435 0.22032601 0.02638122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 [[26881.26192636]] [0.09863436 0.22032601 0.02638122]\n",
      "0.0 [[26881.26192654]] [0.09863435 0.22032602 0.02638122]\n",
      "0.0 [[26881.26192654]] [0.09863435 0.22032601 0.02638123]\n",
      "0.1965968556512684 [[26881.23040025]] [0.10054487 0.22044773 0.02629536]\n",
      "0.19660041339532455 [[26881.2304001]] [0.10054488 0.22044773 0.02629536]\n",
      "0.19659737205310085 [[26881.23040028]] [0.10054487 0.22044774 0.02629536]\n",
      "0.196596855652037 [[26881.23040016]] [0.10054487 0.22044773 0.02629537]\n",
      "0.0 [[26881.2551953]] [0.09902602 0.22035096 0.02636361]\n",
      "0.0 [[26881.25519508]] [0.09902603 0.22035096 0.02636361]\n",
      "0.0 [[26881.2551953]] [0.09902602 0.22035097 0.02636361]\n",
      "0.0 [[26881.25519522]] [0.09902602 0.22035096 0.02636362]\n",
      "0.0 [[26881.25081498]] [0.09928536 0.22036749 0.02635196]\n",
      "0.0 [[26881.25081483]] [0.09928537 0.22036749 0.02635196]\n",
      "0.0 [[26881.25081504]] [0.09928536 0.2203675  0.02635196]\n",
      "0.0 [[26881.25081498]] [0.09928536 0.22036749 0.02635197]\n",
      "35.17696992734038 [[26881.00796764]] [0.11649679 0.2165704  0.02568515]\n",
      "35.1770039444188 [[26881.00796751]] [0.1164968  0.2165704  0.02568515]\n",
      "35.176981985403785 [[26881.00796773]] [0.11649679 0.21657041 0.02568515]\n",
      "35.176969927340004 [[26881.00796704]] [0.11649679 0.2165704  0.02568516]\n",
      "0.0 [[26881.24996426]] [0.09933326 0.22035692 0.0263501 ]\n",
      "0.0 [[26881.24996406]] [0.09933327 0.22035692 0.0263501 ]\n",
      "0.0 [[26881.24996426]] [0.09933326 0.22035693 0.0263501 ]\n",
      "0.0 [[26881.24996422]] [0.09933326 0.22035692 0.02635011]\n",
      "0.0 [[26881.24898967]] [0.09938821 0.2203448  0.02634798]\n",
      "0.0 [[26881.24898952]] [0.09938822 0.2203448  0.02634798]\n",
      "0.0 [[26881.24898972]] [0.09938821 0.22034481 0.02634798]\n",
      "0.0 [[26881.24898965]] [0.09938821 0.2203448  0.02634799]\n",
      "10.575059011909666 [[26881.11532053]] [0.1079425  0.2184576  0.02601656]\n",
      "10.575083219351617 [[26881.11532039]] [0.10794251 0.2184576  0.02601656]\n",
      "10.575064557875864 [[26881.11532058]] [0.1079425  0.21845761 0.02601656]\n",
      "10.57505901189999 [[26881.11532024]] [0.1079425  0.2184576  0.02601657]\n",
      "0.0 [[26881.24810879]] [0.09943798 0.22033382 0.02634605]\n",
      "0.0 [[26881.24810857]] [0.09943799 0.22033382 0.02634605]\n",
      "0.0 [[26881.2481088]] [0.09943798 0.22033383 0.02634605]\n",
      "0.0 [[26881.24810867]] [0.09943798 0.22033382 0.02634606]\n",
      "0.0 [[26881.24698506]] [0.09950157 0.22031979 0.02634358]\n",
      "0.0 [[26881.24698483]] [0.09950158 0.22031979 0.02634358]\n",
      "0.0 [[26881.24698506]] [0.09950157 0.2203198  0.02634358]\n",
      "0.0 [[26881.24698497]] [0.09950157 0.22031979 0.02634359]\n",
      "2.8412385359340817 [[26881.17718173]] [0.10372204 0.21938869 0.02618007]\n",
      "2.8412516396968432 [[26881.17718158]] [0.10372205 0.21938869 0.02618007]\n",
      "2.8412409186930647 [[26881.17718176]] [0.10372204 0.2193887  0.02618007]\n",
      "2.8412385359320567 [[26881.17718159]] [0.10372204 0.21938869 0.02618008]\n",
      "0.0 [[26881.24585715]] [0.09956553 0.22030568 0.02634111]\n",
      "0.0 [[26881.24585698]] [0.09956554 0.22030568 0.02634111]\n",
      "0.0 [[26881.24585713]] [0.09956553 0.22030569 0.02634111]\n",
      "0.0 [[26881.24585709]] [0.09956553 0.22030568 0.02634112]\n",
      "0.00017481800493355956 [[26881.24348028]] [0.09970076 0.22027584 0.02633587]\n",
      "0.0001759819083652187 [[26881.24348008]] [0.09970077 0.22027584 0.02633587]\n",
      "0.0001749857653072068 [[26881.24348028]] [0.09970076 0.22027585 0.02633587]\n",
      "0.00017481800479573584 [[26881.2434802]] [0.09970076 0.22027584 0.02633588]\n",
      "0.0 [[26881.24550925]] [0.09958529 0.22030132 0.02634034]\n",
      "0.0 [[26881.2455091]] [0.0995853  0.22030132 0.02634034]\n",
      "0.0 [[26881.24550933]] [0.09958529 0.22030133 0.02634034]\n",
      "0.0 [[26881.24550924]] [0.09958529 0.22030132 0.02634035]\n",
      "0.0 [[26881.24402328]] [0.09966981 0.22028267 0.02633707]\n",
      "0.0 [[26881.24402309]] [0.09966982 0.22028267 0.02633707]\n",
      "0.0 [[26881.24402329]] [0.09966981 0.22028268 0.02633707]\n",
      "0.0 [[26881.24402323]] [0.09966981 0.22028267 0.02633708]\n",
      "0.00017481800493355956 [[26881.24348028]] [0.09970076 0.22027584 0.02633587]\n",
      "0.0001759819083652187 [[26881.24348008]] [0.09970077 0.22027584 0.02633587]\n",
      "0.0001749857653072068 [[26881.24348028]] [0.09970076 0.22027585 0.02633587]\n",
      "0.00017481800479573584 [[26881.2434802]] [0.09970076 0.22027584 0.02633588]\n",
      "0.0 [[26881.24773831]] [0.09938769 0.21991758 0.02631935]\n",
      "0.0 [[26881.24773815]] [0.0993877  0.21991758 0.02631935]\n",
      "0.0 [[26881.24773832]] [0.09938769 0.21991759 0.02631935]\n",
      "0.0 [[26881.24773831]] [0.09938769 0.21991758 0.02631936]\n",
      "0.0 [[26881.24467232]] [0.09961055 0.22017261 0.02633111]\n",
      "0.0 [[26881.2446721]] [0.09961056 0.22017261 0.02633111]\n",
      "0.0 [[26881.24467231]] [0.09961055 0.22017262 0.02633111]\n",
      "0.0 [[26881.24467223]] [0.09961055 0.22017261 0.02633112]\n",
      "0.0 [[26881.2438432]] [0.09967306 0.22024415 0.0263344 ]\n",
      "0.0 [[26881.24384305]] [0.09967307 0.22024415 0.0263344 ]\n",
      "0.0 [[26881.24384321]] [0.09967306 0.22024416 0.0263344 ]\n",
      "0.0 [[26881.24384318]] [0.09967306 0.22024415 0.02633441]\n",
      "0.0 [[26881.24359447]] [0.09969202 0.22026585 0.02633541]\n",
      "0.0 [[26881.24359426]] [0.09969203 0.22026585 0.02633541]\n",
      "0.0 [[26881.24359448]] [0.09969202 0.22026586 0.02633541]\n",
      "0.0 [[26881.2435944]] [0.09969202 0.22026585 0.02633542]\n",
      "0.0 [[26881.24224319]] [0.09972229 0.22004654 0.02639561]\n",
      "0.0 [[26881.24224299]] [0.0997223  0.22004654 0.02639561]\n",
      "0.0 [[26881.24224317]] [0.09972229 0.22004655 0.02639561]\n",
      "0.0 [[26881.2422432]] [0.09972229 0.22004654 0.02639562]\n",
      "0.0 [[26881.24200154]] [0.09972949 0.22000557 0.02639595]\n",
      "0.0 [[26881.24200137]] [0.0997295  0.22000557 0.02639595]\n",
      "0.0 [[26881.24200157]] [0.09972949 0.22000558 0.02639595]\n",
      "0.0 [[26881.24200154]] [0.09972949 0.22000557 0.02639596]\n",
      "0.0 [[26881.24106375]] [0.0997583  0.21984169 0.02639732]\n",
      "0.0 [[26881.24106364]] [0.09975831 0.21984169 0.02639732]\n",
      "0.0 [[26881.24106378]] [0.0997583  0.2198417  0.02639732]\n",
      "0.0 [[26881.24106378]] [0.0997583  0.21984169 0.02639733]\n",
      "0.001782379535516904 [[26881.23776755]] [0.09987355 0.21918618 0.02640278]\n",
      "0.0017835292123342364 [[26881.23776736]] [0.09987356 0.21918618 0.02640278]\n",
      "0.0017825491242943958 [[26881.23776757]] [0.09987355 0.21918619 0.02640278]\n",
      "0.0017823795355964176 [[26881.23776754]] [0.09987355 0.21918618 0.02640279]\n",
      "0.00889123005270442 [[26881.2316089]] [0.10033453 0.2165641  0.02642461]\n",
      "0.008892345335470822 [[26881.23160867]] [0.10033454 0.2165641  0.02642461]\n",
      "0.008891404388941659 [[26881.23160888]] [0.10033453 0.21656411 0.02642461]\n",
      "0.008891230052675265 [[26881.23160903]] [0.10033453 0.2165641  0.02642462]\n",
      "0.008242554762833393 [[26881.23038205]] [0.10041874 0.21599126 0.02628923]\n",
      "0.008243662635107597 [[26881.23038189]] [0.10041875 0.21599126 0.02628923]\n",
      "0.008242729897446299 [[26881.23038203]] [0.10041874 0.21599127 0.02628923]\n",
      "0.008242554762942061 [[26881.23038215]] [0.10041874 0.21599126 0.02628924]\n",
      "0.0 [[26881.29391009]] [0.09988746 0.2180078  0.02483682]\n",
      "0.0 [[26881.29390994]] [0.09988747 0.2180078  0.02483682]\n",
      "0.0 [[26881.29391016]] [0.09988746 0.21800781 0.02483682]\n",
      "0.0 [[26881.29390926]] [0.09988746 0.2180078  0.02483683]\n",
      "0.0037733574693473004 [[26881.23113397]] [0.10031892 0.21637015 0.02601633]\n",
      "0.003774470477577147 [[26881.2311338]] [0.10031893 0.21637015 0.02601633]\n",
      "0.0037735315410610745 [[26881.23113398]] [0.10031892 0.21637016 0.02601633]\n",
      "0.0037733574696865585 [[26881.2311339]] [0.10031892 0.21637015 0.02601634]\n",
      "0.0 [[26881.23459333]] [0.10047875 0.21508185 0.02574776]\n",
      "0.0 [[26881.23459314]] [0.10047876 0.21508185 0.02574776]\n",
      "0.0 [[26881.2345933]] [0.10047875 0.21508186 0.02574776]\n",
      "0.0 [[26881.23459316]] [0.10047875 0.21508185 0.02574777]\n",
      "0.0011856974149809722 [[26881.23147672]] [0.10044775 0.21540564 0.02596997]\n",
      "0.0011867980261903916 [[26881.23147654]] [0.10044776 0.21540564 0.02596997]\n",
      "0.0011858726596858714 [[26881.23147669]] [0.10044775 0.21540565 0.02596997]\n",
      "0.001185697414827246 [[26881.23147665]] [0.10044775 0.21540564 0.02596998]\n",
      "0.0 [[26881.23125602]] [0.1003794  0.21573243 0.02608142]\n",
      "0.0 [[26881.23125589]] [0.10037941 0.21573243 0.02608142]\n",
      "0.0 [[26881.231256]] [0.1003794  0.21573244 0.02608142]\n",
      "0.0 [[26881.23125604]] [0.1003794  0.21573243 0.02608143]\n",
      "0.0 [[26881.23132127]] [0.10041092 0.21554772 0.02605028]\n",
      "0.0 [[26881.23132109]] [0.10041093 0.21554772 0.02605028]\n",
      "0.0 [[26881.23132126]] [0.10041092 0.21554773 0.02605028]\n",
      "0.0 [[26881.23132124]] [0.10041092 0.21554772 0.02605029]\n",
      "0.0 [[26881.23125953]] [0.10038248 0.21571441 0.02607839]\n",
      "0.0 [[26881.23125936]] [0.10038249 0.21571441 0.02607839]\n",
      "0.0 [[26881.23125954]] [0.10038248 0.21571442 0.02607839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 [[26881.23125954]] [0.10038248 0.21571441 0.0260784 ]\n",
      "0.0 [[26881.23125626]] [0.10037962 0.21573118 0.02608121]\n",
      "0.0 [[26881.23125609]] [0.10037963 0.21573118 0.02608121]\n",
      "0.0 [[26881.23125624]] [0.10037962 0.21573119 0.02608121]\n",
      "0.0 [[26881.2312562]] [0.10037962 0.21573118 0.02608122]\n",
      "0.0 [[26881.2312561]] [0.10037944 0.21573223 0.02608139]\n",
      "0.0 [[26881.23125591]] [0.10037945 0.21573223 0.02608139]\n",
      "0.0 [[26881.23125608]] [0.10037944 0.21573224 0.02608139]\n",
      "0.0 [[26881.23125608]] [0.10037944 0.21573223 0.0260814 ]\n",
      "0.0 [[26881.23125601]] [0.10037941 0.21573241 0.02608142]\n",
      "0.0 [[26881.23125587]] [0.10037942 0.21573241 0.02608142]\n",
      "0.0 [[26881.23125601]] [0.10037941 0.21573242 0.02608142]\n",
      "0.0 [[26881.23125605]] [0.10037941 0.21573241 0.02608143]\n",
      "0.0 [[26881.23125605]] [0.10037942 0.21573237 0.02608141]\n",
      "0.0 [[26881.23125589]] [0.10037943 0.21573237 0.02608141]\n",
      "0.0 [[26881.23125604]] [0.10037942 0.21573238 0.02608141]\n",
      "0.0 [[26881.23125604]] [0.10037942 0.21573237 0.02608142]\n",
      "0.0 [[26881.23125604]] [0.10037941 0.2157324  0.02608142]\n",
      "0.0 [[26881.23125589]] [0.10037942 0.2157324  0.02608142]\n",
      "0.0 [[26881.23125603]] [0.10037941 0.21573241 0.02608142]\n",
      "0.0 [[26881.23125605]] [0.10037941 0.2157324  0.02608143]\n",
      "0.0 [[26881.23125605]] [0.10037941 0.21573241 0.02608142]\n",
      "0.0 [[26881.23125585]] [0.10037942 0.21573241 0.02608142]\n",
      "0.0 [[26881.23125603]] [0.10037941 0.21573242 0.02608142]\n",
      "0.0 [[26881.23125604]] [0.10037941 0.21573241 0.02608143]\n",
      "0.0 [[26881.23125601]] [0.10037941 0.21573241 0.02608142]\n",
      "0.0 [[26881.23125587]] [0.10037942 0.21573241 0.02608142]\n",
      "0.0 [[26881.23125601]] [0.10037941 0.21573242 0.02608142]\n",
      "0.0 [[26881.23125605]] [0.10037941 0.21573241 0.02608143]\n",
      "After parameter optimization: l1=0.10038 l2=0.21573 sigma_f=0.02608\n",
      "1.1055903090280317 1.2407703153499692 1.026424516599678\n"
     ]
    }
   ],
   "source": [
    "Xx = np.random.uniform(size=(100, 2))\n",
    "ss = pass_arg(Xx, 1, 30)\n",
    "# print(ss)\n",
    "# print(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covSEard(hyp=None, x=None, z=None):\n",
    "    ''' Squared Exponential covariance function with Automatic Relevance Detemination\n",
    "     (ARD) distance measure. The covariance function is parameterized as:\n",
    "\n",
    "     k(x^p,x^q) = sf2 * exp(-(x^p - x^q)' * inv(P) * (x^p - x^q)/2)\n",
    "\n",
    "     where the P matrix is diagonal with ARD parameters ell_1^2,...,ell_D^2, where\n",
    "     D is the dimension of the input space and sf2 is the signal variance.\n",
    "\n",
    "     The hyperparameters are:\n",
    "\n",
    "     hyp = [ log(ell_1)\n",
    "             log(ell_2)\n",
    "             ...\n",
    "             log(ell_D)\n",
    "             log(sqrt(sf2)) ]\n",
    "    '''\n",
    "    #     if hyp == None:                 # report number of parameters\n",
    "    #         return ['D + 1']            # USAGE: integer OR D_+_int (spaces are SIGNIFICANT)\n",
    "\n",
    "    [n, D] = x.shape\n",
    "    ell = 1/np.exp(hyp[0:D])        # characteristic length scale\n",
    "\n",
    "\n",
    "    sf2 = np.exp(2.*hyp[D])         # signal variance\n",
    "    tmp = np.dot(np.diag(ell),x.T).T\n",
    "#         A = spdist.cdist(tmp, tmp, 'sqeuclidean')\n",
    "\n",
    "    if z == 'itself':\n",
    "        tmp = np.dot(np.diag(ell),x.T).T\n",
    "        A = spdist.cdist(tmp, tmp, 'sqeuclidean')\n",
    "    else:                           # compute covariance between data sets x and z\n",
    "        A = spdist.cdist(np.dot(np.diag(ell),x.T).T, np.dot(np.diag(ell),z.T).T, 'sqeuclidean') # cross covariances\n",
    "\n",
    "    A = sf2*np.exp(-0.5*A)  \n",
    "\n",
    "    return A\n",
    "\n",
    "\n",
    "#     def kernel(X1, X2, l=1.0, sigma_f=1.0):\n",
    "#         '''\n",
    "#         Isotropic squared exponential kernel. Computes \n",
    "#         a covariance matrix from points in X1 and X2.\n",
    "\n",
    "#         Args:\n",
    "#             X1: Array of m points (m x d).\n",
    "#             X2: Array of n points (n x d).\n",
    "\n",
    "#         Returns:\n",
    "#             Covariance matrix (m x n).\n",
    "#         '''\n",
    "#         sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "#         return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist)\n",
    "\n",
    "def posterior_predictive(X_s, X_train, Y_train, l1=0.5, l2=0.5, sigma_f=1.0, sigma_y=0):\n",
    "    '''  \n",
    "    Computes the suffifient statistics of the GP posterior predictive distribution \n",
    "    from m training data X_train and Y_train and n new inputs X_s.\n",
    "\n",
    "    Args:\n",
    "        X_s: New input locations (n x d).\n",
    "        X_train: Training locations (m x d).\n",
    "        Y_train: Training targets (m x 1).\n",
    "        l: Kernel length parameter.\n",
    "        sigma_f: Kernel vertical variation parameter.\n",
    "        sigma_y: Noise parameter.\n",
    "\n",
    "    Returns:\n",
    "        Posterior mean vector (n x d) and covariance matrix (n x n).\n",
    "    '''\n",
    "\n",
    "#         K = kernel(X_train, X_train, l, sigma_f) + sigma_y**2 * np.eye(len(X_train))\n",
    "#         K_s = kernel(X_train, X_s, l, sigma_f)\n",
    "#         K_ss = kernel(X_s, X_s, l, sigma_f) + 1e-8 * np.eye(len(X_s))\n",
    "#         K_inv = inv(K)\n",
    "\n",
    "\n",
    "    K = covSEard(hyp=[l1,l2,sigma_f], x=X_train, z='itself') + sigma_y**2 * np.eye(len(X_train))\n",
    "    K_s = covSEard(hyp=[l1,l2,sigma_f], x=X_train, z=X_s)\n",
    "    K_ss = covSEard(hyp=[l1,l2,sigma_f], x=X_s, z='itself')  + 1e-8 * np.eye(len(X_s))\n",
    "#         K_inv = inv(K)\n",
    "    K_inv = np.linalg.pinv(K)\n",
    "\n",
    "    # Equation (4)\n",
    "    mu_s = K_s.T.dot(K_inv).dot(Y_train)\n",
    "\n",
    "    # Equation (5)\n",
    "#         print(K_s.T.dot(K_inv).shape, K_s.T.dot(K_inv).dot(K_s).shape, K_ss.shape)\n",
    "    cov_s = K_ss - K_s.T.dot(K_inv).dot(K_s)\n",
    "\n",
    "#         min_eig = np.min(np.real(np.linalg.eigvals(cov_s)))\n",
    "# #         print(\"eig:\",min_eig)\n",
    "#         if min_eig < 0:\n",
    "#             cov_s -= 10*min_eig * np.eye(*cov_s.shape)\n",
    "\n",
    "#         cov_s += 1e8*min_eig * np.eye(*cov_s.shape)\n",
    "\n",
    "    return mu_s, cov_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(autograph=False, experimental_compile=False)\n",
    "def target_log_prob(amplitude, length_scale, poroi, lam):\n",
    "    tf.random.set_seed(1234)\n",
    "    se_kernel = tfk.ExponentiatedQuadratic(amplitude)  # length_scale = None here, implicitly\n",
    "    optimized_kernel = InputScaledKernel(se_kernel, length_scale)\n",
    "    gprm = tfd.GaussianProcessRegressionModel(kernel=optimized_kernel, index_points = x_unlabeled)\n",
    "    samples = gprm.sample(1)\n",
    "    pred = tf.squeeze(samples, axis=0)\n",
    "\n",
    "    phyloss_poro = tf.math.reduce_mean(tf.nn.relu(tf.negative(pred))+tf.nn.relu(pred-poroi))\n",
    "\n",
    "#     print(\"phyloss_poro:\",lam*phyloss_poro)\n",
    "#     return lam*phyloss_poro\n",
    "    return lam*phyloss_poro - gp_joint_model.log_prob({\n",
    "      'amplitude': amplitude,\n",
    "      'length_scale': length_scale,\n",
    "      'observations': trainY\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss_inloop: tf.Tensor(97327.17451122966, shape=(), dtype=float64)\n",
      "1 loss_inloop: tf.Tensor(27517.653673109533, shape=(), dtype=float64)\n",
      "2 loss_inloop: tf.Tensor(26125.867318869172, shape=(), dtype=float64)\n",
      "3 loss_inloop: tf.Tensor(16993.676754054417, shape=(), dtype=float64)\n",
      "4 loss_inloop: tf.Tensor(63578.97302255117, shape=(), dtype=float64)\n",
      "5 loss_inloop: tf.Tensor(3150.835495812226, shape=(), dtype=float64)\n",
      "6 loss_inloop: tf.Tensor(23689.326876749325, shape=(), dtype=float64)\n",
      "7 loss_inloop: tf.Tensor(26075.86432005829, shape=(), dtype=float64)\n",
      "8 loss_inloop: tf.Tensor(33741.71708333892, shape=(), dtype=float64)\n",
      "9 loss_inloop: tf.Tensor(35355.67059762244, shape=(), dtype=float64)\n",
      "10 loss_inloop: tf.Tensor(17614.334487859152, shape=(), dtype=float64)\n",
      "11 loss_inloop: tf.Tensor(43646.772000166406, shape=(), dtype=float64)\n",
      "12 loss_inloop: tf.Tensor(14456.26112218793, shape=(), dtype=float64)\n",
      "13 loss_inloop: tf.Tensor(19869.55961840706, shape=(), dtype=float64)\n",
      "14 loss_inloop: tf.Tensor(27157.113070819923, shape=(), dtype=float64)\n",
      "15 loss_inloop: tf.Tensor(22702.13329835835, shape=(), dtype=float64)\n",
      "16 loss_inloop: tf.Tensor(3567.7264449614204, shape=(), dtype=float64)\n",
      "17 loss_inloop: tf.Tensor(9866.976109664196, shape=(), dtype=float64)\n",
      "18 loss_inloop: tf.Tensor(19225.984733324294, shape=(), dtype=float64)\n",
      "19 loss_inloop: tf.Tensor(13203.337507448356, shape=(), dtype=float64)\n",
      "20 loss_inloop: tf.Tensor(1293.67803109731, shape=(), dtype=float64)\n",
      "21 loss_inloop: tf.Tensor(13518.000383230443, shape=(), dtype=float64)\n",
      "22 loss_inloop: tf.Tensor(5009.1633233610955, shape=(), dtype=float64)\n",
      "23 loss_inloop: tf.Tensor(277.64999388241733, shape=(), dtype=float64)\n",
      "24 loss_inloop: tf.Tensor(7825.713764976132, shape=(), dtype=float64)\n",
      "25 loss_inloop: tf.Tensor(1759.1652639855802, shape=(), dtype=float64)\n",
      "26 loss_inloop: tf.Tensor(14773.369412708786, shape=(), dtype=float64)\n",
      "27 loss_inloop: tf.Tensor(1765.7540772545804, shape=(), dtype=float64)\n",
      "28 loss_inloop: tf.Tensor(5333.5747779841795, shape=(), dtype=float64)\n",
      "29 loss_inloop: tf.Tensor(6760.073662466977, shape=(), dtype=float64)\n",
      "30 loss_inloop: tf.Tensor(1512.9215296156717, shape=(), dtype=float64)\n",
      "31 loss_inloop: tf.Tensor(2654.971829314036, shape=(), dtype=float64)\n",
      "32 loss_inloop: tf.Tensor(4082.207350195424, shape=(), dtype=float64)\n",
      "33 loss_inloop: tf.Tensor(5899.132830379488, shape=(), dtype=float64)\n",
      "34 loss_inloop: tf.Tensor(305.5257969773975, shape=(), dtype=float64)\n",
      "35 loss_inloop: tf.Tensor(2774.96189281477, shape=(), dtype=float64)\n",
      "36 loss_inloop: tf.Tensor(4672.3478895588105, shape=(), dtype=float64)\n",
      "37 loss_inloop: tf.Tensor(910.9701442382734, shape=(), dtype=float64)\n",
      "38 loss_inloop: tf.Tensor(4687.600505674522, shape=(), dtype=float64)\n",
      "39 loss_inloop: tf.Tensor(1024.8677256371059, shape=(), dtype=float64)\n",
      "Trained parameters:\n",
      "amplitude: [0.07632151]\n",
      "length_scale: [0.76986674 0.18299228]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1234)\n",
    "\n",
    "# Optimize the model parameters.\n",
    "num_iters = 40\n",
    "lam = 100000\n",
    "optimizer = tf.optimizers.Adam(learning_rate=.1)\n",
    "\n",
    "# Store the likelihood values during training, so we can plot the progress\n",
    "lls_ = np.zeros(num_iters, np.float64)\n",
    "\n",
    "for i in range(num_iters):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = target_log_prob(amplitude_var, length_scale_var, init_poro, lam) # physics loss & normal loss\n",
    "\n",
    "\n",
    "    print(i,\"loss_inloop:\",loss)\n",
    "    grads = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "    lls_[i] = loss\n",
    "\n",
    "print('Trained parameters:')\n",
    "print('amplitude: {}'.format(amplitude_var._value().numpy()))\n",
    "print('length_scale: {}'.format(length_scale_var._value().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "tfk = tfp.math.psd_kernels\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def pass_arg(Xx, nsim, tr_size):\n",
    "    print(\"Tr_size:\", tr_size)\n",
    "    def fix_seeds(seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "        sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    #     K.set_session(sess)\n",
    "        tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "    ss = 1\n",
    "    fix_seeds(ss)\n",
    "\n",
    "    # Compute the RMSE given the ground truth (y_true) and the predictions(y_pred)\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "            return tf.math.sqrt(tf.math.reduce_mean(tf.math.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "    class InputTransformedKernel(tfp.math.psd_kernels.PositiveSemidefiniteKernel):\n",
    "\n",
    "        def __init__(self, kernel, transformation, name='InputTransformedKernel'):\n",
    "            self._kernel = kernel\n",
    "            self._transformation = transformation\n",
    "            super(InputTransformedKernel, self).__init__(\n",
    "                feature_ndims=kernel.feature_ndims,\n",
    "                dtype=kernel.dtype,\n",
    "                name=name)\n",
    "\n",
    "        def apply(self, x1, x2):\n",
    "            return self._kernel.apply(\n",
    "                self._transformation(x1),\n",
    "                self._transformation(x2))\n",
    "\n",
    "        def matrix(self, x1, x2):\n",
    "            return self._kernel.matrix(\n",
    "                self._transformation(x1),\n",
    "                self._transformation(x2))\n",
    "\n",
    "        @property\n",
    "        def batch_shape(self):\n",
    "            return self._kernel.batch_shape\n",
    "\n",
    "        def batch_shape_tensor(self):\n",
    "            return self._kernel.batch_shape_tensor\n",
    "\n",
    "    class InputScaledKernel(InputTransformedKernel):\n",
    "\n",
    "        def __init__(self, kernel, length_scales):\n",
    "            super(InputScaledKernel, self).__init__(\n",
    "                kernel,\n",
    "                lambda x: x / tf.expand_dims(length_scales,\n",
    "                                         -(kernel.feature_ndims + 1)))\n",
    "\n",
    "    # Load labeled data\n",
    "    data = np.loadtxt('../data/labeled_data.dat')\n",
    "    x_labeled = data[:, :2].astype(np.float64) # -2 because we do not need porosity predictions\n",
    "    y_labeled = data[:, -2:-1].astype(np.float64) # dimensionless bond length and porosity measurements\n",
    "\n",
    "    # Normalize the data.\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    # normalize dataset with MinMaxScaler\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "    x_labeled = scaler.fit_transform(x_labeled)\n",
    "    # y_labeled = scaler.fit_transform(y_labeled)\n",
    "\n",
    "    tr_size = 30\n",
    "\n",
    "    # train and test data\n",
    "    trainX, trainY = x_labeled[:tr_size,:], y_labeled[:tr_size]\n",
    "    testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]\n",
    "\n",
    "    trainY = np.transpose(trainY)\n",
    "    testY = np.transpose(testY)\n",
    "\n",
    "    data_phyloss = np.loadtxt('../data/unlabeled_data_BK_constw_v2_1525.dat')\n",
    "    x_unlabeled = data_phyloss[:, :]\n",
    "\n",
    "    # initial porosity\n",
    "    initporo = x_unlabeled[:, -1]\n",
    "\n",
    "    x_unlabeled1 = x_unlabeled[:1303, :2]\n",
    "    x_unlabeled2 = x_unlabeled[-6:, :2]\n",
    "    x_unlabeled = np.vstack((x_unlabeled1,x_unlabeled2))\n",
    "\n",
    "    x_unlabeled = scaler.fit_transform(x_unlabeled)\n",
    "    init_poro1 = initporo[:1303]\n",
    "    init_poro2 = initporo[-6:]\n",
    "    init_poro = np.hstack((init_poro1,init_poro2))\n",
    "\n",
    "\n",
    "    def build_gp(amplitude, length_scale):\n",
    "        \"\"\"Defines the conditional dist. of GP outputs, given kernel parameters.\"\"\"\n",
    "\n",
    "        # Create the covariance kernel, which will be shared between the prior (which we\n",
    "        # use for maximum likelihood training) and the posterior (which we use for\n",
    "        # posterior predictive sampling)    \n",
    "        se_kernel = tfk.ExponentiatedQuadratic(amplitude)  # length_scale = None here, implicitly\n",
    "\n",
    "        # This is the \"ARD\" kernel (we don't like abbreviations or bizarrely obscure names in\n",
    "        # TFP, so we're probably going to call this \"InputScaledKernel\" since....that's what it is! :)\n",
    "        kernel = InputScaledKernel(se_kernel, length_scale)\n",
    "\n",
    "        # Create the GP prior distribution, which we will use to train the model\n",
    "        # parameters.\n",
    "        return tfd.GaussianProcess(kernel=kernel,index_points=trainX)\n",
    "\n",
    "    gp_joint_model = tfd.JointDistributionNamedAutoBatched({\n",
    "        'amplitude': tfd.TransformedDistribution(\n",
    "                distribution=tfd.Normal(loc=0., scale=np.float64(1.)),\n",
    "                bijector=tfb.Exp(),\n",
    "                batch_shape=[1]),\n",
    "        'length_scale': tfd.TransformedDistribution(\n",
    "                distribution=tfd.Normal(loc=0., scale=np.float64(1.)),\n",
    "                bijector=tfb.Exp(),\n",
    "                batch_shape=[2]),\n",
    "        'observations': build_gp,\n",
    "    })\t\t\t\t\t\n",
    "\n",
    "\n",
    "\n",
    "    # Create the trainable model parameters, which we'll subsequently optimize.\n",
    "    # Note that we constrain them to be strictly positive.\n",
    "    constrain_positive = tfb.Shift(np.finfo(np.float64).tiny)(tfb.Exp())\n",
    "\n",
    "    amplitude_var = tfp.util.TransformedVariable(\n",
    "        initial_value=np.random.uniform(size=1),\n",
    "        bijector=constrain_positive,\n",
    "        name='amplitude',\n",
    "        dtype=np.float64)\n",
    "\n",
    "    length_scale_var = tfp.util.TransformedVariable(\n",
    "        initial_value=np.random.uniform(size=[2]),\n",
    "        bijector=constrain_positive,\n",
    "        name='length_scale',\n",
    "        dtype=np.float64)\n",
    "\n",
    "    trainable_variables = [v.trainable_variables[0] for v in \n",
    "                           [amplitude_var,\n",
    "                           length_scale_var]]\n",
    "\n",
    "\n",
    "\n",
    "    @tf.function(autograph=False, experimental_compile=False)\n",
    "    def target_log_prob(amplitude, length_scale, poroi, lam):\n",
    "        tf.random.set_seed(1234)\n",
    "        se_kernel = tfk.ExponentiatedQuadratic(amplitude)  # length_scale = None here, implicitly\n",
    "        optimized_kernel = InputScaledKernel(se_kernel, length_scale)\n",
    "        gprm = tfd.GaussianProcessRegressionModel(kernel=optimized_kernel, index_points = x_unlabeled)\n",
    "        samples = gprm.sample(1)\n",
    "        pred = tf.squeeze(samples, axis=0)\n",
    "\n",
    "        phyloss_poro = tf.math.reduce_mean(tf.nn.relu(tf.negative(pred))+tf.nn.relu(pred-poroi))\n",
    "\n",
    "    #     print(\"phyloss_poro:\",lam*phyloss_poro)\n",
    "    #     return lam*phyloss_poro\n",
    "        return lam*phyloss_poro - gp_joint_model.log_prob({\n",
    "          'amplitude': amplitude,\n",
    "          'length_scale': length_scale,\n",
    "          'observations': trainY\n",
    "      })\n",
    "\n",
    "\n",
    "    tf.random.set_seed(1234)\n",
    "\n",
    "    # Optimize the model parameters.\n",
    "    num_iters = 40\n",
    "    lam = 100000\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=.1)\n",
    "\n",
    "    # Store the likelihood values during training, so we can plot the progress\n",
    "    lls_ = np.zeros(num_iters, np.float64)\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = target_log_prob(amplitude_var, length_scale_var, init_poro, lam) # physics loss & normal loss\n",
    "\n",
    "\n",
    "        # print(i,\"loss_inloop:\",loss)\n",
    "        grads = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "        lls_[i] = loss\n",
    "\n",
    "    # print('Trained parameters:')\n",
    "    # print('amplitude: {}'.format(amplitude_var._value().numpy()))\n",
    "    # print('length_scale: {}'.format(length_scale_var._value().numpy()))\n",
    "\n",
    "\n",
    "\n",
    "    tf.random.set_seed(1234)\n",
    "    se_kernel = tfk.ExponentiatedQuadratic(amplitude_var)  # length_scale = None here, implicitly\n",
    "    optimized_kernel = InputScaledKernel(se_kernel, length_scale_var)\n",
    "    gprm = tfd.GaussianProcessRegressionModel(kernel=optimized_kernel, index_points = testX)\n",
    "    samples = gprm.sample(1)\n",
    "    samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 9), dtype=float64, numpy=\n",
       "array([[[-0.09655712, -0.09761954, -0.06867765,  0.08486226,\n",
       "          0.07861124,  0.11342095,  0.09207502, -0.07753923,\n",
       "          0.03311804]],\n",
       "\n",
       "       [[ 0.04212455,  0.03606457,  0.06086621,  0.0506872 ,\n",
       "         -0.11496251, -0.09564325, -0.02192549,  0.0386103 ,\n",
       "          0.02477364]]])>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1234)\n",
    "se_kernel = tfk.ExponentiatedQuadratic(amplitude_var)  # length_scale = None here, implicitly\n",
    "optimized_kernel = InputScaledKernel(se_kernel, length_scale_var)\n",
    "gprm = tfd.GaussianProcessRegressionModel(kernel=optimized_kernel, index_points = testX)\n",
    "samples = gprm.sample(2)\n",
    "# optimized_kernel = tfk.ExponentiatedQuadratic(amplitude_var, length_scale_var)\n",
    "# gpr = tfd.GaussianProcessRegressionModel(kernel=optimized_kernel, index_points = testX)\n",
    "# samples = gpr.sample(1)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 9)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tf.squeeze(samples, axis=1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0105933 , 0.03315482, 0.01352198, 0.05269549, 0.00490424,\n",
       "        0.00895544, 0.01003564, 0.01345683, 0.02496436]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[0.08679325]])>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_mean_squared_error(testY, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-c555797ef780>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# Plot the loss evolution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlls_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training iteration\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the loss evolution\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(lls_)\n",
    "plt.xlabel(\"Training iteration\")\n",
    "plt.ylabel(\"Log marginal likelihood\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
