{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, Adadelta, Adagrad, Adam, Nadam, SGD\n",
    "from keras.callbacks import EarlyStopping, TerminateOnNaN\n",
    "from keras import backend as K\n",
    "from keras.losses import mean_squared_error\n",
    "import tensorflow as tf\n",
    "\n",
    "# Normalize the data.\n",
    "from sklearn import preprocessing\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "import random\n",
    "\n",
    "def pass_arg(nsim, tr_size):\n",
    "    print(\"Tr_size:\", tr_size)\n",
    "    def fix_seeds(seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "        sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    #     K.set_session(sess)\n",
    "        tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "    ss = 1\n",
    "    fix_seeds(ss)\n",
    "\n",
    "\n",
    "    # import pickle\n",
    "\n",
    "    # def save_obj(obj, name):\n",
    "    #     with open(name, 'wb') as f:\n",
    "    #         pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "    # Compute the RMSE given the ground truth (y_true) and the predictions(y_pred)\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "            return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "    # Making sure dimensionless bond length is less than 1\n",
    "    def bond(bl):\n",
    "        return tf.add(K.relu(tf.negative(bl)), K.relu(bl-1.0))\n",
    "\n",
    "    # Making sure final porosity is less than initial\n",
    "    def poros(poroi, porof):\n",
    "        return K.relu(tf.negative(porof)) + K.relu(porof-poroi)\n",
    "\n",
    "    def strength1(bl, porof, nlayer=6):\n",
    "        sigma01, sigma02, C1s = 6, 31, 21\n",
    "        sigma_long = sigma01*(K.exp((1.0-porof)**(C1s*nlayer))-porof) + sigma02*(1.0-porof)\n",
    "        n = K.shape(sigma_long)[0]  \n",
    "        sorted_strength, sortedIndices = tf.math.top_k(sigma_long, n, True)\n",
    "        sorted_bl = K.gather(bl, sortedIndices)\n",
    "        sorted_porof = K.gather(porof, sortedIndices)\n",
    "        argg = tf.argsort(sorted_bl,axis=-1,direction='DESCENDING',stable=False,name=None)\n",
    "        sorted_bl_corr = K.gather(sorted_bl, argg)\n",
    "        return sorted_bl_corr-sorted_bl\n",
    "\n",
    "\n",
    "    def strength2(bl, porof, nlayer=6):\n",
    "        sigma01, sigma02, C1s = 6, 31, 21\n",
    "        sigma_long = sigma01*(K.exp((1.0-porof)**(C1s*nlayer))-porof) + sigma02*(1.0-porof)\n",
    "        n = K.shape(sigma_long)[0]  \n",
    "        sorted_strength, sortedIndices = tf.math.top_k(sigma_long, n, True)\n",
    "        sorted_bl = K.gather(bl, sortedIndices)\n",
    "        n = K.cast(n, tf.float32)\n",
    "        rel = K.relu(sorted_bl[1:]-sorted_bl[0:-1])\n",
    "        num_vio = K.cast(tf.math.count_nonzero(rel), tf.float32)\n",
    "        return num_vio/n\n",
    "\n",
    "\n",
    "    def phy_loss_mean(params):\n",
    "        # useful for cross-checking training\n",
    "        loss1, loss2, loss3, loss4, lam1, lam2, lam3, lam4 = params\n",
    "        def loss(y_true,y_pred):\n",
    "    #         return lam1*K.mean(K.relu(loss1)) + lam2*K.mean(K.relu(loss2)) + lam2*K.mean(K.relu(loss3))\n",
    "            return lam1*K.mean(K.relu(loss1)) + lam2*K.mean(K.relu(loss2)) + lam3*K.mean(K.relu(loss3)) + lam4*loss4\n",
    "        return loss\n",
    "\n",
    "    #function to calculate the combined loss = sum of rmse and phy based loss\n",
    "    def combined_loss(params):\n",
    "        loss1, loss2, loss3, loss4, lam1, lam2, lam3, lam4 = params\n",
    "        def loss(y_true,y_pred):\n",
    "    #         return mean_squared_error(y_true, y_pred) + lam1 * K.mean(K.relu(loss1)) + lam2 * K.mean(K.relu(loss2)) + lam2 * K.mean(K.relu(loss3))\n",
    "            return mean_squared_error(y_true, y_pred) + lam1 * K.mean(K.relu(loss1)) + lam2 * K.mean(K.relu(loss2)) + lam3*K.mean(K.relu(loss3)) + lam4 * loss4\n",
    "        return loss\n",
    "\n",
    "    def PGNN_train_test(optimizer_name, optimizer_val, drop_frac, use_YPhy, iteration, n_layers, n_nodes, tr_size, lamda, reg, samp):\n",
    "\n",
    "    #     fix_seeds(ss)\n",
    "\n",
    "        # Hyper-parameters of the training process\n",
    "    #     batch_size = tr_size\n",
    "        batch_size = 1\n",
    "        num_epochs = 50\n",
    "        val_frac = 0.2\n",
    "        patience_val = 50\n",
    "\n",
    "        # Initializing results filename\n",
    "        exp_name = optimizer_name + '_drop' + str(drop_frac) + '_usePhy' + str(use_YPhy) +  '_nL' + str(n_layers) + '_nN' + str(n_nodes) + '_trsize' + str(tr_size) + '_lamda' + str(lamda) + '_iter' + str(iteration)\n",
    "        exp_name = exp_name.replace('.','pt')\n",
    "        results_dir = '../results/'\n",
    "        model_name = results_dir + exp_name + '_model.h5' # storing the trained model\n",
    "\n",
    "        if reg==True and samp==25:\n",
    "            results_name = results_dir + exp_name + '_results_25_regularizer.dat' # storing the results of the model\n",
    "        elif reg==False and samp==25:\n",
    "            results_name = results_dir + exp_name + '_results_25.dat' # storing the results of the model\n",
    "        elif reg==True and samp==1519:\n",
    "            results_name = results_dir + exp_name + '_results_1519_regularizer.dat' # storing the results of the model\n",
    "        elif reg==False and samp==1519:\n",
    "            results_name = results_dir + exp_name + '_results_1519.dat' # storing the results of the model\n",
    "\n",
    "        # Load labeled data\n",
    "        data = np.loadtxt('../data/labeled_data.dat')\n",
    "    #     data = np.loadtxt('../data/labeled_data_BK_constw_unique.dat')\n",
    "    #     data = np.loadtxt('../data/labeled_data_BK_constw_v2.dat')\n",
    "#         x_labeled = data[:, :2] # -2 because we do not need porosity predictions\n",
    "        x_label = data[:, :-3] # -2 because we do not need porosity predictions\n",
    "        x_labeled = np.hstack((x_label[:,:2],x_label[:,-2:]))\n",
    "        y_labeled = data[:, -3:-1] # dimensionless bond length and porosity measurements\n",
    "        if samp==25:\n",
    "            data = np.loadtxt('../data/unlabeled_data_BK_constw_v2_25.dat')\n",
    "            x_unlabeled = data[:, :]\n",
    "        elif samp==1519:\n",
    "            data = np.loadtxt('../data/unlabeled_data_BK_constw_v2_1525.dat')\n",
    "            x_unlabeled = data[:, :]\n",
    "\n",
    "        x_unlabeled1 = x_unlabeled[:1303, :]\n",
    "        x_unlabeled2 = x_unlabeled[-6:, :]\n",
    "        x_unlabeled = np.vstack((x_unlabeled1,x_unlabeled2))\n",
    "\n",
    "        # initial porosity\n",
    "        init_poro = x_unlabeled[:, -1]\n",
    "        x_unlabeled = np.hstack((x_unlabeled[:,:2],x_unlabeled[:,-3:-1]))\n",
    "#         x_unlabeled = x_unlabeled[:, :2]\n",
    "\n",
    "\n",
    "        # normalize dataset with MinMaxScaler\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "    #     scaler = preprocessing.StandardScaler()\n",
    "        x_labeled = scaler.fit_transform(x_labeled)\n",
    "        x_unlabeled = scaler.fit_transform(x_unlabeled)\n",
    "#         y_labeled = scaler.fit_transform(y_labeled)\n",
    "\n",
    "    #     # initial porosity & physics outputs are removed\n",
    "    #     x_unlabeled = x_unlabeled[:, :-3]\n",
    "\n",
    "        # train and test data\n",
    "        trainX, trainY = x_labeled[:tr_size,:], y_labeled[:tr_size]\n",
    "    #     testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]   \n",
    "        testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]\n",
    "\n",
    "        if use_YPhy == 0:\n",
    "            # Removing the last column from x_unlabeled (corresponding to Y_PHY)\n",
    "            x_unlabeled = x_unlabeled[:,:-1]\n",
    "\n",
    "        # Creating the model\n",
    "        model = Sequential()\n",
    "        for layer in np.arange(n_layers):\n",
    "            if layer == 0:\n",
    "                model.add(Dense(n_nodes, activation='relu', input_shape=(np.shape(trainX)[1],)))\n",
    "            else:\n",
    "                if reg:\n",
    "                    model.add(Dense(n_nodes, activation='relu', kernel_regularizer=l1_l2(l1=.001, l2=.001)))\n",
    "                else:\n",
    "                    model.add(Dense(n_nodes, activation='relu'))\n",
    "            model.add(Dropout(rate=drop_frac))\n",
    "        model.add(Dense(2, activation='linear'))\n",
    "\n",
    "        # physics-based regularization\n",
    "        uinp_sc = K.constant(value=x_unlabeled) # unlabeled input data\n",
    "        lam1 = K.constant(value=lamda[0]) # regularization hyper-parameter\n",
    "        lam2 = K.constant(value=lamda[1]) # regularization hyper-parameter\n",
    "        lam3 = K.constant(value=lamda[2]) # regularization hyper-parameter\n",
    "        lam4 = K.constant(value=lamda[3]) # regularization hyper-parameter\n",
    "        predictions = model(uinp_sc) # model output at depth i\n",
    "    #     porosity = K.relu(predictions[:,1])\n",
    "        phyloss1 = bond(predictions[:,0]) # physics loss 1\n",
    "    #     uinp = K.constant(value=x_unlabeled_non) # unlabeled input data\n",
    "        phyloss2 = poros(init_poro, predictions[:,1]) # physics loss 1\n",
    "        phyloss3 = strength1(predictions[:,0], predictions[:,1])\n",
    "        phyloss4 = strength2(predictions[:,0], predictions[:,1])\n",
    "        totloss = combined_loss([phyloss1, phyloss2, phyloss3, phyloss4, lam1, lam2, lam3, lam4])\n",
    "        phyloss = phy_loss_mean([phyloss1, phyloss2, phyloss3, phyloss4, lam1, lam2, lam3, lam4])\n",
    "\n",
    "\n",
    "        model.compile(loss=totloss,\n",
    "                      optimizer=optimizer_val,\n",
    "                      metrics=[phyloss, root_mean_squared_error])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience_val, verbose=1)\n",
    "\n",
    "    #     print('Running...' + optimizer_name)\n",
    "        history = model.fit(trainX, trainY,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=num_epochs,\n",
    "                            verbose=0,\n",
    "                            validation_split=val_frac, callbacks=[early_stopping, TerminateOnNaN()])\n",
    "    \n",
    "#     early_stopping = EarlyStopping(monitor='loss', patience=patience_val, verbose=1)\n",
    "#     history = model.fit(trainX, trainY,\n",
    "#                         batch_size=batch_size,\n",
    "#                         epochs=num_epochs,\n",
    "#                         verbose=1,\n",
    "#                         callbacks=[early_stopping, TerminateOnNaN()])\n",
    "\n",
    "#     test_score = model.evaluate(testX, testY, verbose=0)\n",
    "#     predictions = model.predict(x_labeled) # model output at depth i\n",
    "#     print(np.sort(predictions[:,0], axis=0))\n",
    "    \n",
    "#     predictions = model.predict(x_unlabeled) # model output at depth i\n",
    "#     print(np.sort(predictions[:,0], axis=0))\n",
    "    \n",
    "#     print('iter: ' + str(iteration) + ' useYPhy: ' + str(use_YPhy) + \n",
    "#           ' nL: ' + str(n_layers) + ' nN: ' + str(n_nodes) + \n",
    "#           ' lamda1: ' + str(lamda[0]) + ' lamda2: ' + str(lamda[1]) + ' trsize: ' + str(tr_size) + \n",
    "#           ' TestRMSE: ' + str(test_score[2]) + ' PhyLoss: ' + str(test_score[1]), ' TestLoss: ' + str(test_score[0]), \"\\n\")\n",
    "\n",
    "# #     print('iter: ' + str(iteration) + ' TestRMSE: ' + str(test_score[2]) + ' PhyLoss: ' + str(test_score[1]), \"\\n\")\n",
    "\n",
    "    \n",
    "# #     model.save(model_name)\n",
    "    \n",
    "#     # save results\n",
    "#     results = {'train_loss_1':history.history['loss_1'], \n",
    "#                                 'val_loss_1':history.history['val_loss_1'], \n",
    "#                                 'train_rmse':history.history['root_mean_squared_error'], \n",
    "#                                 'val_rmse':history.history['val_root_mean_squared_error'],\n",
    "#                                 'test_rmse':test_score[2],\n",
    "#                                 'PhyLoss':test_score[1]}\n",
    "\n",
    "#     results = {'train_loss_1':history.history['loss_1'], \n",
    "#                                 'train_rmse':history.history['root_mean_squared_error'], \n",
    "#                                 'test_rmse':test_score[2],\n",
    "#                                 'PhyLoss':test_score[1]}\n",
    "\n",
    "    \n",
    "\n",
    "#     save_obj(results, results_name)\n",
    "\n",
    "#     predictions = model.predict(testX)\n",
    "#     return results, results_name, predictions, testY, test_score[2], trainY\n",
    "    \n",
    "        test_score = model.evaluate(testX, testY, verbose=1)\n",
    "        print(test_score)\n",
    "        \n",
    "        Xx = np.random.uniform(0,1,(1000,2))\n",
    "        xx1 = np.ones((1000,2))\n",
    "        Xx = np.hstack((Xx,xx1))\n",
    "\n",
    "        samples = []\n",
    "        for i in range(int(nsim)):\n",
    "            print(\"simulation num:\",i)\n",
    "            predictions = model.predict(Xx)\n",
    "            predictions = predictions[:,1]\n",
    "            samples.append(predictions[:,np.newaxis])\n",
    "        return np.array(samples)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Main Function\n",
    "    if __name__ == '__main__':\n",
    "\n",
    "        fix_seeds(1)\n",
    "\n",
    "        # List of optimizers to choose from    \n",
    "        optimizer_names = ['Adagrad', 'Adadelta', 'Adam', 'Nadam', 'RMSprop', 'SGD', 'NSGD']\n",
    "        optimizer_vals = [Adagrad(clipnorm=1), Adadelta(clipnorm=1), Adam(clipnorm=1), Nadam(clipnorm=1), RMSprop(clipnorm=1), SGD(clipnorm=1.), SGD(clipnorm=1, nesterov=True)]\n",
    "\n",
    "        # selecting the optimizer\n",
    "        optimizer_num = 1\n",
    "        optimizer_name = optimizer_names[optimizer_num]\n",
    "        optimizer_val = optimizer_vals[optimizer_num]\n",
    "\n",
    "        # Selecting Other Hyper-parameters\n",
    "        drop_frac = 0.1 # Fraction of nodes to be dropped out\n",
    "        use_YPhy = 1 # Whether YPhy is used as another feature in the NN model or not\n",
    "        n_layers = 2 # Number of hidden layers\n",
    "        n_nodes = 5 # Number of nodes per hidden layer\n",
    "\n",
    "        #set lamda\n",
    "        lamda = [0.3, 0.15, 0.008, 0] # Physics-based regularization constant  \n",
    "\n",
    "#         # Iterating over different training fractions and splitting indices for train-test splits\n",
    "#         trsize_range = [4,6,8,10,20]\n",
    "\n",
    "#         #default training size = 5000\n",
    "#         tr_size = trsize_range[4]\n",
    "        \n",
    "        tr_size = int(tr_size)\n",
    "\n",
    "        # use regularizer\n",
    "        reg = True\n",
    "\n",
    "        # sample size used\n",
    "        samp = 1519\n",
    "    #     samp = 25\n",
    "\n",
    "        # total number of runs\n",
    "        iter_range = np.arange(1)\n",
    "        testrmse=[]\n",
    "        # iterating through all possible params\n",
    "        for iteration in iter_range:\n",
    "#             results, result_file, pred, obs, rmse, obs_train = PGNN_train_test(optimizer_name, optimizer_val, drop_frac, use_YPhy, \n",
    "#                             iteration, n_layers, n_nodes, tr_size, lamda, reg, samp)\n",
    "#             testrmse.append(rmse)\n",
    "            pred = PGNN_train_test(optimizer_name, optimizer_val, drop_frac, use_YPhy, \n",
    "                            iteration, n_layers, n_nodes, tr_size, lamda, reg, samp)\n",
    "            \n",
    "\n",
    "    return np.squeeze(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr_size: 20\n",
      "19/19 [==============================] - 0s 54us/step\n",
      "[0.007559574209153652, 0.00016938704357016832, 0.05568365007638931]\n",
      "simulation num: 0\n"
     ]
    }
   ],
   "source": [
    "pred = pass_arg(1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.19163112e-02, -7.86477327e-03,  2.57302132e-02,  9.19948518e-02,\n",
       "        6.98126405e-02, -2.80525759e-02,  1.32034555e-01,  9.90678668e-02,\n",
       "       -2.28048488e-02,  1.59658249e-02,  7.15928748e-02,  1.57780498e-01,\n",
       "        1.01074472e-01,  7.12802261e-02,  1.43954188e-01,  2.50166655e-03,\n",
       "        1.16732500e-01,  1.60241574e-01,  9.25640240e-02, -5.40892407e-03,\n",
       "        5.35078868e-02,  1.08929396e-01, -1.92062370e-02,  9.80005860e-02,\n",
       "        4.10001688e-02,  2.19058413e-02,  7.88101554e-03,  1.58318102e-01,\n",
       "        1.44703776e-01,  1.92421563e-02,  8.56093317e-02,  5.29635251e-02,\n",
       "        7.94239491e-02,  1.03516772e-01,  6.58983067e-02,  7.77754560e-03,\n",
       "        1.18248008e-01,  8.96374583e-02,  4.09888364e-02,  3.47980000e-02,\n",
       "        1.31409857e-02,  2.19240040e-03, -3.29135805e-02, -5.29432297e-03,\n",
       "        3.90245281e-02,  9.48898792e-02, -2.07159296e-03,  7.39300996e-03,\n",
       "        6.52258098e-02, -4.97069582e-03,  2.08424330e-01,  1.27560854e-01,\n",
       "        9.38296616e-02,  7.64915347e-02,  4.10027318e-02,  7.39855319e-03,\n",
       "        1.52080983e-01,  3.56209092e-02,  3.95560414e-02,  9.70470235e-02,\n",
       "       -2.85498425e-02,  1.07339598e-01, -3.90894338e-03,  2.35788263e-02,\n",
       "        4.26525697e-02,  5.73027506e-03,  2.03775615e-01,  7.73447901e-02,\n",
       "        3.53009216e-02, -1.89451426e-02,  9.70057994e-02,  1.85771495e-01,\n",
       "        7.88463280e-03, -3.37697864e-02,  3.48772146e-02,  1.05613008e-01,\n",
       "        1.76311493e-01,  3.17130461e-02, -2.48231068e-02,  1.60093307e-01,\n",
       "        1.90665334e-01, -3.00254896e-02,  9.55076516e-03,  3.29125226e-02,\n",
       "       -1.59216039e-02, -7.52209872e-03, -2.44636610e-02, -2.43327096e-02,\n",
       "       -3.26618105e-02,  1.00561596e-01,  4.09497693e-03,  1.21986084e-01,\n",
       "        1.81181543e-02, -3.05128321e-02,  2.58085653e-02,  3.19450200e-02,\n",
       "       -1.58974640e-02,  3.20814028e-02,  4.81559709e-03,  1.20133370e-01,\n",
       "        2.97015924e-02,  1.64237827e-01,  3.28423008e-02,  3.53950858e-02,\n",
       "        9.28820744e-02,  4.09815162e-02,  1.75865535e-02,  1.48438513e-01,\n",
       "        1.68558154e-02,  1.48983598e-01,  1.74075842e-01, -1.00313574e-02,\n",
       "        4.35106233e-02,  4.09671739e-02,  7.37682730e-02,  1.04530200e-01,\n",
       "        1.75766349e-02,  8.14332440e-03,  4.86020371e-02,  4.10010144e-02,\n",
       "        1.69332296e-01,  3.62007990e-02, -4.46872860e-02,  4.23789807e-02,\n",
       "       -1.32990964e-02,  9.61961448e-02,  3.99682298e-02, -2.62550190e-02,\n",
       "       -3.89679447e-02,  1.02433570e-01,  1.25505030e-02,  5.43884933e-02,\n",
       "        1.23754226e-01,  6.00456446e-02, -5.47154993e-03,  2.18495652e-02,\n",
       "       -2.84115598e-02,  1.28575966e-01, -6.09616190e-03,  7.35439509e-02,\n",
       "       -6.41651452e-04,  6.01874888e-02,  1.33678243e-01,  1.08517319e-01,\n",
       "        2.00307846e-01,  3.04460563e-02,  1.06651641e-01,  3.87596525e-02,\n",
       "        1.06623046e-01, -4.16392833e-02, -5.11692092e-03,  1.61461830e-01,\n",
       "        2.66670957e-02,  1.22740455e-01,  7.26780295e-02,  1.42906725e-01,\n",
       "        1.52598798e-01,  1.18934490e-01,  1.41167462e-01,  1.07930779e-01,\n",
       "        2.00390685e-02,  5.37189394e-02,  1.23661552e-02,  5.30623645e-02,\n",
       "        1.70759428e-02,  1.18234105e-01,  1.09647222e-01, -5.10483980e-04,\n",
       "       -3.30682099e-02,  1.23807892e-01,  1.08691417e-02,  4.09619771e-02,\n",
       "        2.09896602e-02,  3.70512642e-02,  1.61217093e-01,  3.49531770e-02,\n",
       "        3.72026786e-02,  9.87366438e-02, -2.44206563e-02,  2.90392227e-02,\n",
       "       -1.48391500e-02, -2.89075971e-02, -1.40545517e-03,  1.74408853e-01,\n",
       "        1.80172101e-02,  3.33021060e-02, -2.62611061e-02,  2.06516355e-01,\n",
       "       -2.91238353e-02,  4.09672819e-02,  1.22246444e-02, -2.81919241e-02,\n",
       "        1.02149978e-01,  6.67099953e-02,  4.08783592e-02,  1.70241356e-01,\n",
       "       -3.41656059e-02,  4.09595929e-02, -2.71393731e-02,  9.64437723e-02,\n",
       "        1.79801762e-01,  4.10202071e-02,  4.09877822e-02,  3.43268588e-02,\n",
       "        2.08053172e-01,  2.07394481e-01,  7.32106864e-02,  5.80564588e-02,\n",
       "       -1.80100128e-02, -2.92173326e-02,  6.05950318e-02,  1.45382434e-03,\n",
       "        1.82236105e-01, -3.11959684e-02, -2.07165331e-02,  1.32072821e-01,\n",
       "        9.85182151e-02,  4.09666449e-02,  4.11948450e-02,  4.65535596e-02,\n",
       "        8.54559392e-02,  2.63913795e-02,  9.21163410e-02, -6.92179427e-03,\n",
       "        1.23689987e-01,  2.32737213e-02, -3.26024219e-02,  5.80591932e-02,\n",
       "        1.34807885e-01,  2.16990530e-01, -2.35154182e-02,  8.40617046e-02,\n",
       "        9.19682160e-02,  1.98950227e-02,  5.16106635e-02,  2.51962263e-02,\n",
       "        2.35069096e-02,  9.22835916e-02,  1.16548259e-02, -4.64040786e-03,\n",
       "        1.39630258e-01, -4.23547551e-02, -1.24958158e-03,  6.75667524e-02,\n",
       "        3.98794003e-02,  1.55310426e-02,  1.83051527e-01,  1.20787948e-01,\n",
       "        1.37653146e-02,  3.82565893e-02, -3.22912186e-02,  1.73632890e-01,\n",
       "        8.86852592e-02,  3.84164900e-02,  1.78198993e-01,  5.66007979e-02,\n",
       "        4.90399450e-03,  1.06992804e-01,  3.29786167e-02, -4.05417234e-02,\n",
       "        4.09617312e-02,  6.21694177e-02,  3.86789367e-02, -1.34216882e-02,\n",
       "        3.34223211e-02,  6.06799424e-02,  1.13992237e-01,  1.78007111e-02,\n",
       "        4.09773216e-02, -2.25247145e-02,  8.33737478e-03,  3.73127125e-02,\n",
       "       -3.33183557e-02,  1.16575949e-01,  6.39502481e-02,  5.50391376e-02,\n",
       "        1.57831982e-01,  1.08890757e-01,  6.85189366e-02, -2.15485767e-02,\n",
       "        8.63473788e-02,  7.14891180e-02, -4.22838852e-02, -3.16892490e-02,\n",
       "        6.23174384e-02, -3.68091017e-02,  8.83469582e-02,  3.45420837e-03,\n",
       "        3.47572900e-02,  4.09650058e-02,  2.67303213e-02, -7.23160803e-04,\n",
       "       -3.52695957e-03, -4.56096232e-03,  3.45753245e-02,  1.01941489e-02,\n",
       "        1.05162598e-01, -8.09839368e-03,  7.02718645e-02,  4.09602150e-02,\n",
       "        3.44614387e-02,  1.01893224e-01, -1.00228824e-02, -1.90825015e-02,\n",
       "        4.61941138e-02, -3.08750197e-02,  3.17561291e-02,  1.30189843e-02,\n",
       "        9.40456837e-02,  5.23348525e-03,  2.71825064e-02,  4.38443087e-02,\n",
       "        1.39918938e-01, -1.18832588e-02,  4.09870334e-02,  1.96357295e-02,\n",
       "        1.92674756e-01,  1.66792423e-04,  5.18279374e-02, -2.50573382e-02,\n",
       "       -1.11140274e-02,  5.56888431e-03,  4.09653783e-02,  3.22085172e-02,\n",
       "       -1.88966393e-02,  4.21068445e-03,  9.97077674e-02,  7.77126551e-02,\n",
       "        1.02755986e-02,  1.49886981e-01,  4.72744592e-02, -1.47346444e-02,\n",
       "        9.15521309e-02,  4.07849662e-02,  1.09187953e-01,  7.20688924e-02,\n",
       "       -1.72766112e-02, -2.05529295e-02,  9.00282338e-03,  7.40419924e-02,\n",
       "        1.72852755e-01,  9.87195224e-03, -3.05740088e-02,  2.26007365e-02,\n",
       "        1.09332941e-01,  1.10479258e-01,  8.48868862e-02,  4.57274914e-03,\n",
       "       -3.93819064e-03,  3.49475369e-02,  2.54693441e-02,  3.20307463e-02,\n",
       "        1.04066938e-01,  3.95713001e-03, -3.11878026e-02,  2.79879570e-03,\n",
       "        4.47124615e-03,  3.31405401e-02,  4.72187586e-02,  4.09989282e-02,\n",
       "        4.09987196e-02,  1.58135947e-02, -2.73843557e-02,  3.72562408e-02,\n",
       "       -2.01488063e-02, -3.05818766e-02,  6.87281489e-02, -3.48926261e-02,\n",
       "        1.11898549e-01, -3.03009525e-02,  1.80285186e-01,  5.81154749e-02,\n",
       "        4.09562588e-02,  1.73988193e-01, -9.39950347e-03,  6.16306886e-02,\n",
       "        6.05484843e-02,  1.05478235e-01,  6.90977424e-02,  1.59033895e-01,\n",
       "        9.29211527e-02,  1.14969686e-02,  3.96638662e-02,  1.73343152e-01,\n",
       "       -1.66963451e-02,  1.16105266e-01, -3.71707156e-02,  2.02342719e-01,\n",
       "        7.14140385e-02,  5.59106320e-02,  1.52126655e-01, -1.26891136e-02,\n",
       "        9.43528563e-02, -2.69159824e-02,  9.54886973e-02,  2.40915641e-03,\n",
       "       -1.65133066e-02,  6.03692122e-02,  4.14772518e-02,  1.67503476e-01,\n",
       "        1.33319154e-01,  2.30176188e-02, -3.51883918e-02,  4.09702696e-02,\n",
       "        8.50286037e-02,  1.26070172e-01,  7.90507048e-02,  1.24477837e-02,\n",
       "        6.49896711e-02,  5.55589348e-02,  1.60105705e-01,  2.63621751e-02,\n",
       "        1.51362851e-01, -4.12495509e-02,  4.09635566e-02,  1.89210445e-01,\n",
       "        2.22016666e-02,  4.09605727e-02,  1.02197379e-01,  3.46700735e-02,\n",
       "        6.86061531e-02, -1.72861889e-02, -1.55440569e-02,  7.64564052e-02,\n",
       "        3.95539440e-02, -2.37760991e-02,  1.85837951e-02,  3.77484784e-02,\n",
       "        8.90206397e-02, -9.07057151e-03,  4.10256088e-02,  2.01310098e-01,\n",
       "        3.90595943e-03, -1.67462789e-02, -2.98637152e-03,  1.61669582e-01,\n",
       "        1.45007014e-01,  4.33116853e-02,  4.09722887e-02, -7.56978989e-06,\n",
       "        1.13441106e-02,  4.09789607e-02,  9.59057063e-02,  1.14061758e-02,\n",
       "       -1.93092488e-02, -2.01266557e-02,  2.36462820e-02,  1.56063080e-01,\n",
       "        1.39889374e-01,  1.40005443e-02,  1.14384376e-01,  1.60384983e-01,\n",
       "       -6.09819591e-03,  1.93142146e-01, -1.40636899e-02,  2.02798117e-02,\n",
       "        3.46319899e-02,  1.71829104e-01, -2.72688493e-02,  7.30469078e-02,\n",
       "        9.37953591e-03,  2.08297577e-02,  2.96806786e-02, -5.76570630e-04,\n",
       "       -7.57144764e-03,  3.06773558e-02,  6.60986900e-02,  7.14112520e-02,\n",
       "       -3.52708027e-02, -1.05614923e-02,  2.30755638e-02,  6.95006847e-02,\n",
       "        3.77024561e-02,  1.06802255e-01,  5.00547625e-02,  8.84044170e-03,\n",
       "        9.35963541e-03,  4.10036482e-02,  1.32367089e-02,  9.55717936e-02,\n",
       "        5.25243171e-02,  1.81446690e-02, -1.52477622e-02,  8.70405510e-02,\n",
       "       -3.37620452e-03,  1.46712154e-01,  6.36267290e-02, -5.54508716e-03,\n",
       "        1.09598041e-02,  4.14771847e-02,  3.32659073e-02,  4.00674082e-02,\n",
       "        1.20062649e-01,  3.56579535e-02,  8.98404717e-02,  8.74908268e-02,\n",
       "        8.59137475e-02,  3.87070179e-02,  4.22201455e-02,  1.68977380e-01,\n",
       "        1.12397283e-01,  1.46048397e-01, -1.30762048e-02,  3.97854820e-02,\n",
       "        6.97405115e-02, -3.20688710e-02, -1.14873871e-02,  1.18846714e-01,\n",
       "       -2.01284885e-03, -3.26915234e-02,  1.54336572e-01,  3.94902229e-02,\n",
       "       -3.79225537e-02,  1.69754148e-01,  4.10181955e-02, -4.53878120e-02,\n",
       "        1.41235054e-01,  1.64183021e-01, -9.16153193e-03,  2.71779988e-02,\n",
       "        3.49762402e-02,  5.74253723e-02, -3.73569801e-02,  1.50576234e-04,\n",
       "        4.09783460e-02,  7.32780844e-02,  1.03358909e-01,  8.54742080e-02,\n",
       "        1.10783324e-01,  6.24072589e-02,  2.07064357e-02,  8.73571038e-02,\n",
       "       -3.03842649e-02,  8.59418884e-03,  3.96027714e-02,  6.59252107e-02,\n",
       "        8.93169343e-02,  7.39488155e-02, -3.15525159e-02,  4.60880436e-02,\n",
       "        1.98925316e-01,  1.26821309e-01,  8.20436776e-02,  1.22612175e-02,\n",
       "        1.10286865e-02,  9.99913290e-02,  4.09618355e-02,  1.38226990e-02,\n",
       "        4.82328236e-02, -3.41336504e-02,  2.23386958e-02, -1.82715021e-02,\n",
       "        1.44792330e-02,  4.09879573e-02,  2.56347917e-02,  3.79235148e-02,\n",
       "        2.01959442e-02,  2.11561292e-01, -2.19844133e-02, -2.80990675e-02,\n",
       "       -1.55901164e-02,  1.00424245e-01,  3.98476161e-02, -1.93289369e-02,\n",
       "       -6.43454492e-04,  5.89491799e-03, -1.47552676e-02,  1.39258921e-01,\n",
       "        3.18886936e-02, -3.34817842e-02,  1.46215886e-01, -1.06730349e-02,\n",
       "        4.87531982e-02,  6.46269321e-02,  4.33625169e-02,  1.64324120e-02,\n",
       "        1.89496670e-02,  9.88732427e-02,  1.12479605e-01,  8.71634036e-02,\n",
       "        4.09802794e-02, -2.40037963e-02,  4.53520305e-02,  4.09564525e-02,\n",
       "       -2.35107541e-02,  1.02106825e-01, -2.54180282e-02, -4.46959212e-02,\n",
       "        1.18472427e-03,  4.02625091e-02,  1.27425909e-01, -2.33286694e-02,\n",
       "        1.83354858e-02,  9.62540954e-02,  1.74430847e-01,  2.34191045e-02,\n",
       "        1.06999621e-01,  1.18267700e-01,  2.79313885e-02,  1.03158213e-01,\n",
       "        1.74916685e-02,  1.24540195e-01, -1.54522620e-02,  3.69236469e-02,\n",
       "        7.49420226e-02,  6.23665899e-02,  7.45118707e-02, -2.79184207e-02,\n",
       "        3.06154843e-02,  2.04761386e-01,  1.12103857e-02, -2.12808326e-02,\n",
       "        2.60228924e-02,  2.40211450e-02, -3.16535681e-02, -2.80572698e-02,\n",
       "       -2.07037106e-02,  2.75086798e-02, -2.26243138e-02, -1.46467872e-02,\n",
       "        7.85959810e-02,  4.12522256e-02,  5.14116883e-02, -2.09671333e-02,\n",
       "        1.28539279e-03,  6.78659454e-02,  1.48460343e-02, -2.50392929e-02,\n",
       "        1.09197218e-02,  6.71388879e-02,  3.06989122e-02, -5.12351841e-03,\n",
       "        1.31485552e-01,  1.35218911e-02,  6.66556656e-02, -2.94124335e-03,\n",
       "        3.92884389e-02,  2.34976877e-02,  1.19275264e-02,  3.03330217e-02,\n",
       "        8.47517848e-02,  9.90629792e-02,  1.46421567e-01,  1.28127575e-01,\n",
       "        4.46877070e-02,  2.14413673e-01,  2.01980155e-02,  3.75323817e-02,\n",
       "        4.09959778e-02, -2.20141560e-02, -3.06383893e-02,  1.45116448e-01,\n",
       "       -1.02168024e-02, -3.22652981e-03,  1.35098219e-01,  2.06149846e-01,\n",
       "        4.09859829e-02,  4.09754887e-02, -3.65248695e-02,  4.10084575e-02,\n",
       "       -1.67160258e-02,  1.90312769e-02,  3.70339006e-02,  3.91766168e-02,\n",
       "        1.48438755e-02,  2.29052734e-02, -1.86455585e-02,  2.45144721e-02,\n",
       "        3.98651138e-02,  1.47739530e-01,  2.33507510e-02,  1.39219195e-01,\n",
       "       -3.35873291e-02, -2.29352415e-02,  1.27161473e-01, -1.40026100e-02,\n",
       "        4.09542695e-02, -2.91476771e-03,  5.36884442e-02,  1.77425854e-02,\n",
       "        1.31879002e-01,  1.53316751e-01,  1.71443164e-01, -1.96532533e-03,\n",
       "        2.06886262e-01,  7.64076188e-02,  7.59811550e-02,  2.54622344e-02,\n",
       "        1.59487456e-01,  8.67344439e-02,  1.61740288e-01,  1.02615595e-01,\n",
       "        6.19764924e-02,  1.06892530e-02,  1.86267525e-01,  3.16191763e-02,\n",
       "       -1.89941004e-03,  9.82964933e-02,  1.18483193e-01,  9.91078019e-02,\n",
       "       -1.18446015e-02, -3.81922796e-02, -3.04812863e-02, -2.44037509e-02,\n",
       "       -1.46432519e-02,  2.07674652e-02,  1.90390110e-01, -2.22231075e-03,\n",
       "       -2.97973007e-02,  1.04691721e-02,  3.41825299e-02, -4.50351760e-02,\n",
       "        2.89112329e-03,  2.70787794e-02,  1.89767461e-02, -3.86061519e-03,\n",
       "       -2.42713392e-02,  5.37763163e-03,  9.86918807e-02,  2.19589379e-02,\n",
       "        4.10176776e-02,  1.25543103e-01,  1.36116207e-01,  3.65609229e-02,\n",
       "        9.19832736e-02,  2.97993943e-02, -1.20460801e-02,  4.56871241e-02,\n",
       "        6.24392331e-02,  1.27977997e-01,  2.29780003e-03,  3.53617482e-02,\n",
       "        5.23324646e-02,  3.61870266e-02,  8.22318494e-02,  7.29919076e-02,\n",
       "        2.24780701e-02,  1.15460884e-02,  1.91985071e-01, -3.91961262e-03,\n",
       "        1.55494869e-01,  5.26228994e-02,  4.09840420e-02,  8.29619616e-02,\n",
       "       -1.02894269e-02, -2.66856328e-02, -3.69122103e-02,  1.78269356e-01,\n",
       "        4.54526395e-04,  9.73693132e-02, -2.32355297e-02, -3.14936936e-02,\n",
       "        7.33498633e-02,  4.09634635e-02,  1.42667025e-01,  1.71969682e-01,\n",
       "       -1.14295259e-03,  8.48221928e-02,  8.78798217e-02,  8.42564180e-03,\n",
       "        1.57436915e-02,  9.00730491e-02,  2.47301646e-02,  4.09999862e-02,\n",
       "       -3.19981426e-02,  3.60125154e-02,  6.97318465e-03,  3.36751975e-02,\n",
       "        9.24382061e-02,  8.74834657e-02,  2.51993500e-02,  2.74195150e-03,\n",
       "        2.86091678e-02,  2.06876338e-01,  1.01853162e-01,  1.32667661e-01,\n",
       "        7.24849477e-03,  1.64857954e-02,  1.34384722e-01,  1.97955281e-01,\n",
       "        5.78699782e-02,  3.47152762e-02,  6.72814250e-02, -1.34440660e-02,\n",
       "       -2.74140239e-02,  5.81345856e-02,  4.09942046e-02, -2.47762278e-02,\n",
       "       -2.58051604e-02,  6.26533702e-02, -2.49057412e-02,  4.09599654e-02,\n",
       "        1.84060149e-02, -3.88335958e-02,  3.14922109e-02,  6.79710582e-02,\n",
       "        1.21511936e-01,  2.07532272e-02,  1.92408890e-01,  2.43496615e-02,\n",
       "        8.92819911e-02,  1.44730687e-01, -1.12481676e-02,  7.89766684e-02,\n",
       "        1.02318659e-01, -1.24171190e-02,  4.09758799e-02,  8.98479074e-02,\n",
       "        6.48256987e-02,  4.47959937e-02, -2.40906179e-02, -1.68094411e-02,\n",
       "        9.11091566e-02,  2.04027981e-01, -2.68567279e-02,  7.96616077e-05,\n",
       "        3.30714509e-02,  4.74630333e-02, -3.99360210e-02,  4.09573466e-02,\n",
       "       -3.91239971e-02,  5.73538542e-02,  4.07874621e-02,  1.22575216e-01,\n",
       "        1.25505447e-01,  5.48513941e-02,  2.88761631e-02,  1.35830045e-01,\n",
       "        2.18551513e-02,  7.15846717e-02,  4.09669429e-02,  1.10609196e-01,\n",
       "        3.37133408e-02,  8.54828358e-02,  4.06361781e-02,  2.06480116e-01,\n",
       "        1.62735105e-01,  1.90721452e-02, -1.90517120e-02,  1.07709482e-01,\n",
       "       -5.15355915e-03,  5.03224656e-02,  2.07701847e-02,  2.92110741e-02,\n",
       "        3.40988450e-02,  7.07917064e-02,  3.25262547e-02,  1.41674373e-02,\n",
       "        6.48051351e-02, -1.81354657e-02,  1.45941526e-01,  1.00085393e-01,\n",
       "       -4.28583324e-02,  4.09885198e-02,  7.24824369e-02, -1.35781989e-03,\n",
       "        6.72353804e-02, -7.76334107e-03,  4.09692340e-02,  9.28958356e-02,\n",
       "        3.76201458e-02,  1.34413958e-01,  1.88106019e-02,  1.38619363e-01,\n",
       "        1.31434426e-01,  1.55814476e-02,  5.90261966e-02,  9.27776843e-03,\n",
       "        3.12254205e-03,  1.70054495e-01, -2.66132206e-02,  8.01998377e-03,\n",
       "        6.09707087e-03,  4.09973301e-02,  1.42312706e-01,  2.40932275e-02,\n",
       "        4.18159030e-02,  6.32313415e-02,  1.60634108e-02,  1.29304737e-01,\n",
       "        7.80567676e-02,  2.66381074e-02,  1.44610971e-01,  1.17256530e-01,\n",
       "        1.68533653e-01,  5.93937188e-03,  1.50962383e-01,  4.09760922e-02,\n",
       "        1.16172977e-01,  1.51747316e-01,  9.54811200e-02, -2.78127640e-02,\n",
       "       -4.09419090e-03, -3.51824909e-02,  2.77394168e-02,  3.66193764e-02,\n",
       "        5.01674563e-02, -3.18895355e-02,  6.53255731e-02,  1.98074043e-01,\n",
       "        3.52244340e-02,  4.10112590e-02,  1.37729257e-01,  6.58506006e-02,\n",
       "        4.17142957e-02, -1.51087791e-02,  1.04195967e-01, -1.78660080e-03,\n",
       "       -3.22424248e-02,  4.10136469e-02,  2.94747278e-02,  1.37587547e-01,\n",
       "        3.21444497e-02,  7.58672878e-03, -2.64696777e-04,  1.40750185e-02,\n",
       "        2.77574137e-02,  1.39395714e-01,  4.71267514e-02, -1.10537224e-02,\n",
       "        5.68901226e-02,  9.82594043e-02,  1.85912885e-02,  2.20954549e-02,\n",
       "       -6.98662177e-03, -4.16990295e-02,  3.55188996e-02, -2.55167857e-02,\n",
       "       -1.65054426e-02,  1.96543485e-01, -1.07840709e-02,  5.88565990e-02,\n",
       "        1.08843014e-01,  8.69775489e-02,  1.48945421e-01, -7.14630261e-03,\n",
       "        3.51846404e-02,  2.05408130e-02,  6.81412518e-02, -2.59118825e-02,\n",
       "        1.52146667e-02,  6.13955744e-02,  4.09634188e-02, -3.10244113e-02,\n",
       "       -2.73301080e-03,  1.11105606e-01, -3.62746269e-02, -1.90676264e-02,\n",
       "        7.03678280e-02,  3.26640867e-02, -3.44658196e-02,  1.50969446e-01,\n",
       "        1.55000806e-01,  4.09653410e-02,  1.39927596e-01,  8.39352012e-02,\n",
       "       -2.32875571e-02,  8.68353993e-03, -2.74939686e-02,  7.28274584e-02,\n",
       "        5.48648946e-02,  5.66287972e-02,  1.88938230e-01,  2.07495727e-02,\n",
       "       -2.42349505e-02,  1.52943820e-01, -2.75351033e-02,  1.11912325e-01,\n",
       "        4.10153531e-02,  6.84628263e-03,  4.09801789e-02,  3.86239700e-02,\n",
       "        5.60835376e-03,  4.09747958e-02,  6.92525133e-03,  1.37540996e-01,\n",
       "        3.78213562e-02,  1.20342433e-01,  4.06510457e-02,  2.39939354e-02,\n",
       "        1.24184944e-01,  1.89210236e-01,  5.17626628e-02,  2.08961457e-01,\n",
       "        4.10075076e-02, -2.48767659e-02,  3.74585576e-02,  4.72639389e-02,\n",
       "        1.64461374e-01,  2.11355593e-02,  2.11822927e-01,  9.85963792e-02,\n",
       "        8.09227154e-02,  5.03037907e-02, -3.06281596e-02,  1.21842004e-01,\n",
       "       -1.38124824e-03,  1.98208869e-01,  3.72570008e-02,  1.59079880e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "save_obj(pred, \"../pred_loss_hyb_Xx.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
