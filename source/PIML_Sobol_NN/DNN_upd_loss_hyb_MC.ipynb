{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, Adadelta, Adagrad, Adam, Nadam, SGD\n",
    "from keras.callbacks import EarlyStopping, TerminateOnNaN\n",
    "from keras import backend as K\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.models import load_model, Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Normalize the data.\n",
    "from sklearn import preprocessing\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "import random\n",
    "\n",
    "def pass_arg(nsim, tr_size):\n",
    "    print(\"Tr_size:\", tr_size)\n",
    "    def fix_seeds(seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "        sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    #     K.set_session(sess)\n",
    "        tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "    ss = 1\n",
    "    fix_seeds(ss)\n",
    "\n",
    "    # MC dropout\n",
    "    class MCDropout(Dropout):\n",
    "        def call(self, inputs, training=None):\n",
    "            return super(MCDropout, self).call(inputs, training=True)\n",
    "\n",
    "    # import pickle\n",
    "\n",
    "    # def save_obj(obj, name):\n",
    "    #     with open(name, 'wb') as f:\n",
    "    #         pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "    # Compute the RMSE given the ground truth (y_true) and the predictions(y_pred)\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "            return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "    # Making sure dimensionless bond length is less than 1\n",
    "    def bond(bl):\n",
    "        return tf.add(K.relu(tf.negative(bl)), K.relu(bl-1.0))\n",
    "\n",
    "    # Making sure final porosity is less than initial\n",
    "    def poros(poroi, porof):\n",
    "        return K.relu(tf.negative(porof)) + K.relu(porof-poroi)\n",
    "\n",
    "    def strength1(bl, porof, nlayer=6):\n",
    "        sigma01, sigma02, C1s = 6, 31, 21\n",
    "        sigma_long = sigma01*(K.exp((1.0-porof)**(C1s*nlayer))-porof) + sigma02*(1.0-porof)\n",
    "        n = K.shape(sigma_long)[0]  \n",
    "        sorted_strength, sortedIndices = tf.math.top_k(sigma_long, n, True)\n",
    "        sorted_bl = K.gather(bl, sortedIndices)\n",
    "        sorted_porof = K.gather(porof, sortedIndices)\n",
    "        argg = tf.argsort(sorted_bl,axis=-1,direction='DESCENDING',stable=False,name=None)\n",
    "        sorted_bl_corr = K.gather(sorted_bl, argg)\n",
    "        return sorted_bl_corr-sorted_bl\n",
    "\n",
    "\n",
    "    def strength2(bl, porof, nlayer=6):\n",
    "        sigma01, sigma02, C1s = 6, 31, 21\n",
    "        sigma_long = sigma01*(K.exp((1.0-porof)**(C1s*nlayer))-porof) + sigma02*(1.0-porof)\n",
    "        n = K.shape(sigma_long)[0]  \n",
    "        sorted_strength, sortedIndices = tf.math.top_k(sigma_long, n, True)\n",
    "        sorted_bl = K.gather(bl, sortedIndices)\n",
    "        n = K.cast(n, tf.float32)\n",
    "        rel = K.relu(sorted_bl[1:]-sorted_bl[0:-1])\n",
    "        num_vio = K.cast(tf.math.count_nonzero(rel), tf.float32)\n",
    "        return num_vio/n\n",
    "\n",
    "\n",
    "    def phy_loss_mean(params):\n",
    "        # useful for cross-checking training\n",
    "        loss1, loss2, loss3, loss4, lam1, lam2, lam3, lam4 = params\n",
    "        def loss(y_true,y_pred):\n",
    "    #         return lam1*K.mean(K.relu(loss1)) + lam2*K.mean(K.relu(loss2)) + lam2*K.mean(K.relu(loss3))\n",
    "            return lam1*K.mean(K.relu(loss1)) + lam2*K.mean(K.relu(loss2)) + lam3*K.mean(K.relu(loss3)) + lam4*loss4\n",
    "        return loss\n",
    "\n",
    "    #function to calculate the combined loss = sum of rmse and phy based loss\n",
    "    def combined_loss(params):\n",
    "        loss1, loss2, loss3, loss4, lam1, lam2, lam3, lam4 = params\n",
    "        def loss(y_true,y_pred):\n",
    "    #         return mean_squared_error(y_true, y_pred) + lam1 * K.mean(K.relu(loss1)) + lam2 * K.mean(K.relu(loss2)) + lam2 * K.mean(K.relu(loss3))\n",
    "            return mean_squared_error(y_true, y_pred) + lam1 * K.mean(K.relu(loss1)) + lam2 * K.mean(K.relu(loss2)) + lam3*K.mean(K.relu(loss3)) + lam4 * loss4\n",
    "        return loss\n",
    "\n",
    "    def PGNN_train_test(optimizer_name, optimizer_val, use_YPhy, pre_train, tr_size, lamda, iteration, n_nodes, n_layers, drop_frac, reg, samp):\n",
    "\n",
    "    #     fix_seeds(ss)\n",
    "\n",
    "        # Hyper-parameters of the training process\n",
    "    #     batch_size = tr_size\n",
    "        batch_size = 1\n",
    "        num_epochs = 50\n",
    "        val_frac = 0.2\n",
    "        patience_val = 50\n",
    "\n",
    "        # Initializing results filename\n",
    "        exp_name = \"fine-tuned_\" + pre_train + optimizer_name + '_usePhy' + str(use_YPhy) + '_trsize' + str(tr_size) + '_lamda' + str(lamda) + '_iter' + str(iteration)\n",
    "        exp_name = exp_name.replace('.','pt')\n",
    "        results_dir = '../results/'\n",
    "        model_name = results_dir + exp_name + '_model.h5' # storing the trained model\n",
    "\n",
    "        if reg==True and samp==25:\n",
    "            results_name = results_dir + exp_name + '_results_25_regularizer.dat' # storing the results of the model\n",
    "        elif reg==False and samp==25:\n",
    "            results_name = results_dir + exp_name + '_results_25.dat' # storing the results of the model\n",
    "        elif reg==True and samp==1519:\n",
    "            results_name = results_dir + exp_name + '_results_1519_regularizer.dat' # storing the results of the model\n",
    "        elif reg==False and samp==1519:\n",
    "            results_name = results_dir + exp_name + '_results_1519.dat' # storing the results of the model\n",
    "\n",
    "        # Load labeled data\n",
    "        data = np.loadtxt('../data/labeled_data.dat')\n",
    "    #     data = np.loadtxt('../data/labeled_data_BK_constw_unique.dat')\n",
    "    #     data = np.loadtxt('../data/labeled_data_BK_constw_v2.dat')\n",
    "#         x_labeled = data[:, :2] # -2 because we do not need porosity predictions\n",
    "        x_label = data[:, :-3] # -2 because we do not need porosity predictions\n",
    "        x_labeled = np.hstack((x_label[:,:2],x_label[:,-2:]))\n",
    "        y_labeled = data[:, -3:-1] # dimensionless bond length and porosity measurements\n",
    "        if samp==25:\n",
    "            data = np.loadtxt('../data/unlabeled_data_BK_constw_v2_25.dat')\n",
    "            x_unlabeled = data[:, :]\n",
    "        elif samp==1519:\n",
    "            data = np.loadtxt('../data/unlabeled_data_BK_constw_v2_1525.dat')\n",
    "            x_unlabeled = data[:, :]\n",
    "\n",
    "        x_unlabeled1 = x_unlabeled[:1303, :]\n",
    "        x_unlabeled2 = x_unlabeled[-6:, :]\n",
    "        x_unlabeled = np.vstack((x_unlabeled1,x_unlabeled2))\n",
    "\n",
    "        # initial porosity\n",
    "        init_poro = x_unlabeled[:, -1]\n",
    "        x_unlabeled = np.hstack((x_unlabeled[:,:2],x_unlabeled[:,-3:-1]))\n",
    "#         x_unlabeled = x_unlabeled[:, :2]\n",
    "\n",
    "\n",
    "        # normalize dataset with MinMaxScaler\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "    #     scaler = preprocessing.StandardScaler()\n",
    "        x_labeled = scaler.fit_transform(x_labeled)\n",
    "        x_unlabeled = scaler.fit_transform(x_unlabeled)\n",
    "#         y_labeled = scaler.fit_transform(y_labeled)\n",
    "\n",
    "    #     # initial porosity & physics outputs are removed\n",
    "    #     x_unlabeled = x_unlabeled[:, :-3]\n",
    "\n",
    "        # train and test data\n",
    "        trainX, trainY = x_labeled[:tr_size,:], y_labeled[:tr_size]\n",
    "    #     testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]   \n",
    "        testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]\n",
    "\n",
    "        if use_YPhy == 0:\n",
    "            # Removing the last column from x_unlabeled (corresponding to Y_PHY)\n",
    "            x_unlabeled = x_unlabeled[:,:-1]\n",
    "\n",
    "        dependencies = {'root_mean_squared_error': root_mean_squared_error}\n",
    "\n",
    "        # load the pre-trained model using non-calibrated physics-based model predictions (./data/unlabeled.dat)\n",
    "        loaded_model = load_model(results_dir + pre_train, custom_objects=dependencies)\n",
    "\n",
    "        # Creating the model\n",
    "        model = Sequential()\n",
    "        for layer in np.arange(n_layers):\n",
    "            if layer == 0:\n",
    "                model.add(Dense(n_nodes, activation='relu', input_shape=(np.shape(trainX)[1],)))\n",
    "            else:\n",
    "                if reg:\n",
    "                    model.add(Dense(n_nodes, activation='relu', kernel_regularizer=l1_l2(l1=.001, l2=.001)))\n",
    "                else:\n",
    "                    model.add(Dense(n_nodes, activation='relu'))\n",
    "            # model.add(Dropout(rate=drop_frac))\n",
    "            model.add(MCDropout(rate=drop_frac))\n",
    "        model.add(Dense(2, activation='linear'))\n",
    "\t\t\n",
    "\t\t# pass the weights to all layers but 1st input layer, whose dimensions are updated\n",
    "        for new_layer, layer in zip(model.layers[1:], loaded_model.layers[1:]):\n",
    "            new_layer.set_weights(layer.get_weights())\n",
    "\n",
    "        # physics-based regularization\n",
    "        uinp_sc = K.constant(value=x_unlabeled) # unlabeled input data\n",
    "        lam1 = K.constant(value=lamda[0]) # regularization hyper-parameter\n",
    "        lam2 = K.constant(value=lamda[1]) # regularization hyper-parameter\n",
    "        lam3 = K.constant(value=lamda[2]) # regularization hyper-parameter\n",
    "        lam4 = K.constant(value=lamda[3]) # regularization hyper-parameter\n",
    "        predictions = model(uinp_sc) # model output at depth i\n",
    "    #     porosity = K.relu(predictions[:,1])\n",
    "        phyloss1 = bond(predictions[:,0]) # physics loss 1\n",
    "    #     uinp = K.constant(value=x_unlabeled_non) # unlabeled input data\n",
    "        phyloss2 = poros(init_poro, predictions[:,1]) # physics loss 1\n",
    "        phyloss3 = strength1(predictions[:,0], predictions[:,1])\n",
    "        phyloss4 = strength2(predictions[:,0], predictions[:,1])\n",
    "        totloss = combined_loss([phyloss1, phyloss2, phyloss3, phyloss4, lam1, lam2, lam3, lam4])\n",
    "        phyloss = phy_loss_mean([phyloss1, phyloss2, phyloss3, phyloss4, lam1, lam2, lam3, lam4])\n",
    "\n",
    "\n",
    "        model.compile(loss=totloss,\n",
    "                      optimizer=optimizer_val,\n",
    "                      metrics=[phyloss, root_mean_squared_error])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience_val, verbose=1)\n",
    "\n",
    "    #     print('Running...' + optimizer_name)\n",
    "        history = model.fit(trainX, trainY,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=num_epochs,\n",
    "                            verbose=0,\n",
    "                            validation_split=val_frac, callbacks=[early_stopping, TerminateOnNaN()])\n",
    "    \n",
    "#     early_stopping = EarlyStopping(monitor='loss', patience=patience_val, verbose=1)\n",
    "#     history = model.fit(trainX, trainY,\n",
    "#                         batch_size=batch_size,\n",
    "#                         epochs=num_epochs,\n",
    "#                         verbose=1,\n",
    "#                         callbacks=[early_stopping, TerminateOnNaN()])\n",
    "\n",
    "#     test_score = model.evaluate(testX, testY, verbose=0)\n",
    "#     predictions = model.predict(x_labeled) # model output at depth i\n",
    "#     print(np.sort(predictions[:,0], axis=0))\n",
    "    \n",
    "#     predictions = model.predict(x_unlabeled) # model output at depth i\n",
    "#     print(np.sort(predictions[:,0], axis=0))\n",
    "    \n",
    "#     print('iter: ' + str(iteration) + ' useYPhy: ' + str(use_YPhy) + \n",
    "#           ' nL: ' + str(n_layers) + ' nN: ' + str(n_nodes) + \n",
    "#           ' lamda1: ' + str(lamda[0]) + ' lamda2: ' + str(lamda[1]) + ' trsize: ' + str(tr_size) + \n",
    "#           ' TestRMSE: ' + str(test_score[2]) + ' PhyLoss: ' + str(test_score[1]), ' TestLoss: ' + str(test_score[0]), \"\\n\")\n",
    "\n",
    "# #     print('iter: ' + str(iteration) + ' TestRMSE: ' + str(test_score[2]) + ' PhyLoss: ' + str(test_score[1]), \"\\n\")\n",
    "\n",
    "    \n",
    "# #     model.save(model_name)\n",
    "    \n",
    "#     # save results\n",
    "#     results = {'train_loss_1':history.history['loss_1'], \n",
    "#                                 'val_loss_1':history.history['val_loss_1'], \n",
    "#                                 'train_rmse':history.history['root_mean_squared_error'], \n",
    "#                                 'val_rmse':history.history['val_root_mean_squared_error'],\n",
    "#                                 'test_rmse':test_score[2],\n",
    "#                                 'PhyLoss':test_score[1]}\n",
    "\n",
    "#     results = {'train_loss_1':history.history['loss_1'], \n",
    "#                                 'train_rmse':history.history['root_mean_squared_error'], \n",
    "#                                 'test_rmse':test_score[2],\n",
    "#                                 'PhyLoss':test_score[1]}\n",
    "\n",
    "    \n",
    "\n",
    "#     save_obj(results, results_name)\n",
    "\n",
    "#     predictions = model.predict(testX)\n",
    "#     return results, results_name, predictions, testY, test_score[2], trainY\n",
    "    \n",
    "        test_score = model.evaluate(testX, testY, verbose=1)\n",
    "        print(test_score)\n",
    "\n",
    "        Xx = np.random.uniform(0,1,(1000,2))\n",
    "        xx1 = np.ones((1000,2))\n",
    "        Xx = np.hstack((Xx,xx1))\n",
    "        \n",
    "        samples = []\n",
    "        for i in range(int(nsim)):\n",
    "#             print(\"simulation num:\",i)\n",
    "            predictions = model.predict(Xx)\n",
    "            predictions = predictions[:,1]\n",
    "            samples.append(predictions)\n",
    "        return np.array(samples), testY\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Main Function\n",
    "    if __name__ == '__main__':\n",
    "\n",
    "        fix_seeds(1)\n",
    "\n",
    "        # List of optimizers to choose from    \n",
    "        optimizer_names = ['Adagrad', 'Adadelta', 'Adam', 'Nadam', 'RMSprop', 'SGD', 'NSGD']\n",
    "        optimizer_vals = [Adagrad(clipnorm=1), Adadelta(clipnorm=1), Adam(clipnorm=1), Nadam(clipnorm=1), RMSprop(clipnorm=1), SGD(clipnorm=1.), SGD(clipnorm=1, nesterov=True)]\n",
    "\n",
    "        # selecting the optimizer\n",
    "        optimizer_num = 1\n",
    "        optimizer_name = optimizer_names[optimizer_num]\n",
    "        optimizer_val = optimizer_vals[optimizer_num]\n",
    "\n",
    "        # Selecting Other Hyper-parameters\n",
    "        drop_frac = 0.1 # Fraction of nodes to be dropped out\n",
    "        use_YPhy = 1 # Whether YPhy is used as another feature in the NN model or not\n",
    "        n_layers = 2 # Number of hidden layers\n",
    "        n_nodes = 5 # Number of nodes per hidden layer\n",
    "\n",
    "        #set lamda\n",
    "        lamda = [0.3, 0.15, 0.008, 0] # Physics-based regularization constant  \n",
    "\n",
    "#         # Iterating over different training fractions and splitting indices for train-test splits\n",
    "#         trsize_range = [4,6,8,10,20]\n",
    "\n",
    "#         #default training size = 5000\n",
    "#         tr_size = trsize_range[4]\n",
    "        \n",
    "        tr_size = int(tr_size)\n",
    "        pre_train = 'Pre-trainAdadelta_drop0_nL2_nN5_trsize1308_iter0.h5'\n",
    "\n",
    "        # use regularizer\n",
    "        reg = True\n",
    "\n",
    "        # sample size used\n",
    "        samp = 1519\n",
    "    #     samp = 25\n",
    "\n",
    "        # total number of runs\n",
    "        iter_range = np.arange(1)\n",
    "        testrmse=[]\n",
    "        # iterating through all possible params\n",
    "        for iteration in iter_range:\n",
    "#             results, result_file, pred, obs, rmse, obs_train = PGNN_train_test(optimizer_name, optimizer_val, drop_frac, use_YPhy, \n",
    "#                             iteration, n_layers, n_nodes, tr_size, lamda, reg, samp)\n",
    "#             testrmse.append(rmse)\n",
    "            pred, obs = PGNN_train_test(optimizer_name, optimizer_val, use_YPhy, \n",
    "                                               pre_train, tr_size, lamda, iteration, n_nodes, n_layers, drop_frac, reg, samp)\n",
    "            \n",
    "\n",
    "    return np.squeeze(pred), obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr_size: 20\n",
      "19/19 [==============================] - 0s 52us/step\n",
      "[0.022604381665587425, 0.0013917512260377407, 0.10428124666213989]\n"
     ]
    }
   ],
   "source": [
    "pred, obs = pass_arg(50, 20)\n",
    "mc_pred=np.mean(pred,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Plot training & validation accuracy values\n",
    "# fig, ax = plt.subplots()\n",
    "# plt.plot(mc_pred, obs[:,1], 'd')\n",
    "# plt.ylim((-0.01, 0.1))\n",
    "# plt.xlim((-0.01, 0.1))\n",
    "# ax.plot([-0.01,1],[-0.01,1], transform=ax.transAxes, color='k')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05958226, 0.06915719, 0.05733197, 0.0398545 , 0.04122408,\n",
       "       0.09252442, 0.03038587, 0.03382599, 0.0660064 , 0.04784298,\n",
       "       0.04593347, 0.03304945, 0.04339615, 0.04290608, 0.03188588,\n",
       "       0.06057683, 0.03654942, 0.02875252, 0.04120921, 0.08028256,\n",
       "       0.04207018, 0.03816717, 0.0643486 , 0.03541319, 0.04029098,\n",
       "       0.05851   , 0.06464598, 0.02462157, 0.03286206, 0.07815065,\n",
       "       0.04120263, 0.05631733, 0.03737757, 0.03490303, 0.05098108,\n",
       "       0.05251754, 0.02371311, 0.03224104, 0.04958144, 0.04693342,\n",
       "       0.06497452, 0.06144594, 0.09044453, 0.05519437, 0.04809144,\n",
       "       0.03877452, 0.057241  , 0.0752776 , 0.04399383, 0.06202547,\n",
       "       0.02034918, 0.02462558, 0.04870855, 0.04823713, 0.04324883,\n",
       "       0.06696029, 0.02283373, 0.02903545, 0.05433143, 0.04016906,\n",
       "       0.06920074, 0.0400499 , 0.06008915, 0.05823291, 0.03438285,\n",
       "       0.05087456, 0.01263563, 0.02946496, 0.05150693, 0.06455147,\n",
       "       0.04615134, 0.02385854, 0.07156917, 0.08996201, 0.04686141,\n",
       "       0.03196576, 0.02757379, 0.05190719, 0.10078086, 0.03167984,\n",
       "       0.02146128, 0.08760366, 0.05064651, 0.03306507, 0.07063229,\n",
       "       0.09789705, 0.07093234, 0.09720037, 0.0922581 , 0.03882684,\n",
       "       0.06805267, 0.02825798, 0.04892369, 0.08805336, 0.07656418,\n",
       "       0.04886227, 0.07709756, 0.04430406, 0.0593296 , 0.02229968,\n",
       "       0.05931746, 0.02817431, 0.0558503 , 0.04468114, 0.03843036,\n",
       "       0.02576   , 0.04168301, 0.03496978, 0.05603911, 0.02791739,\n",
       "       0.02363135, 0.08159015, 0.03052686, 0.03123228, 0.04625303,\n",
       "       0.03517349, 0.06164001, 0.06741397, 0.05629066, 0.06567572,\n",
       "       0.01746991, 0.04644651, 0.11041325, 0.04005969, 0.0689636 ,\n",
       "       0.04296199, 0.048808  , 0.07364359, 0.10984278, 0.04021391,\n",
       "       0.0585687 , 0.03451698, 0.03038718, 0.03680945, 0.08571161,\n",
       "       0.05604466, 0.06998058, 0.03047146, 0.05972895, 0.03454755,\n",
       "       0.09549183, 0.04516862, 0.0334466 , 0.03657816, 0.01222473,\n",
       "       0.0659751 , 0.02308661, 0.04219776, 0.03024575, 0.1101939 ,\n",
       "       0.08151399, 0.02467377, 0.05746079, 0.0234069 , 0.04704641,\n",
       "       0.02567576, 0.02849929, 0.02847678, 0.02927313, 0.03465592,\n",
       "       0.06551064, 0.04348793, 0.05392639, 0.05650704, 0.05881575,\n",
       "       0.0257071 , 0.03781553, 0.07558007, 0.10120038, 0.04227916,\n",
       "       0.04152276, 0.02326252, 0.06195382, 0.03023117, 0.02949137,\n",
       "       0.04526781, 0.02929268, 0.0350136 , 0.11468256, 0.06355058,\n",
       "       0.06309158, 0.08832002, 0.04554272, 0.02376267, 0.04807246,\n",
       "       0.04370548, 0.08846816, 0.01431761, 0.06308336, 0.02876123,\n",
       "       0.06561036, 0.07573309, 0.02660647, 0.04891549, 0.04917493,\n",
       "       0.01970413, 0.07936274, 0.04256472, 0.09922017, 0.03505762,\n",
       "       0.01820185, 0.04515201, 0.03421429, 0.05586887, 0.0178348 ,\n",
       "       0.01085625, 0.04778424, 0.0491524 , 0.07364621, 0.09172422,\n",
       "       0.03929348, 0.08196705, 0.01659833, 0.11511502, 0.06967877,\n",
       "       0.03789755, 0.03568001, 0.04265357, 0.05609102, 0.04101706,\n",
       "       0.03256838, 0.06535723, 0.037526  , 0.05525069, 0.03541743,\n",
       "       0.0497512 , 0.09770922, 0.04896509, 0.0406139 , 0.01953572,\n",
       "       0.09508861, 0.04259298, 0.05028689, 0.04683154, 0.04998703,\n",
       "       0.05839518, 0.04580894, 0.04726841, 0.04798068, 0.07007506,\n",
       "       0.03241482, 0.11514207, 0.06098633, 0.04652274, 0.0591919 ,\n",
       "       0.05391334, 0.02186887, 0.02851157, 0.05003363, 0.04762889,\n",
       "       0.11610988, 0.01830927, 0.04225822, 0.04279928, 0.02940769,\n",
       "       0.04286301, 0.06263123, 0.0413212 , 0.05565585, 0.13065663,\n",
       "       0.03265889, 0.04044494, 0.03926944, 0.08235157, 0.04173639,\n",
       "       0.03944528, 0.03956416, 0.063301  , 0.03859089, 0.0877943 ,\n",
       "       0.06354284, 0.04433562, 0.1306655 , 0.03525636, 0.05199149,\n",
       "       0.02775028, 0.02269203, 0.02605296, 0.02983831, 0.08254841,\n",
       "       0.03360214, 0.03649632, 0.13437812, 0.07785331, 0.02828266,\n",
       "       0.11249544, 0.04107337, 0.06674787, 0.04811706, 0.02352982,\n",
       "       0.04773892, 0.04189864, 0.05730037, 0.06764431, 0.05459324,\n",
       "       0.05528334, 0.02777865, 0.05541456, 0.0374775 , 0.03611891,\n",
       "       0.04780278, 0.03810971, 0.05857333, 0.10442881, 0.03506036,\n",
       "       0.10648808, 0.04159129, 0.05792127, 0.04346126, 0.07144215,\n",
       "       0.05304193, 0.0495451 , 0.03065235, 0.10001395, 0.03073106,\n",
       "       0.07991026, 0.02456818, 0.06973289, 0.04893182, 0.06075643,\n",
       "       0.09488379, 0.05900395, 0.03337206, 0.04976317, 0.10373116,\n",
       "       0.07188195, 0.03669777, 0.04319066, 0.06182011, 0.02599307,\n",
       "       0.04018085, 0.05817261, 0.03979234, 0.03980849, 0.04136375,\n",
       "       0.04030698, 0.06483911, 0.06524329, 0.06790271, 0.03983877,\n",
       "       0.02363903, 0.08113174, 0.11104447, 0.04623986, 0.03925354,\n",
       "       0.02049396, 0.04886247, 0.08371986, 0.0637733 , 0.0437465 ,\n",
       "       0.05337019, 0.05749724, 0.03945246, 0.07333462, 0.10504041,\n",
       "       0.080019  , 0.07555857, 0.05295616, 0.03576102, 0.0564757 ,\n",
       "       0.04054091, 0.07231855, 0.10391691, 0.05810319, 0.07546938,\n",
       "       0.09947464, 0.03713967, 0.1146792 , 0.0323177 , 0.11019176,\n",
       "       0.01258047, 0.03523299, 0.04207495, 0.02930574, 0.06740665,\n",
       "       0.04773623, 0.05203482, 0.03946571, 0.05629585, 0.01250742,\n",
       "       0.0277017 , 0.05907617, 0.05113668, 0.02043545, 0.07126972,\n",
       "       0.03656566, 0.11643356, 0.00694264, 0.04395451, 0.04516476,\n",
       "       0.03196015, 0.06780308, 0.03665281, 0.0893034 , 0.04550993,\n",
       "       0.05253063, 0.09270189, 0.05025559, 0.03826398, 0.02803076,\n",
       "       0.02951824, 0.08894842, 0.11227801, 0.03862404, 0.02965523,\n",
       "       0.03070879, 0.03658073, 0.05710466, 0.04534535, 0.0364588 ,\n",
       "       0.01883608, 0.04126808, 0.02140127, 0.12185691, 0.04240232,\n",
       "       0.0175422 , 0.04746737, 0.03158407, 0.03652517, 0.04359576,\n",
       "       0.04496587, 0.1014121 , 0.09711357, 0.03049388, 0.05627058,\n",
       "       0.11497946, 0.07824335, 0.0621153 , 0.02683743, 0.07403822,\n",
       "       0.04263357, 0.01735321, 0.05324522, 0.0449051 , 0.05242922,\n",
       "       0.03249833, 0.02486266, 0.05140948, 0.05079858, 0.07450058,\n",
       "       0.05770458, 0.04020057, 0.05453711, 0.05580329, 0.07578655,\n",
       "       0.08611244, 0.0826498 , 0.03728171, 0.03095373, 0.074194  ,\n",
       "       0.03751158, 0.02750256, 0.06711913, 0.01080312, 0.06368497,\n",
       "       0.05292466, 0.04910739, 0.01390362, 0.09594867, 0.04964591,\n",
       "       0.05821722, 0.03732529, 0.04964425, 0.05060175, 0.08754519,\n",
       "       0.04342224, 0.05080879, 0.0529711 , 0.09071061, 0.0639579 ,\n",
       "       0.07344451, 0.04504743, 0.0700365 , 0.03961636, 0.04915975,\n",
       "       0.05203207, 0.04899084, 0.04229387, 0.05085091, 0.04311023,\n",
       "       0.0560917 , 0.05896467, 0.0755826 , 0.0354112 , 0.09970698,\n",
       "       0.02512556, 0.05525371, 0.07857641, 0.06326129, 0.05927321,\n",
       "       0.04575238, 0.04425687, 0.04174971, 0.05302851, 0.02524667,\n",
       "       0.0399693 , 0.04433468, 0.05256655, 0.03978759, 0.02528151,\n",
       "       0.02833404, 0.02923825, 0.09154885, 0.04845389, 0.04037357,\n",
       "       0.10989668, 0.08044886, 0.03675508, 0.07122276, 0.0866578 ,\n",
       "       0.03362419, 0.06043344, 0.10366222, 0.02283532, 0.04445856,\n",
       "       0.12956828, 0.02929422, 0.02060578, 0.10956537, 0.05764297,\n",
       "       0.03551265, 0.05122692, 0.08545616, 0.08702734, 0.0458503 ,\n",
       "       0.02483859, 0.03898869, 0.02445988, 0.03982174, 0.04447298,\n",
       "       0.04690072, 0.04515334, 0.08456228, 0.07740408, 0.05340232,\n",
       "       0.03370484, 0.04670842, 0.04407094, 0.10851808, 0.04308859,\n",
       "       0.01849175, 0.03640819, 0.04079593, 0.06809442, 0.07011254,\n",
       "       0.03643799, 0.04002795, 0.05901105, 0.03205543, 0.09986643,\n",
       "       0.05817302, 0.06728955, 0.06837619, 0.02591527, 0.06795894,\n",
       "       0.04320079, 0.05089236, 0.01646613, 0.10194019, 0.06915206,\n",
       "       0.0814587 , 0.03879732, 0.05137372, 0.08541796, 0.07560552,\n",
       "       0.08909669, 0.08804914, 0.03083738, 0.05586258, 0.10620134,\n",
       "       0.02639301, 0.05252077, 0.04482917, 0.04654911, 0.04676027,\n",
       "       0.0524178 , 0.04634798, 0.03938427, 0.03134976, 0.03851172,\n",
       "       0.04614824, 0.06659193, 0.04927092, 0.03990768, 0.08432474,\n",
       "       0.0277573 , 0.10537026, 0.12172795, 0.06176082, 0.05223925,\n",
       "       0.02532464, 0.07345311, 0.06185649, 0.0462436 , 0.0236899 ,\n",
       "       0.03890069, 0.04115494, 0.04124025, 0.07287449, 0.0390811 ,\n",
       "       0.05046261, 0.0425522 , 0.07835849, 0.03224806, 0.05811309,\n",
       "       0.04428771, 0.03879599, 0.09476269, 0.04440459, 0.00717985,\n",
       "       0.06579388, 0.05851047, 0.05376702, 0.04474438, 0.10510333,\n",
       "       0.07881401, 0.09982207, 0.05344629, 0.0678774 , 0.06168379,\n",
       "       0.04147523, 0.05488088, 0.04965917, 0.12072785, 0.07895622,\n",
       "       0.03931117, 0.08469638, 0.06683405, 0.063154  , 0.04793908,\n",
       "       0.04150052, 0.06335833, 0.02746288, 0.07448803, 0.03801189,\n",
       "       0.08540136, 0.03934788, 0.06119811, 0.05957636, 0.05267123,\n",
       "       0.02993918, 0.03926517, 0.02987204, 0.0311946 , 0.05326032,\n",
       "       0.01407807, 0.04334584, 0.04362377, 0.06473666, 0.0959163 ,\n",
       "       0.08802969, 0.02919215, 0.06406144, 0.07097426, 0.02386237,\n",
       "       0.02153881, 0.04045602, 0.03438653, 0.1105142 , 0.05970594,\n",
       "       0.06585047, 0.07256766, 0.04080485, 0.05171274, 0.06920099,\n",
       "       0.05219601, 0.07466889, 0.06003106, 0.04074209, 0.02004782,\n",
       "       0.05013106, 0.03451289, 0.11718195, 0.0782871 , 0.02999017,\n",
       "       0.08339121, 0.03027475, 0.08464632, 0.05542345, 0.04728249,\n",
       "       0.02804757, 0.03000633, 0.02616343, 0.06030134, 0.00831902,\n",
       "       0.03887698, 0.04708081, 0.05398757, 0.02263889, 0.02941621,\n",
       "       0.03553155, 0.03849048, 0.03693075, 0.06923471, 0.01778425,\n",
       "       0.04957342, 0.08378087, 0.04576396, 0.03520342, 0.02634727,\n",
       "       0.05398413, 0.10702597, 0.08442836, 0.07089655, 0.06487592,\n",
       "       0.06010353, 0.02306991, 0.07915817, 0.08452035, 0.06212623,\n",
       "       0.05992559, 0.11618165, 0.05133508, 0.04649949, 0.05767535,\n",
       "       0.07832869, 0.07111599, 0.05174257, 0.03608309, 0.05095547,\n",
       "       0.05767254, 0.03250145, 0.02486914, 0.05050809, 0.04596717,\n",
       "       0.0454823 , 0.0867502 , 0.04948269, 0.0482464 , 0.02320835,\n",
       "       0.06042177, 0.04022138, 0.0518644 , 0.03635478, 0.04183247,\n",
       "       0.04656819, 0.05025596, 0.06716838, 0.01935228, 0.05872595,\n",
       "       0.02918713, 0.04799429, 0.030808  , 0.03308811, 0.08189247,\n",
       "       0.07482158, 0.11013617, 0.01898626, 0.06458166, 0.03772883,\n",
       "       0.08308738, 0.0977472 , 0.04250972, 0.03183943, 0.02372192,\n",
       "       0.02269674, 0.06791797, 0.03786502, 0.04561126, 0.05906262,\n",
       "       0.05012291, 0.03937519, 0.04980351, 0.05212123, 0.11348329,\n",
       "       0.07255396, 0.05140932, 0.07138369, 0.04226003, 0.02542016,\n",
       "       0.06385299, 0.0756531 , 0.05079671, 0.01262488, 0.03223141,\n",
       "       0.036306  , 0.06299911, 0.0664939 , 0.03269387, 0.01729939,\n",
       "       0.02809643, 0.03966794, 0.03646867, 0.08486418, 0.07786423,\n",
       "       0.05436908, 0.04675097, 0.06314149, 0.07078145, 0.04577342,\n",
       "       0.07729603, 0.04658266, 0.07500342, 0.11376266, 0.05488236,\n",
       "       0.04250836, 0.03951778, 0.05332968, 0.01406636, 0.06173879,\n",
       "       0.03402424, 0.04010389, 0.08827662, 0.04368894, 0.03967145,\n",
       "       0.06123184, 0.03538711, 0.04261533, 0.05654206, 0.0482605 ,\n",
       "       0.06460103, 0.10658046, 0.03559117, 0.02420928, 0.09830951,\n",
       "       0.06772059, 0.05720922, 0.04207816, 0.11674756, 0.03998513,\n",
       "       0.10377202, 0.04907184, 0.05545621, 0.02777565, 0.03682284,\n",
       "       0.03846758, 0.0527226 , 0.03356947, 0.06093385, 0.03852294,\n",
       "       0.03667657, 0.03762425, 0.06344921, 0.04497539, 0.0406239 ,\n",
       "       0.01467807, 0.02713582, 0.0692301 , 0.09894581, 0.03454851,\n",
       "       0.10986543, 0.04387185, 0.05114289, 0.05776094, 0.0526598 ,\n",
       "       0.0352551 , 0.04061637, 0.05465772, 0.04675417, 0.09028754,\n",
       "       0.02971663, 0.02939375, 0.13059093, 0.03135658, 0.04540318,\n",
       "       0.09697132, 0.04724054, 0.0765751 , 0.04185669, 0.04732521,\n",
       "       0.04566126, 0.02764348, 0.07419441, 0.02383482, 0.03833338,\n",
       "       0.05372046, 0.04881215, 0.09191757, 0.06899746, 0.01851684,\n",
       "       0.09098974, 0.0775164 , 0.06671753, 0.05462592, 0.02728025,\n",
       "       0.03840392, 0.0557019 , 0.05000433, 0.07940309, 0.03056446,\n",
       "       0.04209292, 0.05733429, 0.02214065, 0.03778665, 0.02334378,\n",
       "       0.06844846, 0.03025034, 0.04026422, 0.03928435, 0.02797122,\n",
       "       0.04032202, 0.10244237, 0.0723867 , 0.10842709, 0.05359077,\n",
       "       0.04373077, 0.0542914 , 0.10918827, 0.04456263, 0.01224714,\n",
       "       0.04392374, 0.06632993, 0.03336293, 0.02685396, 0.04750156,\n",
       "       0.07595858, 0.03644875, 0.06051486, 0.08031169, 0.05295419,\n",
       "       0.06151896, 0.0329956 , 0.04233168, 0.05606735, 0.06457186,\n",
       "       0.09038821, 0.05054248, 0.03054965, 0.05269277, 0.06761188,\n",
       "       0.03507945, 0.03021401, 0.06239142, 0.06497142, 0.07146496,\n",
       "       0.11891502, 0.07169389, 0.07935271, 0.10717656, 0.01905846,\n",
       "       0.10506983, 0.0479312 , 0.03598269, 0.04546053, 0.01791505,\n",
       "       0.08210575, 0.04847954, 0.04693943, 0.05612532, 0.08342628,\n",
       "       0.06765403, 0.04902052, 0.03413849, 0.10487624, 0.0581844 ,\n",
       "       0.04000931, 0.11044525, 0.08728558, 0.03602146, 0.04049123,\n",
       "       0.09249125, 0.023632  , 0.03034472, 0.0407468 , 0.02943421,\n",
       "       0.04232606, 0.07443299, 0.05713984, 0.10730904, 0.03985211,\n",
       "       0.04735612, 0.05110713, 0.01562457, 0.05739261, 0.07527633,\n",
       "       0.01891381, 0.07371956, 0.03289994, 0.04301722, 0.07505749,\n",
       "       0.02982935, 0.05880006, 0.06286748, 0.04965683, 0.07349092,\n",
       "       0.03743769, 0.03927979, 0.03502338, 0.05892773, 0.07606813,\n",
       "       0.03733607, 0.01542945, 0.03667672, 0.01889128, 0.05462366,\n",
       "       0.10394137, 0.05272011, 0.0563313 , 0.02574247, 0.0791036 ,\n",
       "       0.01541226, 0.03395588, 0.03424072, 0.04189141, 0.12566961,\n",
       "       0.04184391, 0.06717986, 0.0125456 , 0.0309948 , 0.02719077],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01666471, 0.01358868, 0.05432798, 0.03850274, 0.01010751,\n",
       "       0.01129286, 0.009357  , 0.03579652, 0.01614062, 0.00770211,\n",
       "       0.0105933 , 0.03315482, 0.01352198, 0.05269549, 0.00490424,\n",
       "       0.00895544, 0.01003564, 0.01345683, 0.02496436])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "save_obj(mc_pred, \"../pred_upd_loss_hyb_MC_Xx.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
