{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, Adadelta, Adagrad, Adam, Nadam, SGD\n",
    "from keras.callbacks import EarlyStopping, TerminateOnNaN\n",
    "from keras import backend as K\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.models import load_model, Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Normalize the data.\n",
    "from sklearn import preprocessing\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "import random\n",
    "\n",
    "def pass_arg(nsim, tr_size):\n",
    "    print(\"Tr_size:\", tr_size)\n",
    "    def fix_seeds(seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "        sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    #     K.set_session(sess)\n",
    "        tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "    ss = 1\n",
    "    fix_seeds(ss)\n",
    "\n",
    "    # MC dropout\n",
    "    class MCDropout(Dropout):\n",
    "        def call(self, inputs, training=None):\n",
    "            return super(MCDropout, self).call(inputs, training=True)\n",
    "\n",
    "    # import pickle\n",
    "\n",
    "    # def save_obj(obj, name):\n",
    "    #     with open(name, 'wb') as f:\n",
    "    #         pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "    # Compute the RMSE given the ground truth (y_true) and the predictions(y_pred)\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "            return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "    # Making sure dimensionless bond length is less than 1\n",
    "    def bond(bl):\n",
    "        return tf.add(K.relu(tf.negative(bl)), K.relu(bl-1.0))\n",
    "\n",
    "    # Making sure final porosity is less than initial\n",
    "    def poros(poroi, porof):\n",
    "        return K.relu(tf.negative(porof)) + K.relu(porof-poroi)\n",
    "\n",
    "    def strength1(bl, porof, nlayer=6):\n",
    "        sigma01, sigma02, C1s = 6, 31, 21\n",
    "        sigma_long = sigma01*(K.exp((1.0-porof)**(C1s*nlayer))-porof) + sigma02*(1.0-porof)\n",
    "        n = K.shape(sigma_long)[0]  \n",
    "        sorted_strength, sortedIndices = tf.math.top_k(sigma_long, n, True)\n",
    "        sorted_bl = K.gather(bl, sortedIndices)\n",
    "        sorted_porof = K.gather(porof, sortedIndices)\n",
    "        argg = tf.argsort(sorted_bl,axis=-1,direction='DESCENDING',stable=False,name=None)\n",
    "        sorted_bl_corr = K.gather(sorted_bl, argg)\n",
    "        return sorted_bl_corr-sorted_bl\n",
    "\n",
    "\n",
    "    def strength2(bl, porof, nlayer=6):\n",
    "        sigma01, sigma02, C1s = 6, 31, 21\n",
    "        sigma_long = sigma01*(K.exp((1.0-porof)**(C1s*nlayer))-porof) + sigma02*(1.0-porof)\n",
    "        n = K.shape(sigma_long)[0]  \n",
    "        sorted_strength, sortedIndices = tf.math.top_k(sigma_long, n, True)\n",
    "        sorted_bl = K.gather(bl, sortedIndices)\n",
    "        n = K.cast(n, tf.float32)\n",
    "        rel = K.relu(sorted_bl[1:]-sorted_bl[0:-1])\n",
    "        num_vio = K.cast(tf.math.count_nonzero(rel), tf.float32)\n",
    "        return num_vio/n\n",
    "\n",
    "\n",
    "    def phy_loss_mean(params):\n",
    "        # useful for cross-checking training\n",
    "        loss1, loss2, loss3, loss4, lam1, lam2, lam3, lam4 = params\n",
    "        def loss(y_true,y_pred):\n",
    "    #         return lam1*K.mean(K.relu(loss1)) + lam2*K.mean(K.relu(loss2)) + lam2*K.mean(K.relu(loss3))\n",
    "            return lam1*K.mean(K.relu(loss1)) + lam2*K.mean(K.relu(loss2)) + lam3*K.mean(K.relu(loss3)) + lam4*loss4\n",
    "        return loss\n",
    "\n",
    "    #function to calculate the combined loss = sum of rmse and phy based loss\n",
    "    def combined_loss(params):\n",
    "        loss1, loss2, loss3, loss4, lam1, lam2, lam3, lam4 = params\n",
    "        def loss(y_true,y_pred):\n",
    "    #         return mean_squared_error(y_true, y_pred) + lam1 * K.mean(K.relu(loss1)) + lam2 * K.mean(K.relu(loss2)) + lam2 * K.mean(K.relu(loss3))\n",
    "            return mean_squared_error(y_true, y_pred) + lam1 * K.mean(K.relu(loss1)) + lam2 * K.mean(K.relu(loss2)) + lam3*K.mean(K.relu(loss3)) + lam4 * loss4\n",
    "        return loss\n",
    "\n",
    "    def PGNN_train_test(optimizer_name, optimizer_val, use_YPhy, pre_train, tr_size, lamda, iteration, n_nodes, n_layers, drop_frac, reg, samp):\n",
    "\n",
    "    #     fix_seeds(ss)\n",
    "\n",
    "        # Hyper-parameters of the training process\n",
    "    #     batch_size = tr_size\n",
    "        batch_size = 1\n",
    "        num_epochs = 50\n",
    "        val_frac = 0.2\n",
    "        patience_val = 50\n",
    "\n",
    "        # Initializing results filename\n",
    "        exp_name = \"DNN_pre_loss_\" + pre_train + optimizer_name + '_trsize' + str(tr_size) + '_lamda' + str(lamda) + '_iter' + str(iteration)\n",
    "        exp_name = exp_name.replace('.','pt')\n",
    "        results_dir = '../results/'\n",
    "        model_name = results_dir + exp_name + '_model.h5' # storing the trained model\n",
    "\n",
    "        if reg==True and samp==25:\n",
    "            results_name = results_dir + exp_name + '_results_25_regularizer.dat' # storing the results of the model\n",
    "        elif reg==False and samp==25:\n",
    "            results_name = results_dir + exp_name + '_results_25.dat' # storing the results of the model\n",
    "        elif reg==True and samp==1519:\n",
    "            results_name = results_dir + exp_name + '_results_1519_regularizer.dat' # storing the results of the model\n",
    "        elif reg==False and samp==1519:\n",
    "            results_name = results_dir + exp_name + '_results_1519.dat' # storing the results of the model\n",
    "\n",
    "        # Load labeled data\n",
    "        data = np.loadtxt('../data/labeled_data.dat')\n",
    "        x_labeled = data[:, :2] # -2 because we do not need porosity predictions\n",
    "        y_labeled = data[:, -3:-1] # dimensionless bond length and porosity measurements\n",
    "        if samp==25:\n",
    "            data = np.loadtxt('../data/unlabeled_data_BK_constw_v2_25.dat')\n",
    "            x_unlabeled = data[:, :]\n",
    "        elif samp==1519:\n",
    "            data = np.loadtxt('../data/unlabeled_data_BK_constw_v2_1525.dat')\n",
    "            x_unlabeled = data[:, :]\n",
    "\n",
    "        x_unlabeled1 = x_unlabeled[:1303, :]\n",
    "        x_unlabeled2 = x_unlabeled[-6:, :]\n",
    "        x_unlabeled = np.vstack((x_unlabeled1,x_unlabeled2))\n",
    "\n",
    "        # initial porosity\n",
    "        init_poro = x_unlabeled[:, -1]\n",
    "        x_unlabeled = x_unlabeled[:, :2]\n",
    "\n",
    "        # normalize dataset with MinMaxScaler\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "        x_labeled = scaler.fit_transform(x_labeled)\n",
    "        x_unlabeled = scaler.fit_transform(x_unlabeled)\n",
    "#         y_labeled = scaler.fit_transform(y_labeled)\n",
    "\n",
    "    #     # initial porosity & physics outputs are removed\n",
    "    #     x_unlabeled = x_unlabeled[:, :-3]\n",
    "\n",
    "        # train and test data\n",
    "        trainX, trainY = x_labeled[:tr_size,:], y_labeled[:tr_size]\n",
    "    #     testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]   \n",
    "        testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]\n",
    "\n",
    "        if use_YPhy == 0:\n",
    "            # Removing the last column from x_unlabeled (corresponding to Y_PHY)\n",
    "            x_unlabeled = x_unlabeled[:,:-1]\n",
    "\n",
    "        dependencies = {\n",
    "         'root_mean_squared_error': root_mean_squared_error\n",
    "            }\n",
    "\n",
    "        # load the pre-trained model using non-calibrated physics-based model predictions (./data/unlabeled.dat)\n",
    "        loaded_model = load_model(results_dir + pre_train, custom_objects=dependencies)\n",
    "\n",
    "        # Creating the model\n",
    "        model = Sequential()\n",
    "        for layer in np.arange(n_layers):\n",
    "            if layer == 0:\n",
    "                model.add(Dense(n_nodes, activation='relu', input_shape=(np.shape(trainX)[1],)))\n",
    "            else:\n",
    "                if reg:\n",
    "                    model.add(Dense(n_nodes, activation='relu', kernel_regularizer=l1_l2(l1=.001, l2=.001)))\n",
    "                else:\n",
    "                    model.add(Dense(n_nodes, activation='relu'))\n",
    "            model.add(Dropout(rate=drop_frac))\n",
    "#             model.add(MCDropout(rate=drop_frac))\n",
    "        model.add(Dense(2, activation='linear'))\n",
    "\n",
    "        # pass the weights to all layers but 1st input layer, whose dimensions are updated\n",
    "        for new_layer, layer in zip(model.layers[1:], loaded_model.layers[1:]):\n",
    "            new_layer.set_weights(layer.get_weights())\n",
    "\n",
    "        # physics-based regularization\n",
    "        uinp_sc = K.constant(value=x_unlabeled) # unlabeled input data\n",
    "        lam1 = K.constant(value=lamda[0]) # regularization hyper-parameter\n",
    "        lam2 = K.constant(value=lamda[1]) # regularization hyper-parameter\n",
    "        lam3 = K.constant(value=lamda[2]) # regularization hyper-parameter\n",
    "        lam4 = K.constant(value=lamda[3]) # regularization hyper-parameter\n",
    "        predictions = model(uinp_sc) # model output at depth i\n",
    "    #     porosity = K.relu(predictions[:,1])\n",
    "        phyloss1 = bond(predictions[:,0]) # physics loss 1\n",
    "    #     uinp = K.constant(value=x_unlabeled_non) # unlabeled input data\n",
    "        phyloss2 = poros(init_poro, predictions[:,1]) # physics loss 1\n",
    "        phyloss3 = strength1(predictions[:,0], predictions[:,1])\n",
    "        phyloss4 = strength2(predictions[:,0], predictions[:,1])\n",
    "        totloss = combined_loss([phyloss1, phyloss2, phyloss3, phyloss4, lam1, lam2, lam3, lam4])\n",
    "        phyloss = phy_loss_mean([phyloss1, phyloss2, phyloss3, phyloss4, lam1, lam2, lam3, lam4])\n",
    "\n",
    "\n",
    "        model.compile(loss=totloss,\n",
    "                      optimizer=optimizer_val,\n",
    "                      metrics=[phyloss, root_mean_squared_error])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience_val, verbose=1)\n",
    "\n",
    "    #     print('Running...' + optimizer_name)\n",
    "        history = model.fit(trainX, trainY,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=num_epochs,\n",
    "                            verbose=0,\n",
    "                            validation_split=val_frac, callbacks=[early_stopping, TerminateOnNaN()])\n",
    "    \n",
    "#     early_stopping = EarlyStopping(monitor='loss', patience=patience_val, verbose=1)\n",
    "#     history = model.fit(trainX, trainY,\n",
    "#                         batch_size=batch_size,\n",
    "#                         epochs=num_epochs,\n",
    "#                         verbose=1,\n",
    "#                         callbacks=[early_stopping, TerminateOnNaN()])\n",
    "\n",
    "#     test_score = model.evaluate(testX, testY, verbose=0)\n",
    "#     predictions = model.predict(x_labeled) # model output at depth i\n",
    "#     print(np.sort(predictions[:,0], axis=0))\n",
    "    \n",
    "#     predictions = model.predict(x_unlabeled) # model output at depth i\n",
    "#     print(np.sort(predictions[:,0], axis=0))\n",
    "    \n",
    "#     print('iter: ' + str(iteration) + ' useYPhy: ' + str(use_YPhy) + \n",
    "#           ' nL: ' + str(n_layers) + ' nN: ' + str(n_nodes) + \n",
    "#           ' lamda1: ' + str(lamda[0]) + ' lamda2: ' + str(lamda[1]) + ' trsize: ' + str(tr_size) + \n",
    "#           ' TestRMSE: ' + str(test_score[2]) + ' PhyLoss: ' + str(test_score[1]), ' TestLoss: ' + str(test_score[0]), \"\\n\")\n",
    "\n",
    "# #     print('iter: ' + str(iteration) + ' TestRMSE: ' + str(test_score[2]) + ' PhyLoss: ' + str(test_score[1]), \"\\n\")\n",
    "\n",
    "    \n",
    "# #     model.save(model_name)\n",
    "    \n",
    "#     # save results\n",
    "#     results = {'train_loss_1':history.history['loss_1'], \n",
    "#                                 'val_loss_1':history.history['val_loss_1'], \n",
    "#                                 'train_rmse':history.history['root_mean_squared_error'], \n",
    "#                                 'val_rmse':history.history['val_root_mean_squared_error'],\n",
    "#                                 'test_rmse':test_score[2],\n",
    "#                                 'PhyLoss':test_score[1]}\n",
    "\n",
    "#     results = {'train_loss_1':history.history['loss_1'], \n",
    "#                                 'train_rmse':history.history['root_mean_squared_error'], \n",
    "#                                 'test_rmse':test_score[2],\n",
    "#                                 'PhyLoss':test_score[1]}\n",
    "\n",
    "    \n",
    "\n",
    "#     save_obj(results, results_name)\n",
    "\n",
    "#     predictions = model.predict(testX)\n",
    "#     return results, results_name, predictions, testY, test_score[2], trainY\n",
    "    \n",
    "        test_score = model.evaluate(testX, testY, verbose=1)\n",
    "        print(test_score)\n",
    "\n",
    "        Xx = np.random.uniform(0,1,(1000,2))\n",
    "        \n",
    "        samples = []\n",
    "        for i in range(int(nsim)):\n",
    "            print(\"simulation num:\",i)\n",
    "            predictions = model.predict(Xx)\n",
    "            predictions = predictions[:,1]\n",
    "            samples.append(predictions)\n",
    "        return np.array(samples)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Main Function\n",
    "    if __name__ == '__main__':\n",
    "\n",
    "        fix_seeds(1)\n",
    "\n",
    "        # List of optimizers to choose from    \n",
    "        optimizer_names = ['Adagrad', 'Adadelta', 'Adam', 'Nadam', 'RMSprop', 'SGD', 'NSGD']\n",
    "        optimizer_vals = [Adagrad(clipnorm=1), Adadelta(clipnorm=1), Adam(clipnorm=1), Nadam(clipnorm=1), RMSprop(clipnorm=1), SGD(clipnorm=1.), SGD(clipnorm=1, nesterov=True)]\n",
    "\n",
    "        # selecting the optimizer\n",
    "        optimizer_num = 1\n",
    "        optimizer_name = optimizer_names[optimizer_num]\n",
    "        optimizer_val = optimizer_vals[optimizer_num]\n",
    "\n",
    "        # Selecting Other Hyper-parameters\n",
    "        drop_frac = 0.1 # Fraction of nodes to be dropped out\n",
    "        use_YPhy = 1 # Whether YPhy is used as another feature in the NN model or not\n",
    "        n_layers = 2 # Number of hidden layers\n",
    "        n_nodes = 5 # Number of nodes per hidden layer\n",
    "\n",
    "        # pre-trained model\n",
    "        pre_train = 'Pre-trainAdadelta_drop0_nL2_nN5_trsize1308_iter0.h5'\n",
    "\n",
    "        #set lamda\n",
    "        lamda = [0.3, 0.15, 0.008, 0] # Physics-based regularization constant  \n",
    "\n",
    "#         # Iterating over different training fractions and splitting indices for train-test splits\n",
    "#         trsize_range = [4,6,8,10,20]\n",
    "\n",
    "#         #default training size = 5000\n",
    "#         tr_size = trsize_range[4]\n",
    "        \n",
    "        tr_size = int(tr_size)\n",
    "\n",
    "        # use regularizer\n",
    "        reg = True\n",
    "\n",
    "        # sample size used\n",
    "        samp = 1519\n",
    "    #     samp = 25\n",
    "\n",
    "        # total number of runs\n",
    "        iter_range = np.arange(1)\n",
    "        testrmse=[]\n",
    "        # iterating through all possible params\n",
    "        for iteration in iter_range:\n",
    "#             results, result_file, pred, obs, rmse, obs_train = PGNN_train_test(optimizer_name, optimizer_val, drop_frac, use_YPhy, \n",
    "#                             iteration, n_layers, n_nodes, tr_size, lamda, reg, samp)\n",
    "#             testrmse.append(rmse)\n",
    "            pred = PGNN_train_test(optimizer_name, optimizer_val, use_YPhy, \n",
    "                                               pre_train, tr_size, lamda, iteration, n_nodes, n_layers, drop_frac, reg, samp)\n",
    "            \n",
    "\n",
    "    return np.squeeze(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr_size: 20\n",
      "19/19 [==============================] - 0s 52us/step\n",
      "[0.0124769676476717, 9.808832692215219e-05, 0.08148614317178726]\n",
      "simulation num: 0\n"
     ]
    }
   ],
   "source": [
    "pred = pass_arg(1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03299434, 0.04710265, 0.0331237 , 0.05003393, 0.05454639,\n",
       "       0.03913652, 0.04294143, 0.03502013, 0.04569039, 0.03877975,\n",
       "       0.04454562, 0.03782   , 0.04002428, 0.05246618, 0.04100242,\n",
       "       0.04795265, 0.0510598 , 0.02809509, 0.05074146, 0.0316502 ,\n",
       "       0.04341102, 0.04706356, 0.043993  , 0.05186173, 0.03298269,\n",
       "       0.03974508, 0.04972262, 0.03696626, 0.02792871, 0.030736  ,\n",
       "       0.05398536, 0.04781182, 0.05077339, 0.04681418, 0.05312671,\n",
       "       0.03989446, 0.04598115, 0.04638732, 0.03756752, 0.0508569 ,\n",
       "       0.03352835, 0.04880513, 0.04306109, 0.04512783, 0.03478729,\n",
       "       0.04626065, 0.04266175, 0.0345915 , 0.04414864, 0.04602354,\n",
       "       0.012908  , 0.05112004, 0.0521664 , 0.04901184, 0.03383476,\n",
       "       0.0307606 , 0.0321417 , 0.04131206, 0.04244629, 0.05091786,\n",
       "       0.04376851, 0.04733174, 0.04430893, 0.04620473, 0.04286466,\n",
       "       0.04110525, 0.01275717, 0.04524851, 0.0492696 , 0.04686001,\n",
       "       0.04995763, 0.02080137, 0.03248481, 0.03997782, 0.05234771,\n",
       "       0.03225144, 0.02574496, 0.05147076, 0.04117397, 0.02891094,\n",
       "       0.02251783, 0.04221284, 0.04200964, 0.03696314, 0.04568692,\n",
       "       0.03226228, 0.04509373, 0.04021302, 0.04335338, 0.03603929,\n",
       "       0.04353459, 0.05135361, 0.04324245, 0.03978162, 0.03064252,\n",
       "       0.05153639, 0.04324748, 0.03638573, 0.04980853, 0.04802867,\n",
       "       0.03428406, 0.0221239 , 0.04883861, 0.03741314, 0.05177493,\n",
       "       0.0372329 , 0.03688278, 0.03219239, 0.03247299, 0.04088555,\n",
       "       0.02998128, 0.03105153, 0.04307517, 0.03994284, 0.04999177,\n",
       "       0.04884029, 0.0497985 , 0.04560688, 0.04676912, 0.03129681,\n",
       "       0.02990726, 0.03328887, 0.03735577, 0.0444733 , 0.03709249,\n",
       "       0.03895395, 0.03654128, 0.04613879, 0.03916651, 0.04954965,\n",
       "       0.04511212, 0.04370446, 0.02531937, 0.04397496, 0.03334043,\n",
       "       0.04570731, 0.04145922, 0.05153642, 0.04402949, 0.04498769,\n",
       "       0.03256519, 0.05438289, 0.03924093, 0.04530711, 0.00672164,\n",
       "       0.03248782, 0.04727995, 0.0385977 , 0.04688928, 0.03774824,\n",
       "       0.04672416, 0.03120638, 0.04343245, 0.05095238, 0.04839257,\n",
       "       0.02193234, 0.02364518, 0.04702651, 0.03242107, 0.03533972,\n",
       "       0.03487159, 0.05367102, 0.04482052, 0.04718135, 0.05026639,\n",
       "       0.04649381, 0.05073478, 0.03643426, 0.04105479, 0.04533163,\n",
       "       0.03614776, 0.04263452, 0.04524981, 0.03894005, 0.03629388,\n",
       "       0.04377145, 0.04091764, 0.04464259, 0.03238301, 0.05150387,\n",
       "       0.04289506, 0.03898405, 0.04035551, 0.01755303, 0.04663127,\n",
       "       0.03353167, 0.04192203, 0.00868595, 0.04522818, 0.0428794 ,\n",
       "       0.04464477, 0.04399227, 0.04697026, 0.04814731, 0.04281173,\n",
       "       0.02464289, 0.04167903, 0.04018488, 0.04412104, 0.05322415,\n",
       "       0.02199948, 0.03103772, 0.03577314, 0.04623646, 0.01002286,\n",
       "       0.00867068, 0.04994495, 0.04853108, 0.04366842, 0.0434327 ,\n",
       "       0.04386981, 0.03075946, 0.01457636, 0.03547319, 0.04342141,\n",
       "       0.02306159, 0.04793684, 0.03871868, 0.05069762, 0.04294097,\n",
       "       0.04575316, 0.04786782, 0.046243  , 0.04035526, 0.02469229,\n",
       "       0.0363434 , 0.03786615, 0.04667587, 0.05039604, 0.00955747,\n",
       "       0.03229296, 0.05294584, 0.05221754, 0.03961065, 0.04942343,\n",
       "       0.03156928, 0.04024251, 0.04955073, 0.03589006, 0.04700203,\n",
       "       0.04683785, 0.03575779, 0.03716473, 0.05169538, 0.05112997,\n",
       "       0.04774908, 0.02682083, 0.04802889, 0.03640674, 0.04251154,\n",
       "       0.03376163, 0.02194273, 0.04617544, 0.03535044, 0.02370296,\n",
       "       0.05046289, 0.03749719, 0.05219242, 0.04871798, 0.03650518,\n",
       "       0.04271956, 0.0441401 , 0.0409051 , 0.03130646, 0.04239703,\n",
       "       0.0533607 , 0.04593645, 0.03529426, 0.03895039, 0.03904028,\n",
       "       0.03697994, 0.03944321, 0.03829177, 0.05121159, 0.05322243,\n",
       "       0.04363482, 0.02701179, 0.05093538, 0.04834735, 0.04710785,\n",
       "       0.04556523, 0.04719509, 0.0363395 , 0.04275845, 0.04442247,\n",
       "       0.03982037, 0.05238225, 0.03907983, 0.0465406 , 0.04289263,\n",
       "       0.03765623, 0.03536366, 0.03972089, 0.0465821 , 0.05217347,\n",
       "       0.04653466, 0.04709557, 0.04296564, 0.04474419, 0.04193599,\n",
       "       0.03637607, 0.05014781, 0.04304609, 0.03487416, 0.04488733,\n",
       "       0.03863084, 0.03929178, 0.04059102, 0.05106544, 0.0467739 ,\n",
       "       0.0388439 , 0.04821196, 0.04393444, 0.03249633, 0.03651103,\n",
       "       0.03250781, 0.01665075, 0.03153609, 0.05410584, 0.0435908 ,\n",
       "       0.03478903, 0.04910306, 0.04105791, 0.03373037, 0.03332054,\n",
       "       0.03194307, 0.03870537, 0.04743413, 0.03356973, 0.0362077 ,\n",
       "       0.04333726, 0.0434415 , 0.04812643, 0.04191576, 0.05174625,\n",
       "       0.04928315, 0.04334104, 0.04443359, 0.04601749, 0.05083238,\n",
       "       0.02005265, 0.03124394, 0.03586445, 0.0380458 , 0.05156414,\n",
       "       0.0472784 , 0.05001968, 0.03323828, 0.04604235, 0.03742632,\n",
       "       0.04851714, 0.04984039, 0.0332212 , 0.03311419, 0.04027496,\n",
       "       0.03431971, 0.03371986, 0.04870063, 0.04288912, 0.03278421,\n",
       "       0.03395918, 0.0321807 , 0.0374298 , 0.03355634, 0.04448672,\n",
       "       0.03991915, 0.04442703, 0.03582836, 0.05151152, 0.03518268,\n",
       "       0.02705598, 0.04378541, 0.04052438, 0.02958237, 0.04199262,\n",
       "       0.04870625, 0.05488165, 0.05141991, 0.05337417, 0.02829141,\n",
       "       0.04624184, 0.03718733, 0.0354351 , 0.031249  , 0.04782195,\n",
       "       0.04929574, 0.04107222, 0.00616495, 0.04855707, 0.04354785,\n",
       "       0.03682072, 0.04216045, 0.04721713, 0.03986974, 0.05124465,\n",
       "       0.04964308, 0.03546678, 0.05494804, 0.04257351, 0.01764667,\n",
       "       0.03924628, 0.03069353, 0.03647352, 0.03978407, 0.04576074,\n",
       "       0.03318295, 0.04869733, 0.03954138, 0.04990158, 0.05309962,\n",
       "       0.02916692, 0.0405879 , 0.04092181, 0.0368961 , 0.03909505,\n",
       "       0.02380249, 0.03845856, 0.0406004 , 0.05128248, 0.0444302 ,\n",
       "       0.04874257, 0.03549714, 0.03221737, 0.04549666, 0.04775137,\n",
       "       0.03253204, 0.03249491, 0.04665089, 0.04627844, 0.04207371,\n",
       "       0.04740204, 0.0170614 , 0.03964996, 0.04235601, 0.04279748,\n",
       "       0.03567298, 0.03375344, 0.05338553, 0.03711595, 0.0353183 ,\n",
       "       0.03559737, 0.03592402, 0.05053498, 0.04935019, 0.0439511 ,\n",
       "       0.03819497, 0.03085721, 0.03378192, 0.04560797, 0.03192934,\n",
       "       0.05043211, 0.02692247, 0.03616715, 0.00996137, 0.0420138 ,\n",
       "       0.04925466, 0.0340824 , 0.02113713, 0.04250082, 0.04852255,\n",
       "       0.04872505, 0.04038083, 0.03652065, 0.03769197, 0.03004765,\n",
       "       0.03565896, 0.05431175, 0.04972201, 0.04016391, 0.04763605,\n",
       "       0.03252604, 0.04725952, 0.03289698, 0.04935201, 0.04751746,\n",
       "       0.04951333, 0.0348972 , 0.0345193 , 0.05008413, 0.04000839,\n",
       "       0.04744118, 0.04543167, 0.03946212, 0.0489771 , 0.03051103,\n",
       "       0.03723463, 0.05519106, 0.03718501, 0.03354939, 0.046479  ,\n",
       "       0.03885416, 0.03533909, 0.05094435, 0.03276273, 0.03963932,\n",
       "       0.04675042, 0.05188393, 0.04293824, 0.04262357, 0.01799116,\n",
       "       0.05052885, 0.04192877, 0.03496519, 0.05070414, 0.04840589,\n",
       "       0.03480025, 0.04416683, 0.05118644, 0.03839646, 0.04323501,\n",
       "       0.04052015, 0.03202805, 0.0374213 , 0.02485781, 0.03152523,\n",
       "       0.03633643, 0.03284629, 0.03624669, 0.0310706 , 0.04664342,\n",
       "       0.0427578 , 0.04901253, 0.03935523, 0.03380675, 0.03636151,\n",
       "       0.04470861, 0.047397  , 0.04580253, 0.02640517, 0.05063818,\n",
       "       0.03879693, 0.04689886, 0.04206844, 0.03181511, 0.05037298,\n",
       "       0.04433665, 0.0537288 , 0.05047091, 0.03810547, 0.04285683,\n",
       "       0.01895108, 0.04398444, 0.0452605 , 0.04991484, 0.03429192,\n",
       "       0.04676002, 0.04263481, 0.03781963, 0.0429918 , 0.04004601,\n",
       "       0.04638349, 0.04733987, 0.04534099, 0.0337341 , 0.03135873,\n",
       "       0.03798328, 0.03911362, 0.01186942, 0.03434085, 0.04544516,\n",
       "       0.03498185, 0.04809594, 0.04918355, 0.0360983 , 0.0348673 ,\n",
       "       0.03137695, 0.04099965, 0.04565854, 0.04720132, 0.03932066,\n",
       "       0.02984522, 0.04327354, 0.04961792, 0.04946835, 0.05080656,\n",
       "       0.04301266, 0.03582078, 0.05190288, 0.02664055, 0.05161951,\n",
       "       0.0385197 , 0.04364032, 0.05226789, 0.04124469, 0.04400701,\n",
       "       0.046578  , 0.03198341, 0.03753123, 0.04896496, 0.04449866,\n",
       "       0.04319708, 0.04451462, 0.04561785, 0.05178696, 0.01789449,\n",
       "       0.04416592, 0.03832287, 0.051101  , 0.03092058, 0.05026799,\n",
       "       0.03907142, 0.04532149, 0.03626183, 0.03932244, 0.05001824,\n",
       "       0.05005006, 0.05434425, 0.03571072, 0.04277166, 0.00822078,\n",
       "       0.04899078, 0.04684323, 0.04571815, 0.04207862, 0.03824991,\n",
       "       0.0452294 , 0.04002073, 0.0337643 , 0.04669461, 0.04715858,\n",
       "       0.04534893, 0.04269622, 0.05240736, 0.03102088, 0.03117844,\n",
       "       0.0443532 , 0.03122347, 0.04506277, 0.03941069, 0.04812451,\n",
       "       0.0369192 , 0.04382098, 0.05194378, 0.03100652, 0.04810865,\n",
       "       0.03366967, 0.04245128, 0.03683042, 0.04762559, 0.04210772,\n",
       "       0.04572631, 0.05111957, 0.03423174, 0.04857095, 0.04662063,\n",
       "       0.01138047, 0.03839172, 0.03792309, 0.0322413 , 0.0341185 ,\n",
       "       0.04432221, 0.04491529, 0.0483126 , 0.03941926, 0.02729898,\n",
       "       0.01572599, 0.03433125, 0.04040749, 0.04068429, 0.03086215,\n",
       "       0.04319637, 0.03139341, 0.03908454, 0.05063442, 0.0324513 ,\n",
       "       0.03506815, 0.04392542, 0.03318776, 0.04250106, 0.03913295,\n",
       "       0.04189784, 0.04809117, 0.03781539, 0.04373164, 0.05180834,\n",
       "       0.04175749, 0.04273063, 0.03609353, 0.05392691, 0.0507056 ,\n",
       "       0.04972993, 0.03408724, 0.02396522, 0.04072345, 0.01231508,\n",
       "       0.04910894, 0.04605312, 0.04279424, 0.03339083, 0.04555329,\n",
       "       0.03208109, 0.04976133, 0.04452838, 0.03420386, 0.02075462,\n",
       "       0.03407854, 0.03357604, 0.04196205, 0.05106336, 0.05056977,\n",
       "       0.04685301, 0.0405234 , 0.04032695, 0.04563346, 0.04272071,\n",
       "       0.05036315, 0.0225434 , 0.04546552, 0.04438019, 0.04195736,\n",
       "       0.04666872, 0.03749938, 0.03762307, 0.04108175, 0.04561334,\n",
       "       0.03458604, 0.04563275, 0.03599554, 0.04716806, 0.03908096,\n",
       "       0.03006865, 0.04518024, 0.04574446, 0.03316454, 0.05037217,\n",
       "       0.04183894, 0.03346923, 0.0501629 , 0.04603865, 0.04091159,\n",
       "       0.03506967, 0.04120355, 0.05129503, 0.0412425 , 0.05276811,\n",
       "       0.05169585, 0.03868633, 0.03441949, 0.01713464, 0.04146176,\n",
       "       0.01991349, 0.04800813, 0.03745267, 0.04966094, 0.03586599,\n",
       "       0.04403618, 0.03998339, 0.02736236, 0.03582531, 0.04823739,\n",
       "       0.04424714, 0.03886996, 0.05058971, 0.04171371, 0.0244809 ,\n",
       "       0.02486665, 0.04676006, 0.04510351, 0.05262162, 0.03629847,\n",
       "       0.03831575, 0.05141129, 0.04589698, 0.03281645, 0.03960923,\n",
       "       0.03050802, 0.04532924, 0.03165092, 0.05379757, 0.04570857,\n",
       "       0.03244983, 0.03082392, 0.05210605, 0.0064786 , 0.05001841,\n",
       "       0.05071735, 0.03645321, 0.03264091, 0.03613161, 0.01878535,\n",
       "       0.04365652, 0.03519788, 0.04424712, 0.03787518, 0.04440429,\n",
       "       0.05168877, 0.03299538, 0.04536424, 0.04364924, 0.05366551,\n",
       "       0.04588377, 0.03931575, 0.03296889, 0.03739214, 0.03789039,\n",
       "       0.05151302, 0.05136589, 0.04354866, 0.01680079, 0.0448974 ,\n",
       "       0.04634649, 0.02560217, 0.03048924, 0.04750609, 0.03062231,\n",
       "       0.04366734, 0.03909922, 0.05245151, 0.05006945, 0.05302816,\n",
       "       0.0455305 , 0.02996808, 0.03870551, 0.01417642, 0.0350574 ,\n",
       "       0.03951631, 0.0314115 , 0.0540156 , 0.0354671 , 0.04167821,\n",
       "       0.04036812, 0.0477071 , 0.03198273, 0.04913462, 0.05107099,\n",
       "       0.05384906, 0.03112005, 0.04840344, 0.0497667 , 0.04461193,\n",
       "       0.03812397, 0.04137324, 0.03113069, 0.04738096, 0.03913746,\n",
       "       0.01255825, 0.02962974, 0.04547844, 0.03178491, 0.02879351,\n",
       "       0.03067966, 0.04731414, 0.03803999, 0.05095322, 0.05276519,\n",
       "       0.04452238, 0.03385694, 0.04111442, 0.05525604, 0.04167234,\n",
       "       0.03989732, 0.04752603, 0.03698602, 0.03753649, 0.04952672,\n",
       "       0.03093603, 0.05015587, 0.03573947, 0.03910712, 0.05257281,\n",
       "       0.03372721, 0.03175761, 0.03384809, 0.04823355, 0.05180612,\n",
       "       0.04444761, 0.05255659, 0.03089155, 0.03625377, 0.02401007,\n",
       "       0.04022915, 0.03123476, 0.0491704 , 0.03247262, 0.04595913,\n",
       "       0.03805742, 0.04964258, 0.04405659, 0.03193858, 0.0515786 ,\n",
       "       0.05110637, 0.03802758, 0.02125117, 0.05113125, 0.02239451,\n",
       "       0.03637765, 0.02663077, 0.0371962 , 0.03160959, 0.0374501 ,\n",
       "       0.04996753, 0.0399159 , 0.03802617, 0.03945987, 0.04571723,\n",
       "       0.0484317 , 0.04720853, 0.03920791, 0.04773912, 0.00768611,\n",
       "       0.03530354, 0.03140254, 0.03857163, 0.04446604, 0.05303227,\n",
       "       0.04362871, 0.03561512, 0.04035731, 0.04329582, 0.03010534,\n",
       "       0.05063801, 0.04608255, 0.04141917, 0.044549  , 0.04032689,\n",
       "       0.0308628 , 0.03479362, 0.04755333, 0.05368377, 0.04827614,\n",
       "       0.04406097, 0.04963261, 0.03407323, 0.04624289, 0.03724434,\n",
       "       0.03802695, 0.03075694, 0.04335567, 0.02999009, 0.01965695,\n",
       "       0.03110261, 0.05331998, 0.0285355 , 0.05192382, 0.03697184,\n",
       "       0.03380363, 0.03425945, 0.03963111, 0.05382601, 0.0421276 ,\n",
       "       0.03221276, 0.05111413, 0.04163611, 0.03766303, 0.04133622,\n",
       "       0.04732173, 0.03857771, 0.04161032, 0.04525815, 0.03540369,\n",
       "       0.04052269, 0.04004814, 0.02110559, 0.03931522, 0.04435858,\n",
       "       0.05158396, 0.04483398, 0.04243543, 0.03413807, 0.04513723,\n",
       "       0.05298013, 0.04961247, 0.02139605, 0.04675102, 0.04363849,\n",
       "       0.03084843, 0.04437064, 0.05180509, 0.03155695, 0.03233844,\n",
       "       0.04004241, 0.04906198, 0.03651524, 0.03664903, 0.03303035,\n",
       "       0.0291419 , 0.03934378, 0.04700744, 0.03338809, 0.03122397,\n",
       "       0.02842632, 0.01477453, 0.04323759, 0.01201461, 0.03242746,\n",
       "       0.03261645, 0.0489556 , 0.04983853, 0.02354763, 0.03077593,\n",
       "       0.01300522, 0.05235334, 0.05302031, 0.04640544, 0.03700077,\n",
       "       0.04168575, 0.03511391, 0.01530456, 0.0416575 , 0.03005965],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "save_obj(pred, \"../pred_upd_loss_Xx.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
