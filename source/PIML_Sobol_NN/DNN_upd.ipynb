{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, Adadelta, Adagrad, Adam, Nadam, SGD\n",
    "from keras.callbacks import EarlyStopping, TerminateOnNaN\n",
    "from keras import backend as K\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.models import load_model, Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Normalize the data.\n",
    "from sklearn import preprocessing\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "import random\n",
    "\n",
    "def pass_arg(nsim, tr_size):\n",
    "    print(\"Tr_size:\", tr_size)\n",
    "    def fix_seeds(seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "        sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    #     K.set_session(sess)\n",
    "        tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "    ss = 1\n",
    "    fix_seeds(ss)\n",
    "\n",
    "\n",
    "    # import pickle\n",
    "\n",
    "    # def save_obj(obj, name):\n",
    "    #     with open(name, 'wb') as f:\n",
    "    #         pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Compute the RMSE given the ground truth (y_true) and the predictions(y_pred)\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "    # # Making sure dimensionless bond length is less than 1\n",
    "    # def bond(bl):\n",
    "    #     return bl-1.0\n",
    "\n",
    "    # Making sure dimensionless bond length is less than 1\n",
    "    def bond(bl):\n",
    "        bln = -bl*(bl<0)\n",
    "        blp = bl*(bl>=1.0) - 1*(bl>=1.0)\n",
    "        return bln+blp\n",
    "\n",
    "    # # Making sure final porosity is less than initial\n",
    "    # def poros(poroi, porof):\n",
    "    # #     porof[porof < 0] = 1-porof[porof < 0]\n",
    "    #     porof[porof < 0] = poroi[0]-porof[porof < 0]\n",
    "    #     print(porof)\n",
    "    #     return porof-poroi\n",
    "\n",
    "    # Making sure final porosity is less than initial\n",
    "    def poros(poroi, porof):\n",
    "        porofn = -porof*(porof<0)\n",
    "        porofp = porof*(porof>=poroi) - poroi*(porof>=poroi)\n",
    "        return porofp+porofn\n",
    "\n",
    "    # def strength(bl, porof, nlayer=6):\n",
    "    #     discp = []\n",
    "    #     sigma01, sigma02 = 6, 31\n",
    "    #     C1s = 21\n",
    "    #     sigma_long = sigma01*(np.exp((1.0-porof)**(C1s*nlayer))-porof) + sigma02*(1.0-porof)\n",
    "    # #     print(\"sigma_long:\",sigma_long)\n",
    "    #     for i in range(len(sigma_long)):\n",
    "    #         for j in range(i + 1, len(sigma_long)):\n",
    "    #             if (sigma_long[j] > sigma_long[i]):\n",
    "    #                 discp.append(bl[i] - bl[j])\n",
    "    #     discp = np.array(discp)\n",
    "    #     print(discp)\n",
    "    #     return discp\n",
    "\n",
    "    def strength1(bl, porof, nlayer=6):\n",
    "        sigma01, sigma02 = 6, 31\n",
    "        C1s = 21\n",
    "        sigma_long = sigma01*(np.exp((1.0-porof)**(C1s*nlayer))-porof) + sigma02*(1.0-porof)\n",
    "        sigma_long_sorted = np.sort(sigma_long, axis=-1)  # sorts along first axis (down)\n",
    "        ind = np.argsort(sigma_long, axis=-1)  # sorts along first axis (down)\n",
    "        bl_sorted = np.take_along_axis(bl, ind, axis=-1)  # same as np.sort(x, axis=0)\n",
    "        corr_bl_sorted = np.sort(bl, axis=-1)  # sorts along first axis (down)\n",
    "        return corr_bl_sorted-bl_sorted\n",
    "\n",
    "    def strength2(bl, porof, nlayer=6):\n",
    "        sigma01, sigma02 = 6, 31\n",
    "        C1s = 21\n",
    "        sigma_long = sigma01*(np.exp((1.0-porof)**(C1s*nlayer))-porof) + sigma02*(1.0-porof)\n",
    "        sigma_long_sorted = np.sort(sigma_long, axis=-1)  # sorts along first axis (down)\n",
    "        ind = np.argsort(sigma_long, axis=-1)  # sorts along first axis (down)\n",
    "        bl_sorted = np.take_along_axis(bl, ind, axis=-1)  # same as np.sort(x, axis=0)\n",
    "        return sum(bl_sorted[1:]-bl_sorted[:-1]<0)/14\n",
    "\n",
    "    def phy_loss_mean(params):\n",
    "        # useful for cross-checking training\n",
    "        loss1, loss2, loss3, loss4, lam1, lam2 = params\n",
    "        x1, x2, x3 = loss1*(loss1>0), loss2*(loss2>0), loss3*(loss3>0)\n",
    "    #     print(np.mean(x1), x1.shape[0])\n",
    "    #     print(np.mean(x2), x2.shape[0])\n",
    "    #     print(np.mean(x3), x3.shape[0])\n",
    "\n",
    "        if x1.any() and x1.shape[0]>1:\n",
    "            X_scaled1 = (x1 - np.min(x1)) / (np.max(x1) - np.min(x1))\n",
    "            x1 = X_scaled1\n",
    "        if x2.any() and x2.shape[0]>1:\n",
    "            X_scaled2 = (x2 - np.min(x2)) / (np.max(x2) - np.min(x2))\n",
    "            x2 = X_scaled2\n",
    "        if x3.any() and x3.shape[0]>1:\n",
    "            X_scaled3 = (x3 - np.min(x3)) / (np.max(x3) - np.min(x3))\n",
    "            x3 = X_scaled3\n",
    "        return (lam1*np.mean(x1) + lam2*np.mean(x2) + lam2*np.mean(x3))\n",
    "    #     return (lam1*np.mean(x1) + lam2*np.mean(x2) + lam2*np.mean(x3) + lam2*loss4)\n",
    "\n",
    "    # def phy_loss_mean(params):\n",
    "    #     # useful for cross-checking training\n",
    "    #     diff1, diff2, lam1, lam2 = params\n",
    "    #     x1, x2 = diff1*(diff1>0), diff2*(diff2>0)\n",
    "    #     if np.any(x1):\n",
    "    #         X_scaled1 = (x1 - np.min(x1)) / (np.max(x1) - np.min(x1))\n",
    "    #         x1 = X_scaled1\n",
    "    #     if np.any(x2):\n",
    "    #         X_scaled2 = (x2 - np.min(x2)) / (np.max(x2) - np.min(x2))\n",
    "    #         x2 = X_scaled2\n",
    "    #     return lam1*np.mean(x1) + lam2*np.mean(x2)\n",
    "\n",
    "    def PGNN_train_test(optimizer_name, optimizer_val, drop_rate, iteration, n_layers, n_nodes, tr_size, pre_train):\n",
    "\n",
    "        # Hyper-parameters of the training process\n",
    "    #     batch_size = int(tr_size/2)\n",
    "        batch_size = 1\n",
    "        num_epochs = 50\n",
    "        val_frac = 0.2\n",
    "        patience_val = 50\n",
    "\n",
    "        # Initializing results filename\n",
    "        exp_name = \"Pre-train\" + optimizer_name + '_drop' + str(drop_rate) + '_nL' + str(n_layers) + '_nN' + str(n_nodes) + '_trsize' + str(tr_size) + '_iter' + str(iteration)\n",
    "        exp_name = exp_name.replace('.','pt')\n",
    "        results_dir = '../results/'\n",
    "        model_name = results_dir + exp_name + '.h5' # storing the trained model\n",
    "        results_name = results_dir + exp_name + '_results.dat' # storing the results of the model\n",
    "        \n",
    "        # Load labeled data\n",
    "        # data = np.loadtxt('../data/unlabeled_data_BK_constw_v2_1525.dat')\n",
    "        \n",
    "        # data1 = data[:1303, :]\n",
    "        # data2 = data[-6:, :]\n",
    "        # datah = np.vstack((data1,data2))\n",
    "        # np.random.shuffle(datah)\n",
    "        # x_unlabeled = datah[:, :2] # 1303 last regular sample\n",
    "        # y_unlabeled = datah[:, -3:-1]\n",
    "\n",
    "        # Load labeled data\n",
    "        # Load labeled data\n",
    "        data = np.loadtxt('../data/labeled_data.dat')\n",
    "        x_labeled = data[:, :2] # -2 because we do not need porosity predictions\n",
    "        y_labeled = data[:, -3:-1] # dimensionless bond length and\n",
    "\n",
    "        # normalize dataset with MinMaxScaler\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range=(0, 1.0))\n",
    "    #     scaler = preprocessing.StandardScaler()\n",
    "        x_labeled = scaler.fit_transform(x_labeled)\n",
    "        # x_unlabeled = scaler.fit_transform(x_unlabeled)\n",
    "#         y_labeled = scaler.fit_transform(y_labeled)\n",
    "\n",
    "        # train and test data\n",
    "        trainX, trainY = x_labeled[:tr_size,:], y_labeled[:tr_size]\n",
    "        testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]\n",
    "\n",
    "        dependencies = {'root_mean_squared_error': root_mean_squared_error}\n",
    "\n",
    "        # load the pre-trained model using non-calibrated physics-based model predictions (./data/unlabeled.dat)\n",
    "        loaded_model = load_model(results_dir + pre_train, custom_objects=dependencies)\n",
    "\n",
    "        # Creating the model\n",
    "        model = Sequential()\n",
    "        for layer in np.arange(n_layers):\n",
    "            if layer == 0:\n",
    "                model.add(Dense(n_nodes, activation='relu', input_shape=(np.shape(trainX)[1],)))\n",
    "            else:\n",
    "                model.add(Dense(n_nodes, activation='relu', kernel_regularizer=l1_l2(l1=.001, l2=.001)))\n",
    "            model.add(Dropout(rate=drop_rate))\n",
    "        model.add(Dense(2, activation='linear'))\n",
    "\n",
    "        # pass the weights to all layers but 1st input layer, whose dimensions are updated\n",
    "        for new_layer, layer in zip(model.layers[1:], loaded_model.layers[1:]):\n",
    "            new_layer.set_weights(layer.get_weights())\n",
    "\t\t\t\n",
    "        model.compile(loss='mean_squared_error',\n",
    "                      optimizer=optimizer_val,\n",
    "                      metrics=[root_mean_squared_error])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience_val,verbose=1)\n",
    "\n",
    "        print('Running...' + optimizer_name)\n",
    "        history = model.fit(trainX, trainY,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=num_epochs,\n",
    "                            verbose=0,\n",
    "                            validation_split=val_frac, callbacks=[early_stopping, TerminateOnNaN()])\n",
    "\n",
    "        test_score = model.evaluate(testX, testY, verbose=1)\n",
    "        print(test_score)\n",
    "        # predictions = model.predict(testX)\n",
    "    # #     inv_pred = scaler.inverse_transform(predictions)\n",
    "        # phyloss1 = bond(predictions[:,0]) # physics loss 1\n",
    "\n",
    "    # #     init_poro_ndim = np.ones((init_poro.shape))\n",
    "    # #     diff2 = poros(init_poro_ndim, predictions[:,1]) # physics loss 2\n",
    "\n",
    "        # phyloss2 = poros(init_poro, predictions[:,1]) # physics loss 2\n",
    "        # phyloss3 = strength1(predictions[:,0], predictions[:,1])\n",
    "        # phyloss4 = strength2(predictions[:,0], predictions[:,1])\n",
    "\n",
    "        # lam1, lam2 = lamda[0], lamda[1]    \n",
    "        # phyloss = phy_loss_mean([phyloss1, phyloss2, phyloss3, phyloss4, lam1, lam2])\n",
    "\n",
    "        # print('iter: ' + str(iteration) + \n",
    "              # ' nL: ' + str(n_layers) + ' nN: ' + str(n_nodes) + \n",
    "              # ' trsize: ' + str(tr_size) + \n",
    "              # ' TestRMSE: ' + str(test_score[1]) + ' PhyLoss: ' + str(phyloss), \"\\n\")\n",
    "\n",
    "    # #     model.save(model_name)\n",
    "\n",
    "        # # save results\n",
    "        # results = {'train_rmse':history.history['root_mean_squared_error'], \n",
    "                                    # 'val_rmse':history.history['val_root_mean_squared_error'],\n",
    "                                    # 'test_rmse':test_score[1], 'PhyLoss':phyloss}\n",
    "\n",
    "    #     save_obj(results, results_name)\n",
    "\n",
    "        # return results, results_name, predictions, testY, test_score[1]\n",
    "        # predictions = model.predict(Xx)\n",
    "\n",
    "        Xx = np.random.uniform(0,1,(1000,2))\n",
    "        \n",
    "        samples = []\n",
    "        for i in range(int(nsim)):\n",
    "            print(\"simulation num:\",i)\n",
    "            predictions = model.predict(Xx)\n",
    "            predictions = predictions[:,1]\n",
    "            samples.append(predictions[:,np.newaxis])\n",
    "        return np.array(samples)\n",
    "\n",
    "\n",
    "\n",
    "    # Main Function\n",
    "    if __name__ == '__main__':\n",
    "\n",
    "        fix_seeds(1)\n",
    "\n",
    "        # List of optimizers to choose from    \n",
    "        optimizer_names = ['Adagrad', 'Adadelta', 'Adam', 'Nadam', 'RMSprop', 'SGD', 'NSGD']\n",
    "        optimizer_vals = [Adagrad(clipnorm=1), Adadelta(clipnorm=1), Adam(clipnorm=1), Nadam(clipnorm=1), RMSprop(clipnorm=1), SGD(clipnorm=1.), SGD(clipnorm=1, nesterov=True)]\n",
    "\n",
    "        # selecting the optimizer\n",
    "        optimizer_num = 1\n",
    "        optimizer_name = optimizer_names[optimizer_num]\n",
    "        optimizer_val = optimizer_vals[optimizer_num]\n",
    "\n",
    "        # Selecting Other Hyper-parameters\n",
    "        drop_rate = 0.1 # Fraction of nodes to be dropped out\n",
    "        n_layers = 2 # Number of hidden layers\n",
    "        n_nodes = 5 # Number of nodes per hidden layer\n",
    "\n",
    "        # # Iterating over different training fractions and splitting indices for train-test splits\n",
    "        # trsize_range = [4,6,8,10,20]\n",
    "\n",
    "        # #default training size = 5000\n",
    "        # tr_size = trsize_range[4]\n",
    "        \n",
    "        # pre-trained model\n",
    "        pre_train = 'Pre-trainAdadelta_drop0_nL2_nN5_trsize1308_iter0.h5'\n",
    "        tr_size = int(tr_size)\n",
    "\n",
    "#         # use regularizer\n",
    "#         reg = True\n",
    "\n",
    "#         #set lamda=0 for pgnn0\n",
    "#         lamda = [1, 1] # Physics-based regularization constant\n",
    "\n",
    "        # total number of runs\n",
    "        iter_range = np.arange(1)\n",
    "        testrmse=[]\n",
    "        # iterating through all possible params\n",
    "        for iteration in iter_range:\n",
    "            # results, result_file, pred, obs, rmse = PGNN_train_test(optimizer_name, optimizer_val, drop_rate, \n",
    "                            # iteration, n_layers, n_nodes, tr_size, lamda, reg)\n",
    "            # testrmse.append(rmse)\n",
    "            pred = PGNN_train_test(optimizer_name, optimizer_val, drop_rate, \n",
    "                        iteration, n_layers, n_nodes, tr_size, pre_train)\n",
    "    \n",
    "    return np.squeeze(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr_size: 20\n",
      "Running...Adadelta\n",
      "19/19 [==============================] - 0s 0us/step\n",
      "[0.012904394418001175, 0.08278346806764603]\n",
      "simulation num: 0\n"
     ]
    }
   ],
   "source": [
    "pred = pass_arg(1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.27344877e-02,  1.50126070e-02,  1.30841136e-02,  1.73304752e-02,\n",
       "        1.77808702e-02,  1.22344047e-02,  4.51873243e-03, -5.98543882e-03,\n",
       "        1.44838318e-02,  1.29277259e-02,  1.59599781e-02, -2.02246010e-03,\n",
       "        5.53391874e-04,  1.66163966e-02,  2.05358118e-03,  1.53552294e-02,\n",
       "        1.80249587e-02, -1.32690109e-02,  1.44797489e-02,  1.01118535e-02,\n",
       "        1.59644037e-02,  1.60767734e-02,  1.42303035e-02,  1.59686729e-02,\n",
       "        1.47594362e-02,  1.35805607e-02,  1.57814473e-02, -3.03868204e-03,\n",
       "       -1.49864927e-02,  1.22066811e-02,  1.79559663e-02,  1.62233114e-02,\n",
       "        1.72458962e-02,  1.64307952e-02,  1.74492151e-02,  1.38251558e-02,\n",
       "        1.01050735e-02,  8.78829509e-03,  1.58664808e-02,  1.64705813e-02,\n",
       "        1.20054036e-02,  1.55091211e-02,  1.32201165e-02,  1.46889910e-02,\n",
       "        1.44609138e-02,  1.64107904e-02,  1.46237090e-02,  1.16287172e-02,\n",
       "        1.48445070e-02,  1.48621649e-02, -3.42340693e-02,  1.58374980e-02,\n",
       "        1.77616403e-02,  1.68646276e-02,  1.51614323e-02,  1.11400858e-02,\n",
       "       -8.07090849e-03,  1.50146857e-02,  1.43839642e-02,  1.47332400e-02,\n",
       "        1.36555955e-02,  1.01123378e-02,  1.45943090e-02,  1.54020637e-02,\n",
       "        1.62097141e-02,  1.48070455e-02, -3.43455225e-02,  1.65746659e-02,\n",
       "        1.61827505e-02,  1.47708580e-02,  1.74633935e-02, -2.40612328e-02,\n",
       "        1.14035681e-02,  1.23563781e-02,  1.67506710e-02, -9.56194103e-03,\n",
       "       -1.76663660e-02,  1.65306479e-02,  1.30884871e-02, -1.36242919e-02,\n",
       "       -2.17572078e-02,  1.31603777e-02,  1.52439475e-02,  1.42113417e-02,\n",
       "        1.46051645e-02,  1.00016072e-02,  1.42386034e-02,  1.27497315e-02,\n",
       "        1.33117288e-02, -4.64735925e-03,  1.50267184e-02,  1.81744173e-02,\n",
       "        1.54994279e-02,  1.23939812e-02,  1.27674416e-02,  1.65470317e-02,\n",
       "        1.41371340e-02,  1.40635520e-02,  1.57431960e-02,  1.26387775e-02,\n",
       "        1.35819018e-02, -2.24529169e-02,  1.60586163e-02,  1.44866332e-02,\n",
       "        1.76716670e-02,  1.54433474e-02,  1.28300190e-02, -9.40774754e-03,\n",
       "        1.22048631e-02,  2.04793364e-03, -1.20899267e-02,  9.62331146e-03,\n",
       "        1.64917335e-02,  1.57763958e-02,  1.69995576e-02,  1.79295093e-02,\n",
       "        1.59674957e-02,  1.50167197e-02,  1.62554011e-02,  1.42128021e-02,\n",
       "       -1.21434294e-02,  1.40191913e-02,  1.10415816e-02,  1.62039176e-02,\n",
       "        1.27333105e-02, -8.68320465e-04,  1.47719607e-02,  1.44128278e-02,\n",
       "        1.18458793e-02,  1.29778981e-02,  1.51037127e-02,  1.63932592e-02,\n",
       "       -1.85085088e-02,  1.62398219e-02,  1.03111342e-02,  1.53418854e-02,\n",
       "        1.30530819e-02,  1.68364719e-02,  1.45872235e-02,  1.65353194e-02,\n",
       "        1.06537044e-02,  1.75797120e-02, -2.96667218e-04,  7.47922808e-03,\n",
       "       -4.09743935e-02,  1.34126469e-02,  1.69147700e-02,  1.49346143e-02,\n",
       "        1.61635056e-02,  1.12900510e-02,  1.49905533e-02, -9.71735641e-03,\n",
       "        1.57635808e-02,  1.83515102e-02,  1.68431029e-02, -2.28215493e-02,\n",
       "       -2.05321833e-02,  1.14092603e-02, -9.14779305e-03, -5.51947951e-03,\n",
       "        1.27935186e-02,  1.73319876e-02,  1.51972175e-02,  1.63251609e-02,\n",
       "        1.60460621e-02,  1.07581615e-02,  1.77983791e-02,  1.20353326e-02,\n",
       "        1.26809776e-02,  7.59143382e-03,  1.21372938e-02,  1.64875835e-02,\n",
       "        1.54611245e-02,  1.48302019e-02, -3.93219292e-03,  1.59897059e-02,\n",
       "        1.51012689e-02,  6.56159222e-03,  1.10289082e-02,  1.64894536e-02,\n",
       "        1.40408725e-02,  1.21509582e-02,  1.41866654e-02, -2.83609554e-02,\n",
       "        1.53832287e-02,  1.37979463e-02,  1.32978708e-02, -3.90700772e-02,\n",
       "        1.40087456e-02,  1.67950690e-02,  1.52172670e-02,  1.37351304e-02,\n",
       "        1.68667808e-02,  1.66422650e-02,  1.44274384e-02, -1.77091099e-02,\n",
       "        1.27835125e-02,  1.55414492e-02,  1.38291195e-02,  1.80056989e-02,\n",
       "       -2.17035860e-02,  1.50237158e-02,  1.52025893e-02,  1.57560259e-02,\n",
       "       -3.78801487e-02, -3.92379239e-02,  1.33398250e-02,  1.64480209e-02,\n",
       "        1.41908079e-02,  1.35288239e-02,  1.49759650e-02,  1.06099546e-02,\n",
       "       -3.08257267e-02,  1.12317801e-02,  1.40099376e-02, -2.14092396e-02,\n",
       "        1.76714510e-02,  1.53313577e-02,  1.65541694e-02,  1.49483755e-02,\n",
       "        1.56327039e-02,  1.57628134e-02,  1.66930854e-02,  1.38277411e-02,\n",
       "       -1.93278603e-02,  1.32733658e-02,  1.16386935e-02,  1.65674612e-02,\n",
       "        1.42512098e-02, -3.85182127e-02,  1.10690147e-02,  1.73087716e-02,\n",
       "        1.64008513e-02,  1.33841261e-02,  1.65005848e-02,  1.28348470e-02,\n",
       "        1.37868077e-02,  1.73791870e-02,  1.21744573e-02,  1.50509477e-02,\n",
       "        9.69546288e-03,  9.72565264e-03,  1.26990154e-02,  1.72080994e-02,\n",
       "        1.66116878e-02,  1.55481398e-02, -1.62149966e-02,  1.32062733e-02,\n",
       "        1.24285817e-02,  1.41943693e-02,  1.08040124e-02, -2.12256759e-02,\n",
       "        1.59867853e-02,  1.44812837e-02, -2.03184411e-02,  1.67833418e-02,\n",
       "        1.22557357e-02,  1.79998055e-02,  1.60384774e-02,  1.08665749e-02,\n",
       "        1.65066719e-02,  1.62967667e-02,  1.52284354e-02,  9.45518911e-03,\n",
       "        1.43244117e-02,  1.73973516e-02,  8.32837820e-03,  1.26462877e-02,\n",
       "        1.58602446e-02,  1.25388503e-02,  1.20178610e-02,  1.49186403e-02,\n",
       "        1.17553025e-02,  1.79862976e-02,  1.74294636e-02,  1.61941275e-02,\n",
       "       -1.61119141e-02,  1.77984536e-02,  1.66590214e-02,  1.47710368e-02,\n",
       "        1.57025307e-02,  1.69680044e-02,  1.07500702e-02,  1.32098645e-02,\n",
       "        1.67886391e-02,  1.21417046e-02,  1.77048072e-02,  1.36761889e-02,\n",
       "        1.56699941e-02,  1.67039856e-02,  1.37542412e-02,  1.12115145e-02,\n",
       "        1.39164627e-02,  1.49738640e-02,  1.67127326e-02,  1.52265131e-02,\n",
       "        1.67512968e-02,  1.42949298e-02,  1.64672807e-02,  1.61722451e-02,\n",
       "        1.42699629e-02,  1.76316276e-02,  1.42498091e-02,  1.18950382e-02,\n",
       "        1.63003206e-02,  1.19580552e-02,  1.44140795e-02,  1.37751251e-02,\n",
       "        1.75596476e-02,  1.51833147e-02,  1.39416829e-02,  1.61364675e-02,\n",
       "        5.85461408e-03,  1.02212876e-02,  1.54258534e-02,  1.24664456e-02,\n",
       "       -2.90541612e-02,  1.05943456e-02,  1.73797533e-02,  1.38075128e-02,\n",
       "        1.19495094e-02,  1.56246573e-02,  1.60861164e-02,  1.37284175e-02,\n",
       "        1.15835220e-02,  1.10068470e-02, -1.17326528e-03,  1.00903213e-02,\n",
       "        1.17552131e-02, -4.16572392e-03,  1.47323757e-02,  1.42456815e-02,\n",
       "        1.75562575e-02,  1.55439526e-02,  1.58768669e-02,  1.24710947e-02,\n",
       "        1.41277537e-02,  1.42868012e-02,  1.51087269e-02,  1.71615407e-02,\n",
       "       -2.51079537e-02,  1.14222914e-02,  1.13537237e-02,  1.34255439e-02,\n",
       "        1.56402290e-02,  1.61650553e-02,  1.72018334e-02,  1.12045109e-02,\n",
       "        1.48840100e-02,  1.44492760e-02,  1.58678368e-02,  1.62314847e-02,\n",
       "       -8.30511376e-03,  1.11337602e-02,  1.25544295e-02,  1.11844167e-02,\n",
       "        1.12569183e-02,  1.60381049e-02,  1.57366022e-02,  1.46387294e-02,\n",
       "        1.50357783e-02,  1.20725483e-02,  1.18873045e-02,  1.41461492e-02,\n",
       "        1.43058822e-02,  1.24424845e-02,  1.60707831e-02,  1.10652670e-02,\n",
       "        1.79594755e-02,  1.12295076e-02, -1.58743002e-02,  1.61169991e-02,\n",
       "        1.55185312e-02, -1.26760788e-02,  1.40075982e-02,  1.65441185e-02,\n",
       "        1.76793411e-02,  1.78285539e-02,  1.75490677e-02, -1.30228177e-02,\n",
       "        1.65998712e-02,  1.23260170e-02,  1.46011561e-02, -1.04814246e-02,\n",
       "        1.49905384e-02,  1.82742476e-02,  1.24519467e-02, -4.16862145e-02,\n",
       "        1.67245716e-02,  1.51856542e-02, -2.73077190e-03,  1.38383880e-02,\n",
       "        1.69915035e-02,  1.25409141e-02,  1.76187530e-02,  1.56696141e-02,\n",
       "        1.21855959e-02,  1.76885799e-02,  1.58183426e-02, -2.82769091e-02,\n",
       "       -2.91563570e-04,  1.25305727e-02,  1.11804381e-02,  1.58516914e-02,\n",
       "        1.66271105e-02, -8.23385268e-03,  1.70425251e-02,  1.30090490e-02,\n",
       "        1.68270990e-02,  1.72577798e-02, -1.18611567e-02,  1.40854567e-02,\n",
       "        2.05532461e-03,  1.09882206e-02,  1.53316930e-02, -2.01308839e-02,\n",
       "        1.34426802e-02,  1.57260746e-02,  1.77447051e-02,  1.59573629e-02,\n",
       "        1.66745335e-02,  1.21416524e-02,  1.04206875e-02,  1.71222836e-02,\n",
       "        1.59743279e-02,  1.11018494e-02,  1.23674721e-02,  1.57750994e-02,\n",
       "        1.71075612e-02,  1.40459165e-02,  1.59351155e-02, -2.88009569e-02,\n",
       "        1.40760392e-02,  1.37813836e-02,  1.45924613e-02, -4.72151488e-03,\n",
       "       -5.80292195e-03,  1.70941502e-02,  1.50150880e-02,  1.10934079e-02,\n",
       "        1.21093988e-02,  1.48838609e-02,  1.74934566e-02,  1.57743245e-02,\n",
       "        1.42206475e-02,  1.25187933e-02,  1.26067922e-02, -7.29556382e-03,\n",
       "        8.14224035e-03,  1.18779093e-02,  1.81323886e-02, -1.62149630e-02,\n",
       "        1.24654248e-02, -3.67856622e-02,  1.37405396e-02,  1.59137249e-02,\n",
       "        1.39857754e-02, -2.23026536e-02,  1.33934915e-02,  1.68152153e-02,\n",
       "        1.56215355e-02,  1.35657936e-02,  1.38717070e-02,  1.30394548e-02,\n",
       "        9.71446931e-03,  1.38463005e-02,  1.76711828e-02,  1.69073492e-02,\n",
       "        1.23200715e-02,  1.50645226e-02,  1.27745643e-02,  1.68994814e-02,\n",
       "        1.40991136e-02,  1.27431750e-02,  1.61167234e-02,  1.57593414e-02,\n",
       "        1.18430778e-02,  1.54363662e-02,  1.59439519e-02,  5.03778458e-04,\n",
       "        1.62080824e-02,  1.52605176e-02,  1.31001249e-02,  1.73284411e-02,\n",
       "        1.01476163e-02, -2.84367055e-03,  1.77917406e-02,  1.32083893e-02,\n",
       "        1.18136555e-02,  1.60146803e-02,  1.44887492e-02,  1.46238580e-02,\n",
       "        1.82256028e-02,  1.39032230e-02, -9.14931297e-06,  1.67882368e-02,\n",
       "        1.75685287e-02,  1.47127658e-02,  1.44362524e-02, -2.78192684e-02,\n",
       "        1.80028975e-02,  3.40911001e-03,  1.23047158e-02,  1.65304020e-02,\n",
       "        1.66971982e-02,  1.10348165e-02,  1.43995807e-02,  1.80821493e-02,\n",
       "        1.37391165e-02,  1.32787600e-02,  1.50156021e-03,  1.41417831e-02,\n",
       "        1.12877935e-02, -1.74313486e-02,  1.50136724e-02,  1.02699175e-02,\n",
       "       -8.59311596e-03, -4.03934717e-03,  9.70330089e-03,  1.55478567e-02,\n",
       "        1.46957338e-02,  1.65268406e-02,  1.19875446e-02,  1.08828396e-02,\n",
       "        1.50091499e-02,  1.54239461e-02,  1.67065561e-02,  1.66519210e-02,\n",
       "       -1.71652772e-02,  1.69190243e-02,  1.33525580e-02,  9.44317132e-03,\n",
       "        1.31017640e-02,  1.13813207e-02,  1.64652616e-02,  1.62254721e-02,\n",
       "        1.80669203e-02,  1.70923173e-02,  1.17568597e-02,  1.52529851e-02,\n",
       "       -2.63825022e-02,  5.85105270e-03,  1.57438889e-02,  1.58950239e-02,\n",
       "        1.19145736e-02,  1.67325288e-02,  1.64816678e-02,  1.26141012e-02,\n",
       "        1.53291076e-02,  1.23536959e-02,  1.54134855e-02,  1.48725063e-02,\n",
       "        1.51161104e-02,  1.45057589e-02,  1.28456652e-02,  1.47815496e-02,\n",
       "        1.33474767e-02, -3.55719179e-02,  1.15951821e-02,  1.41244605e-02,\n",
       "        1.21446326e-02,  1.10722780e-02,  1.62472576e-02,  1.21339709e-02,\n",
       "        1.09367520e-02,  1.10846981e-02,  1.34524032e-02,  8.09870660e-03,\n",
       "        1.57356486e-02,  1.21295005e-02, -1.24792084e-02,  1.43129975e-02,\n",
       "        1.64862946e-02,  1.67396963e-02,  1.66129619e-02,  1.54598877e-02,\n",
       "        1.28159747e-02,  1.78017765e-02, -1.68480687e-02,  1.75412521e-02,\n",
       "        1.58358067e-02,  1.38806105e-02,  1.69214085e-02,  1.57753825e-02,\n",
       "        1.40056163e-02,  1.61569938e-02,  1.08807608e-02,  1.10874549e-02,\n",
       "        1.55211464e-02,  1.61338001e-02,  6.20818138e-03,  1.41501501e-02,\n",
       "        1.52043328e-02,  1.77336708e-02, -2.79152729e-02,  1.56106502e-02,\n",
       "       -1.63350999e-03,  1.80842951e-02,  1.29889175e-02,  1.76502615e-02,\n",
       "        1.31010190e-02,  7.58204609e-03,  1.24201998e-02,  1.48689523e-02,\n",
       "        1.34444460e-02,  1.68083757e-02,  1.78263709e-02,  1.14944726e-02,\n",
       "        1.49624348e-02, -3.92076559e-02,  1.57036632e-02,  1.47263259e-02,\n",
       "        1.55387372e-02,  1.44552514e-02,  1.17928386e-02,  1.40698254e-02,\n",
       "        1.28615499e-02,  1.33227929e-02,  1.46747157e-02,  1.49028599e-02,\n",
       "        1.55089721e-02,  1.42588019e-02,  1.70548707e-02,  1.00954622e-02,\n",
       "        1.06484219e-02,  1.53125897e-02,  1.18656605e-02,  1.41976401e-02,\n",
       "        1.30809918e-02,  1.66708678e-02,  1.40124708e-02,  1.46914944e-02,\n",
       "        1.62575170e-02,  1.17173567e-02,  1.66532025e-02,  1.05896965e-02,\n",
       "        1.48730129e-02,  1.33537725e-02,  1.54611692e-02,  1.41318962e-02,\n",
       "        1.65964440e-02,  1.50067806e-02, -6.75980002e-03,  1.32659227e-02,\n",
       "        1.61193237e-02, -3.61678042e-02,  1.32554993e-02,  1.47395805e-02,\n",
       "        1.43272579e-02,  1.15469545e-02,  1.36830285e-02,  7.18568265e-03,\n",
       "        1.51970983e-02,  1.38731897e-02, -1.58612467e-02, -3.05705890e-02,\n",
       "        1.46290660e-02,  1.62863955e-02,  1.23862326e-02,  1.43754929e-02,\n",
       "        1.40922442e-02,  1.22738704e-02,  1.48475319e-02,  1.65065750e-02,\n",
       "        1.20196864e-02,  1.30829439e-02,  1.42275915e-02,  1.29863769e-02,\n",
       "        1.46496892e-02,  2.41264701e-04,  1.42630041e-02,  1.12696961e-02,\n",
       "        1.15710646e-02,  1.39655843e-02,  1.82854831e-02,  1.36592165e-02,\n",
       "        1.61967799e-02,  1.20415092e-02,  1.73792988e-02,  1.61400065e-02,\n",
       "        1.34627968e-02, -6.91197813e-03, -2.00120099e-02,  1.42268762e-02,\n",
       "       -3.49272862e-02,  1.68813020e-02,  1.65563151e-02,  1.52855441e-02,\n",
       "       -7.28832930e-03,  1.59301609e-02, -9.48300958e-03,  1.78019926e-02,\n",
       "        1.70180649e-02,  1.18730888e-02, -2.39860825e-02,  1.37224048e-02,\n",
       "        1.06706098e-02,  3.06490809e-03,  1.81078762e-02,  1.75567195e-02,\n",
       "        1.48953944e-02,  1.22487247e-02,  1.25960633e-02,  1.43846571e-02,\n",
       "        1.39828026e-02,  1.61295906e-02, -2.17205696e-02,  1.48065686e-02,\n",
       "        1.37462392e-02,  1.51505321e-02,  1.56767294e-02,  1.10597461e-02,\n",
       "        1.25866458e-02,  1.42123923e-02,  1.52367651e-02,  1.09556764e-02,\n",
       "        1.43919960e-02,  1.16283223e-02,  1.66811645e-02,  1.35004967e-02,\n",
       "        1.47756338e-02,  7.40318745e-03,  9.25961882e-03,  1.40343979e-02,\n",
       "        1.73934996e-02,  1.45102218e-02,  1.10108629e-02,  1.65338293e-02,\n",
       "        1.67507231e-02,  3.27777863e-03,  1.12355202e-02,  1.49779245e-02,\n",
       "        1.68632492e-02,  1.50550082e-02,  1.76685825e-02,  1.73043534e-02,\n",
       "        1.34963766e-02,  1.19769871e-02, -2.88098007e-02,  1.42535344e-02,\n",
       "       -2.53855810e-02,  1.62539557e-02,  1.56253353e-02,  1.71006545e-02,\n",
       "        1.26736462e-02,  1.38325170e-02,  1.21788681e-02, -1.54559799e-02,\n",
       "        1.14420503e-02,  1.12406388e-02,  1.40847564e-02,  1.20265484e-02,\n",
       "        1.41810775e-02,  1.62321329e-02, -1.94965526e-02, -1.88341402e-02,\n",
       "        1.50677264e-02,  7.08941370e-03,  1.77412778e-02,  1.19387135e-02,\n",
       "        1.28486976e-02,  1.75539032e-02,  1.54138058e-02,  1.46944225e-02,\n",
       "        1.22831315e-02,  1.36401877e-02,  1.49440989e-02,  1.35852918e-02,\n",
       "        1.80419013e-02,  1.62551105e-02,  1.29499212e-02,  1.07329935e-02,\n",
       "        1.65943876e-02, -4.12392095e-02,  1.76751167e-02,  1.46937892e-02,\n",
       "        1.18535459e-02,  1.21911764e-02, -4.34628874e-03, -2.65756324e-02,\n",
       "        1.53436214e-02,  1.41384676e-02,  1.59105808e-02,  1.28868893e-02,\n",
       "        1.38883293e-02,  1.70396864e-02,  1.45132095e-02,  1.42922997e-02,\n",
       "        1.37803927e-02,  1.74893066e-02,  1.44222453e-02,  1.52570084e-02,\n",
       "        1.24091357e-02,  1.12480372e-02,  1.42083317e-02,  1.71811804e-02,\n",
       "        1.81473121e-02,  1.55646950e-02, -2.88645886e-02,  1.55930221e-02,\n",
       "        1.61708146e-02, -1.80220902e-02,  9.44302231e-03,  1.71899274e-02,\n",
       "       -1.17063709e-02,  1.44027472e-02,  1.58510059e-02,  1.77443624e-02,\n",
       "        1.68555677e-02,  1.70536563e-02,  1.43752992e-02,  8.85685533e-03,\n",
       "       -1.22059882e-03, -3.26032676e-02,  1.14274696e-02,  1.41084641e-02,\n",
       "        1.35013759e-02,  1.72855183e-02,  1.06619298e-02,  1.59627795e-02,\n",
       "        1.21546537e-02,  1.63469315e-02,  1.42490417e-02,  1.25425905e-02,\n",
       "        1.74279064e-02,  1.73853338e-02,  1.30973533e-02,  1.17284879e-02,\n",
       "        1.60373822e-02,  1.53037235e-02,  1.51386559e-02,  2.36194581e-03,\n",
       "        1.35207549e-02,  1.74085796e-02,  1.51687339e-02, -3.46128345e-02,\n",
       "       -1.26733817e-02,  1.52886733e-02,  1.04808509e-02, -1.40637457e-02,\n",
       "        1.00106969e-02,  1.61468014e-02,  1.32615939e-02,  1.63895488e-02,\n",
       "        1.68149173e-02,  1.49794817e-02,  1.37726441e-02,  1.40579417e-02,\n",
       "        1.78471133e-02,  1.34850815e-02,  6.23404980e-04,  1.70580745e-02,\n",
       "        1.09701827e-02,  1.58424526e-02,  1.68898329e-02,  1.03818327e-02,\n",
       "        1.69144422e-02,  1.23177618e-02,  1.55744702e-02,  1.78210661e-02,\n",
       "        1.42001659e-02, -1.00481436e-02,  1.25543475e-02,  1.14696398e-02,\n",
       "        1.60849541e-02,  1.53387338e-02,  1.72177255e-02,  1.13243163e-02,\n",
       "        1.14751160e-02, -1.85429603e-02,  1.26830637e-02,  1.12560466e-02,\n",
       "        1.56465918e-02,  1.44639388e-02,  8.54554027e-03,  1.35600716e-02,\n",
       "        1.63679495e-02,  1.51716769e-02,  1.20659918e-02,  1.63853914e-02,\n",
       "        1.72839165e-02,  1.37912109e-02, -2.37013474e-02,  1.80250704e-02,\n",
       "       -2.20760852e-02,  1.17271766e-02, -1.66455247e-02,  1.52026936e-02,\n",
       "       -1.03411265e-02, -2.53634900e-03,  1.73866078e-02,  1.25294179e-02,\n",
       "        1.35266110e-02,  1.21267289e-02,  1.56215653e-02,  1.60494968e-02,\n",
       "        1.61764324e-02,  1.21385977e-02,  1.67189837e-02, -3.97283919e-02,\n",
       "        1.41966641e-02,  1.46798193e-02, -1.14790350e-03,  1.64669827e-02,\n",
       "        1.69998109e-02,  1.42945871e-02, -5.18076122e-03,  1.41622648e-02,\n",
       "        1.33203939e-02,  1.44605488e-02,  1.63353086e-02,  8.64267349e-03,\n",
       "        1.47166625e-02,  1.50627419e-02,  1.42549202e-02,  1.17500797e-02,\n",
       "        1.34787112e-02,  1.06003955e-02,  1.72175318e-02,  1.51754618e-02,\n",
       "        1.67521685e-02,  1.30641311e-02,  1.25616565e-02,  1.53829157e-02,\n",
       "        1.31755844e-02,  1.13891512e-02,  1.36295632e-02,  1.37191936e-02,\n",
       "        8.89102370e-03, -2.54551210e-02,  9.56273079e-03,  1.73574239e-02,\n",
       "       -1.43941753e-02,  1.59917548e-02, -2.03134865e-03,  1.07131004e-02,\n",
       "        1.40570849e-02,  1.34443119e-02,  1.76165849e-02,  1.33720934e-02,\n",
       "        1.20223090e-02,  1.69900581e-02,  1.62033439e-02,  1.16990805e-02,\n",
       "        1.43045858e-02,  1.01190880e-02,  1.17666423e-02,  1.34325325e-02,\n",
       "        1.58631057e-02,  1.39863491e-02,  1.24607310e-02,  9.61251557e-04,\n",
       "       -2.38326415e-02,  1.54826194e-02,  6.40757382e-03,  1.74773932e-02,\n",
       "        1.42369047e-02,  1.52468979e-02,  1.11945346e-02,  1.55684501e-02,\n",
       "        1.72231495e-02,  1.66248605e-02, -2.31725127e-02,  1.54540315e-02,\n",
       "        1.38669610e-02, -9.73736122e-03,  1.38725564e-02,  1.59678683e-02,\n",
       "        1.49052590e-02,  1.12922862e-02,  1.63573846e-02,  1.62028372e-02,\n",
       "        1.17152259e-02,  1.49592608e-02,  1.13876909e-02, -1.34430528e-02,\n",
       "        1.49500519e-02,  9.75785404e-03,  1.44203827e-02,  1.26849189e-02,\n",
       "       -1.44505277e-02, -3.19046788e-02,  1.53228864e-02, -3.53225283e-02,\n",
       "        1.48760825e-02,  1.10478401e-02,  1.61622986e-02,  1.65011436e-02,\n",
       "       -2.05939449e-02,  1.23771131e-02, -3.40884514e-02,  1.78810656e-02,\n",
       "        1.76925138e-02,  1.63342208e-02,  1.15868896e-02,  2.82830745e-03,\n",
       "        1.10891983e-02, -3.10458206e-02,  1.52024850e-02, -1.21321380e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "save_obj(pred, \"../pred_upd_Xx.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
