{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, Adadelta, Adagrad, Adam, Nadam, SGD\n",
    "from keras.callbacks import EarlyStopping, TerminateOnNaN\n",
    "from keras import backend as K\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.models import load_model, Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Normalize the data.\n",
    "from sklearn import preprocessing\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "import random\n",
    "\n",
    "def pass_arg(nsim, tr_size, dropoutrate):\n",
    "    print(\"Tr_size:\", tr_size)\n",
    "    def fix_seeds(seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "        sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    #     K.set_session(sess)\n",
    "        tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "    ss = 1\n",
    "    fix_seeds(ss)\n",
    "\n",
    "    # MC dropout\n",
    "    class MCDropout(Dropout):\n",
    "        def call(self, inputs, training=None):\n",
    "            return super(MCDropout, self).call(inputs, training=True)\n",
    "\n",
    "    # import pickle\n",
    "\n",
    "    # def save_obj(obj, name):\n",
    "    #     with open(name, 'wb') as f:\n",
    "    #         pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "    # Compute the RMSE given the ground truth (y_true) and the predictions(y_pred)\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "            return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "    # Making sure final porosity is less than initial\n",
    "    def poros(poroi, porof):\n",
    "        return K.relu(tf.negative(porof)) + K.relu(porof-poroi)\n",
    "\n",
    "    def phy_loss_mean(params):\n",
    "        # useful for cross-checking training\n",
    "        loss1, lam1 = params\n",
    "        def loss(y_true,y_pred):\n",
    "            return lam1*K.mean(K.relu(loss1))\n",
    "        return loss\n",
    "\n",
    "    #function to calculate the combined loss = sum of rmse and phy based loss\n",
    "    def combined_loss(params):\n",
    "        loss1, lam1 = params\n",
    "        def loss(y_true,y_pred):\n",
    "            return mean_squared_error(y_true, y_pred) + lam1 * K.mean(K.relu(loss1))\n",
    "        return loss\n",
    "\n",
    "    def PGNN_train_test(optimizer_name, optimizer_val, use_YPhy, pre_train, tr_size, lamda, iteration, n_nodes, n_layers, drop_frac, reg, samp):\n",
    "\n",
    "    #     fix_seeds(ss)\n",
    "\n",
    "        # Hyper-parameters of the training process\n",
    "    #     batch_size = tr_size\n",
    "        batch_size = 10\n",
    "        num_epochs = 300\n",
    "        val_frac = 0.25\n",
    "        patience_val = 80\n",
    "\n",
    "        # Initializing results filename\n",
    "        exp_name = \"DNN_pre_loss_\" + pre_train + optimizer_name + '_trsize' + str(tr_size) + '_lamda' + str(lamda) + '_iter' + str(iteration)\n",
    "        exp_name = exp_name.replace('.','pt')\n",
    "        results_dir = '../results/'\n",
    "        model_name = results_dir + exp_name + '_model.h5' # storing the trained model\n",
    "\n",
    "        if reg==True and samp==25:\n",
    "            results_name = results_dir + exp_name + '_results_25_regularizer.dat' # storing the results of the model\n",
    "        elif reg==False and samp==25:\n",
    "            results_name = results_dir + exp_name + '_results_25.dat' # storing the results of the model\n",
    "        elif reg==True and samp==1519:\n",
    "            results_name = results_dir + exp_name + '_results_1519_regularizer.dat' # storing the results of the model\n",
    "        elif reg==False and samp==1519:\n",
    "            results_name = results_dir + exp_name + '_results_1519.dat' # storing the results of the model\n",
    "\n",
    "        # Load labeled data\n",
    "        data = np.loadtxt('../data/labeled_data.dat')\n",
    "        x_labeled = data[:, :2] # -2 because we do not need porosity predictions\n",
    "        y_labeled = data[:, -2:-1] # dimensionless bond length and porosity measurements\n",
    "        if samp==25:\n",
    "            data = np.loadtxt('../data/unlabeled_data_BK_constw_v2_25.dat')\n",
    "            x_unlabeled = data[:, :]\n",
    "        elif samp==1519:\n",
    "            data = np.loadtxt('../data/unlabeled_data_BK_constw_v2_1525.dat')\n",
    "            x_unlabeled = data[:, :]\n",
    "\n",
    "        x_unlabeled1 = x_unlabeled[:1303, :]\n",
    "        x_unlabeled2 = x_unlabeled[-6:, :]\n",
    "        x_unlabeled = np.vstack((x_unlabeled1,x_unlabeled2))\n",
    "\n",
    "        # initial porosity\n",
    "        init_poro = x_unlabeled[:, -1]\n",
    "        x_unlabeled = x_unlabeled[:, :2]\n",
    "\n",
    "        # normalize dataset with MinMaxScaler\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "        x_labeled = scaler.fit_transform(x_labeled)\n",
    "        x_unlabeled = scaler.fit_transform(x_unlabeled)\n",
    "        # y_labeled = scaler.fit_transform(y_labeled)\n",
    "\n",
    "    #     # initial porosity & physics outputs are removed\n",
    "    #     x_unlabeled = x_unlabeled[:, :-3]\n",
    "\n",
    "        # train and test data\n",
    "        trainX, trainY = x_labeled[:tr_size,:], y_labeled[:tr_size]\n",
    "    #     testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]   \n",
    "        testX, testY = x_labeled[30:,:], y_labeled[30:]\n",
    "\n",
    "        if use_YPhy == 0:\n",
    "            # Removing the last column from x_unlabeled (corresponding to Y_PHY)\n",
    "            x_unlabeled = x_unlabeled[:,:-1]\n",
    "\n",
    "        dependencies = {\n",
    "         'root_mean_squared_error': root_mean_squared_error\n",
    "            }\n",
    "\n",
    "        # load the pre-trained model using non-calibrated physics-based model predictions (./data/unlabeled.dat)\n",
    "        loaded_model = load_model(results_dir + pre_train, custom_objects=dependencies)\n",
    "\n",
    "        # Creating the model\n",
    "        model = Sequential()\n",
    "        for layer in np.arange(n_layers):\n",
    "            if layer == 0:\n",
    "                model.add(Dense(n_nodes, activation='relu', input_shape=(np.shape(trainX)[1],)))\n",
    "            else:\n",
    "                if reg:\n",
    "                    model.add(Dense(n_nodes, activation='relu', kernel_regularizer=l1_l2(l1=.001, l2=.001)))\n",
    "                else:\n",
    "                    model.add(Dense(n_nodes, activation='relu'))\n",
    "            # model.add(Dropout(rate=drop_frac))\n",
    "            model.add(MCDropout(rate=drop_frac))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "\n",
    "        # pass the weights to all layers but 1st input layer, whose dimensions are updated\n",
    "        for new_layer, layer in zip(model.layers[1:], loaded_model.layers[1:]):\n",
    "            new_layer.set_weights(layer.get_weights())\n",
    "\n",
    "        # physics-based regularization\n",
    "        uinp_sc = K.constant(value=x_unlabeled) # unlabeled input data\n",
    "        lam1 = K.constant(value=lamda[0]) # regularization hyper-parameter\n",
    "    \n",
    "        predictions = model(uinp_sc) # model output at depth i\n",
    "\n",
    "        phyloss2 = poros(init_poro, predictions) # physics loss 1\n",
    "        totloss = combined_loss([phyloss2, lam1])\n",
    "        phyloss = phy_loss_mean([phyloss2, lam1])\n",
    "\n",
    "\n",
    "        model.compile(loss=totloss,\n",
    "                      optimizer=optimizer_val,\n",
    "                      metrics=[phyloss, root_mean_squared_error])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience_val, verbose=1)\n",
    "\n",
    "    #     print('Running...' + optimizer_name)\n",
    "        history = model.fit(trainX, trainY,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=num_epochs,\n",
    "                            verbose=0,\n",
    "                            validation_split=val_frac, callbacks=[early_stopping, TerminateOnNaN()])\n",
    "    \n",
    "    \n",
    "        test_score = model.evaluate(testX, testY, verbose=0)\n",
    "        print(test_score)\n",
    "\n",
    "        test_scores = []\n",
    "        for i in range(int(nsim)):\n",
    "#             print(\"simulation num:\",i)\n",
    "#             predictions = model.predict(testX)\n",
    "#             samples.append(predictions)\n",
    "            test_score = model.evaluate(testX, testY, verbose=0)\n",
    "            test_scores.append(test_score[2])\n",
    "        return np.array(test_scores)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Main Function\n",
    "    if __name__ == '__main__':\n",
    "\n",
    "        fix_seeds(1)\n",
    "\n",
    "        # List of optimizers to choose from    \n",
    "        optimizer_names = ['Adagrad', 'Adadelta', 'Adam', 'Nadam', 'RMSprop', 'SGD', 'NSGD']\n",
    "        optimizer_vals = [Adagrad(clipnorm=1), Adadelta(clipnorm=1), Adam(clipnorm=1), Nadam(clipnorm=1), RMSprop(clipnorm=1), SGD(clipnorm=1.), SGD(clipnorm=1, nesterov=True)]\n",
    "\n",
    "        # selecting the optimizer\n",
    "        optimizer_num = 1\n",
    "        optimizer_name = optimizer_names[optimizer_num]\n",
    "        optimizer_val = optimizer_vals[optimizer_num]\n",
    "\n",
    "        # Selecting Other Hyper-parameters\n",
    "        drop_frac = dropoutrate # Fraction of nodes to be dropped out\n",
    "        use_YPhy = 1 # Whether YPhy is used as another feature in the NN model or not\n",
    "        n_layers = 2 # Number of hidden layers\n",
    "        n_nodes = 5 # Number of nodes per hidden layer\n",
    "\n",
    "        # pre-trained model\n",
    "        pre_train = 'Poro_Pre-trainAdadelta_drop0_nL2_nN5_trsize1308_iter0.h5'\n",
    "\n",
    "        #set lamda\n",
    "        lamda = [0.01] # Physics-based regularization constant  \n",
    "\n",
    "#         # Iterating over different training fractions and splitting indices for train-test splits\n",
    "#         trsize_range = [4,6,8,10,20]\n",
    "\n",
    "#         #default training size = 5000\n",
    "#         tr_size = trsize_range[4]\n",
    "        \n",
    "        tr_size = int(tr_size)\n",
    "\n",
    "        # use regularizer\n",
    "        reg = True\n",
    "\n",
    "        # sample size used\n",
    "        samp = 1519\n",
    "    #     samp = 25\n",
    "\n",
    "        # total number of runs\n",
    "        iter_range = np.arange(1)\n",
    "        testrmse=[]\n",
    "        # iterating through all possible params\n",
    "        for iteration in iter_range:\n",
    "#             results, result_file, pred, obs, rmse, obs_train = PGNN_train_test(optimizer_name, optimizer_val, drop_frac, use_YPhy, \n",
    "#                             iteration, n_layers, n_nodes, tr_size, lamda, reg, samp)\n",
    "#             testrmse.append(rmse)\n",
    "            pred = PGNN_train_test(optimizer_name, optimizer_val, use_YPhy, \n",
    "                                               pre_train, tr_size, lamda, iteration, n_nodes, n_layers, drop_frac, reg, samp)\n",
    "            \n",
    "\n",
    "    return np.squeeze(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr_size: 5\n",
      "[0.00465495977550745, 0.0, 0.018452517688274384]\n",
      "Tr_size: 10\n",
      "[0.0038051134906709194, 0.0, 0.012717374600470066]\n",
      "Tr_size: 15\n",
      "[0.0004660136764869094, 0.0, 0.015454704873263836]\n",
      "Tr_size: 20\n",
      "[0.00044848566176369786, 0.0, 0.015597406774759293]\n",
      "Tr_size: 30\n",
      "[0.0002660432073753327, 0.0, 0.012856529094278812]\n"
     ]
    }
   ],
   "source": [
    "mean_rmses=[]\n",
    "std_rmses=[]\n",
    "# for ii in ([.005,.01,.02,.05,.1,.15,.2,.25,.3,0.5]):\n",
    "for ii in ([5,10,15,20,30]):\n",
    "    test_rmse = pass_arg(50, ii, 0.05)\n",
    "    mean_rmse = np.mean(test_rmse)\n",
    "    std_rmse = np.std(test_rmse)\n",
    "    mean_rmses.append(mean_rmse)\n",
    "    std_rmses.append(std_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.014654044937342405,\n",
       " 0.010849869307130576,\n",
       " 0.014632775839418173,\n",
       " 0.01477941695600748,\n",
       " 0.013223492782562971]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.002223141053603035,\n",
       " 0.001305800732591835,\n",
       " 0.0006557760316852748,\n",
       " 0.0006305779694075574,\n",
       " 0.0009157213424093879]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "save_obj(mean_rmses, \"../mean_rmse_dnn_updloss_MC.dat\")\n",
    "save_obj(std_rmses, \"../std_rmse_dnn_updloss_MC.dat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr_size: 20\n",
      "19/19 [==============================] - 0s 52us/step\n",
      "[0.015291403979063034, 0.0011981830466538668, 0.08771183341741562]\n"
     ]
    }
   ],
   "source": [
    "pred = pass_arg(50, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.37335584e-03, 1.94856222e-03, 4.34233528e-03, 1.79976993e-03,\n",
       "       1.79456628e-03, 2.80441530e-03, 1.06975134e-03, 1.08144968e-03,\n",
       "       2.36601476e-03, 2.80755199e-03, 9.52134375e-04, 7.28175917e-04,\n",
       "       8.28316202e-04, 1.34716392e-03, 7.11925852e-04, 1.69787777e-03,\n",
       "       1.15746888e-03, 1.34285321e-04, 1.15999870e-03, 3.23958276e-03,\n",
       "       8.84820300e-04, 9.76184907e-04, 2.75417347e-03, 1.36949599e-03,\n",
       "       3.28332465e-03, 1.53482391e-03, 1.64989592e-03, 7.30333093e-04,\n",
       "       5.57333988e-04, 7.09685683e-03, 1.49872701e-03, 1.56181736e-03,\n",
       "       2.13612244e-03, 8.86846101e-04, 9.92977992e-04, 2.32536369e-03,\n",
       "       1.08897465e-03, 1.44896924e-03, 7.44325982e-04, 2.09604646e-03,\n",
       "       2.44449568e-03, 1.82411820e-03, 3.52882384e-03, 2.68475548e-03,\n",
       "       2.96447333e-03, 1.25102268e-03, 1.98329519e-03, 3.01357242e-03,\n",
       "       1.52132928e-03, 1.97036774e-03, 3.22001870e-04, 1.20475620e-03,\n",
       "       1.21259701e-03, 1.74869748e-03, 2.93545797e-03, 5.46205556e-03,\n",
       "       6.87613327e-04, 1.58237119e-03, 1.10273971e-03, 1.41157373e-03,\n",
       "       3.00987461e-03, 1.07044110e-03, 2.62309238e-03, 1.65570586e-03,\n",
       "       1.65940321e-03, 2.49659456e-03, 3.31906107e-04, 1.43669837e-03,\n",
       "       2.18945742e-03, 2.59990990e-03, 1.09258329e-03, 4.14972455e-04,\n",
       "       3.95773165e-03, 4.08071093e-03, 1.81711791e-03, 1.00682082e-03,\n",
       "       3.50410264e-04, 1.91434799e-03, 1.39033655e-03, 4.15947492e-04,\n",
       "       4.11392772e-04, 2.31009978e-03, 1.91079278e-03, 2.29125354e-03,\n",
       "       2.30197213e-03, 3.18277394e-03, 1.89586205e-03, 1.96042028e-03,\n",
       "       2.60184240e-03, 8.89128714e-04, 1.62012456e-03, 1.76986994e-03,\n",
       "       1.34760642e-03, 1.98449916e-03, 2.75753462e-03, 2.17244751e-03,\n",
       "       1.39975525e-03, 2.28840346e-03, 2.05638865e-03, 1.43471942e-03,\n",
       "       2.23472621e-03, 3.88704502e-04, 1.43136224e-03, 2.13991851e-03,\n",
       "       1.66211743e-03, 3.36923450e-03, 3.31885437e-03, 5.76924067e-04,\n",
       "       5.36943506e-03, 6.03094930e-04, 5.30584774e-04, 4.26786346e-03,\n",
       "       1.80769013e-03, 1.87099481e-03, 1.36484718e-03, 1.19938375e-03,\n",
       "       1.20230485e-03, 1.40851049e-03, 1.27808820e-03, 2.51923082e-03,\n",
       "       7.64126715e-04, 2.46555102e-03, 3.08289798e-03, 1.82271644e-03,\n",
       "       3.46362311e-03, 7.20512879e-04, 2.15392048e-03, 1.85663102e-03,\n",
       "       3.67306592e-03, 1.02369161e-03, 2.17567687e-03, 1.43804238e-03,\n",
       "       6.06069691e-04, 1.22808141e-03, 3.06720496e-03, 2.17816280e-03,\n",
       "       4.42833826e-03, 1.54644996e-03, 2.04289588e-03, 1.59252947e-03,\n",
       "       2.27452489e-03, 1.49372534e-03, 9.55258380e-04, 1.03331066e-03,\n",
       "       2.26074233e-04, 2.34800391e-03, 1.64784596e-03, 2.01403163e-03,\n",
       "       1.16071035e-03, 3.85072269e-03, 1.39647711e-03, 6.68803754e-04,\n",
       "       1.05348288e-03, 1.54406123e-03, 1.85482530e-03, 6.49323745e-04,\n",
       "       5.17975714e-04, 9.09863040e-04, 7.49628758e-04, 8.03266070e-04,\n",
       "       2.52983579e-03, 1.87790848e-03, 2.08459143e-03, 1.27444614e-03,\n",
       "       1.97126786e-03, 1.01551006e-03, 1.52471522e-03, 1.07667525e-03,\n",
       "       2.80389329e-03, 4.33629670e-04, 3.34224012e-03, 2.16656900e-03,\n",
       "       1.26842002e-03, 2.30466318e-03, 5.21768350e-04, 2.36167270e-03,\n",
       "       2.69041536e-03, 1.36938214e-03, 3.41131422e-03, 1.34536845e-03,\n",
       "       2.12670444e-03, 4.31027962e-03, 3.09372810e-03, 4.59222589e-04,\n",
       "       1.74527289e-03, 3.52844223e-03, 2.51920335e-03, 1.13029935e-04,\n",
       "       2.99569732e-03, 1.85405102e-03, 1.13076554e-03, 2.05573300e-03,\n",
       "       1.33164437e-03, 1.66815938e-03, 1.43480150e-03, 6.13310549e-04,\n",
       "       1.87271135e-03, 1.69210008e-03, 9.71889589e-04, 1.72546611e-03,\n",
       "       4.21875273e-04, 4.57733590e-03, 3.56213818e-03, 1.54440326e-03,\n",
       "       1.69422550e-04, 2.16347442e-04, 9.46339977e-04, 1.30703347e-03,\n",
       "       1.16823346e-03, 2.05477769e-03, 1.65749295e-03, 4.35824273e-03,\n",
       "       3.40202852e-04, 2.82389601e-03, 1.76244823e-03, 4.71297535e-04,\n",
       "       1.38275372e-03, 1.67292205e-03, 1.56733661e-03, 1.85492728e-03,\n",
       "       1.66011055e-03, 1.63639232e-03, 1.10909820e-03, 2.93733529e-03,\n",
       "       4.96691500e-04, 2.24388647e-03, 4.72824415e-03, 1.24438200e-03,\n",
       "       4.97058325e-04, 1.11327412e-04, 3.83846508e-03, 1.31565239e-03,\n",
       "       7.99338566e-04, 1.45171047e-03, 1.70061737e-03, 4.29901900e-03,\n",
       "       1.99438934e-03, 1.43904018e-03, 3.75763490e-03, 2.07401533e-03,\n",
       "       9.33736796e-04, 4.35220357e-03, 3.50691797e-03, 1.86516810e-03,\n",
       "       1.29900163e-03, 1.91738538e-03, 6.82684418e-04, 1.20174407e-03,\n",
       "       3.77716357e-03, 1.12473790e-03, 4.36271075e-03, 5.48367854e-04,\n",
       "       1.24453125e-03, 1.84281124e-03, 3.04239307e-04, 2.07661279e-03,\n",
       "       2.65928335e-03, 1.44828588e-03, 1.83798256e-03, 6.58528041e-03,\n",
       "       1.88076857e-03, 1.29724341e-03, 1.37988059e-03, 3.39755882e-03,\n",
       "       1.72046502e-03, 1.81685470e-03, 1.02060137e-03, 2.70746299e-03,\n",
       "       2.12775101e-03, 2.44217389e-03, 2.40475428e-03, 1.25467440e-03,\n",
       "       3.23379855e-03, 1.37088215e-03, 1.49863958e-03, 1.74381118e-03,\n",
       "       6.28553273e-04, 2.13731686e-03, 2.11658725e-03, 1.51355634e-03,\n",
       "       2.09678966e-03, 1.70660240e-03, 5.34403045e-03, 1.70251704e-03,\n",
       "       1.90464885e-03, 3.43330228e-03, 1.71249057e-03, 1.73086778e-03,\n",
       "       2.18572887e-03, 2.08597281e-03, 2.53421627e-03, 3.22641642e-03,\n",
       "       2.12380360e-03, 2.12187297e-03, 1.61105092e-03, 1.73907063e-03,\n",
       "       1.40167226e-03, 2.65452825e-03, 8.78651510e-04, 1.29330321e-03,\n",
       "       1.81163230e-03, 1.50026532e-03, 2.64039100e-03, 1.69933145e-03,\n",
       "       2.04101391e-03, 2.59868987e-03, 1.81068643e-03, 1.46385282e-03,\n",
       "       1.26507250e-03, 1.04024645e-03, 1.71842601e-03, 2.05457839e-03,\n",
       "       9.61498008e-04, 3.04595334e-03, 2.32398463e-03, 1.64924411e-03,\n",
       "       7.47964077e-05, 5.25087863e-03, 1.60074118e-03, 2.69235810e-03,\n",
       "       2.94408738e-03, 1.49848417e-03, 2.20249826e-03, 4.44575539e-03,\n",
       "       2.71139923e-03, 5.36346529e-03, 8.19967128e-04, 1.09726796e-03,\n",
       "       4.66773426e-03, 7.78088812e-04, 1.87904038e-03, 2.14017928e-03,\n",
       "       1.30398152e-03, 1.64529576e-03, 9.63211933e-04, 1.31455075e-03,\n",
       "       2.13739206e-03, 2.03069323e-03, 1.78476155e-03, 1.99011923e-03,\n",
       "       5.28906414e-04, 2.53354455e-03, 5.03822463e-03, 2.83407327e-03,\n",
       "       1.19439326e-03, 1.88254262e-03, 1.51083665e-03, 2.55328696e-03,\n",
       "       1.79290457e-03, 1.75652304e-03, 1.36432447e-03, 1.73724978e-03,\n",
       "       6.91135006e-04, 3.17832595e-03, 1.73647504e-03, 2.91773328e-03,\n",
       "       3.49075766e-03, 1.80847046e-03, 1.37490383e-03, 2.78482889e-03,\n",
       "       2.87862890e-03, 3.84324929e-03, 3.44986166e-03, 1.76009582e-03,\n",
       "       1.70242146e-03, 2.65729497e-03, 1.21256011e-03, 3.88380256e-03,\n",
       "       1.65930553e-03, 2.40684766e-03, 8.03442905e-04, 1.48561515e-03,\n",
       "       1.58048712e-03, 3.92102695e-04, 2.69242865e-03, 1.61607703e-03,\n",
       "       1.34726404e-03, 1.45350862e-03, 9.11978423e-04, 8.19280685e-04,\n",
       "       1.87044858e-03, 2.31959880e-03, 1.78509019e-03, 9.01360007e-04,\n",
       "       1.79394521e-03, 1.04218640e-03, 2.75761238e-03, 2.68544158e-04,\n",
       "       1.97970169e-03, 1.00424082e-03, 3.71031638e-04, 1.67573418e-03,\n",
       "       1.27226813e-03, 4.07270016e-03, 1.34427857e-03, 2.07642443e-03,\n",
       "       5.31848846e-03, 1.20746077e-03, 1.73162052e-03, 1.83323151e-04,\n",
       "       9.36540717e-04, 3.98680428e-03, 4.71264264e-03, 1.92929979e-03,\n",
       "       1.73837261e-03, 9.84974555e-04, 2.00459803e-03, 2.02245219e-03,\n",
       "       1.68267265e-03, 1.93501858e-03, 6.22880820e-04, 2.52874452e-03,\n",
       "       1.03901001e-03, 4.20478266e-03, 1.28212466e-03, 5.27268276e-04,\n",
       "       1.92201836e-03, 1.89176376e-03, 1.57258078e-03, 1.84193300e-03,\n",
       "       1.50105276e-03, 3.51930875e-03, 4.34871763e-03, 1.20759511e-03,\n",
       "       1.44105300e-03, 4.59809089e-03, 3.16561433e-03, 1.82884256e-03,\n",
       "       1.08805508e-03, 2.54554837e-03, 2.31727702e-03, 3.79275327e-04,\n",
       "       1.47564523e-03, 2.37756898e-03, 2.73218541e-03, 2.44007344e-04,\n",
       "       7.14689901e-04, 1.72213581e-03, 1.00299157e-03, 2.60051386e-03,\n",
       "       4.14144527e-03, 2.59471964e-03, 7.47402373e-04, 1.66017062e-03,\n",
       "       1.77922228e-03, 4.51324554e-03, 3.08509497e-03, 3.25025117e-04,\n",
       "       8.48961645e-04, 2.43479526e-03, 1.05854811e-03, 4.23991587e-04,\n",
       "       3.43587785e-03, 1.82388030e-04, 3.26650124e-03, 1.88757863e-03,\n",
       "       1.87740708e-03, 4.29676991e-04, 1.87906937e-03, 1.18762907e-03,\n",
       "       2.06320430e-03, 1.72533735e-03, 2.00207881e-03, 2.75776582e-03,\n",
       "       5.16591035e-03, 2.45819241e-03, 1.59925490e-03, 1.43861398e-03,\n",
       "       5.01019554e-03, 1.72044279e-03, 3.07282712e-03, 1.30688667e-03,\n",
       "       1.47164648e-03, 1.06704549e-03, 1.56685943e-03, 2.08205380e-03,\n",
       "       2.58135144e-03, 2.13144394e-03, 2.13051145e-03, 9.47494875e-04,\n",
       "       1.75961421e-03, 2.25773384e-03, 1.28737674e-03, 1.61096058e-03,\n",
       "       4.70830407e-03, 9.83719947e-04, 8.65731388e-04, 3.96571867e-03,\n",
       "       4.85788472e-03, 1.34302850e-03, 1.38707832e-03, 2.52982299e-03,\n",
       "       3.31599003e-04, 2.40527140e-03, 1.43815507e-03, 1.23172661e-03,\n",
       "       1.30443810e-03, 1.45403808e-03, 1.61999115e-03, 3.40591796e-04,\n",
       "       1.66008621e-03, 8.05970514e-04, 3.35104135e-03, 1.74817035e-03,\n",
       "       2.17282260e-03, 5.88613329e-03, 8.88319628e-04, 1.44071050e-03,\n",
       "       2.55274563e-03, 2.40612496e-03, 8.36245134e-04, 3.19221825e-03,\n",
       "       6.02357835e-03, 2.97648017e-04, 2.00800295e-03, 3.18393810e-03,\n",
       "       6.87687949e-04, 1.05439185e-03, 1.25952333e-03, 2.08878960e-03,\n",
       "       1.97549979e-03, 1.35366630e-03, 4.64649824e-03, 1.94363890e-03,\n",
       "       1.79999089e-03, 1.91776617e-03, 1.11015874e-03, 1.68345880e-03,\n",
       "       5.15304040e-04, 1.78317330e-03, 1.93115743e-03, 8.34299834e-04,\n",
       "       2.67269765e-03, 3.72284092e-03, 1.74921227e-03, 1.84222497e-03,\n",
       "       1.26253476e-03, 1.83049345e-03, 5.36911841e-03, 1.39611540e-03,\n",
       "       2.57639680e-04, 8.80381442e-04, 8.72378354e-04, 1.23086758e-03,\n",
       "       1.39249954e-03, 1.13293354e-03, 1.14797021e-03, 1.57945498e-03,\n",
       "       2.08819867e-03, 3.58874374e-03, 2.23210058e-03, 2.29372038e-03,\n",
       "       1.87589857e-03, 3.68391373e-03, 3.09901382e-03, 2.04616878e-03,\n",
       "       2.63892300e-03, 3.13260389e-04, 4.77711158e-03, 3.12515767e-03,\n",
       "       5.54099446e-03, 1.09686377e-03, 2.06109625e-03, 3.21403914e-03,\n",
       "       2.38553900e-03, 4.28715535e-03, 2.38459720e-03, 1.20915170e-03,\n",
       "       2.07333523e-03, 2.59643677e-03, 5.74867765e-04, 2.79125571e-03,\n",
       "       1.97082665e-03, 1.67687505e-03, 2.03875545e-03, 1.69822969e-03,\n",
       "       2.57931883e-03, 1.45824277e-03, 6.97850599e-04, 1.66058808e-03,\n",
       "       1.46691315e-03, 2.52610305e-03, 1.78568391e-03, 1.50287873e-03,\n",
       "       1.62526150e-03, 1.72220229e-03, 4.09793667e-03, 4.87686461e-03,\n",
       "       1.55010610e-03, 1.24826981e-03, 1.16621389e-03, 2.04000599e-03,\n",
       "       2.08122167e-03, 1.04125449e-03, 1.92602922e-04, 2.53375852e-03,\n",
       "       6.78363489e-04, 1.26649276e-03, 2.85472418e-03, 1.42320886e-03,\n",
       "       1.39677164e-03, 2.65291455e-04, 2.48539331e-03, 1.75384711e-03,\n",
       "       5.95974445e-04, 1.40655099e-03, 1.91938516e-03, 4.84273816e-03,\n",
       "       1.69923110e-03, 3.25413974e-04, 1.14208716e-03, 2.50728498e-03,\n",
       "       1.76121888e-03, 1.55500986e-03, 4.04777471e-03, 2.23522470e-03,\n",
       "       1.98502489e-03, 3.75275756e-03, 1.88514160e-03, 1.48801284e-03,\n",
       "       1.41669286e-03, 9.34431562e-04, 1.65363413e-03, 2.35882495e-03,\n",
       "       4.25913977e-03, 1.44845690e-03, 4.06462373e-03, 3.05995392e-03,\n",
       "       1.84649089e-03, 1.52667088e-03, 2.69177277e-03, 1.31910807e-03,\n",
       "       1.84409495e-03, 4.29992191e-03, 1.84186106e-03, 2.66472227e-03,\n",
       "       1.73141342e-03, 1.48776430e-03, 2.14916258e-03, 1.91124948e-03,\n",
       "       1.69521745e-03, 1.40587881e-03, 8.71006574e-04, 1.01118255e-03,\n",
       "       1.49713270e-03, 3.58312303e-04, 2.78240326e-03, 1.60052569e-03,\n",
       "       2.56324559e-03, 5.12660248e-03, 2.83215707e-03, 9.61157028e-04,\n",
       "       1.70354708e-03, 2.45038653e-03, 7.72641157e-04, 2.87030096e-04,\n",
       "       3.67452228e-03, 1.62305462e-03, 2.85290997e-03, 4.39165765e-03,\n",
       "       1.94913719e-03, 3.80755402e-03, 1.72803400e-03, 2.03585858e-03,\n",
       "       4.49863868e-03, 2.09245621e-03, 2.55811657e-03, 3.44299292e-03,\n",
       "       1.50385173e-03, 1.10059837e-03, 1.32135418e-03, 6.07596128e-04,\n",
       "       2.90476670e-03, 1.69811759e-03, 1.62289524e-03, 1.26559485e-03,\n",
       "       1.73365371e-03, 3.15553229e-03, 1.13035878e-03, 2.50225072e-03,\n",
       "       1.00603397e-03, 8.25202034e-04, 4.93066385e-04, 2.05282588e-03,\n",
       "       4.78584872e-04, 1.79587095e-03, 9.59904224e-04, 1.59676862e-03,\n",
       "       6.20914274e-04, 1.88983872e-03, 3.80878279e-04, 1.30070688e-03,\n",
       "       1.12871360e-03, 1.81798951e-03, 4.85810131e-04, 1.85151852e-03,\n",
       "       2.25446816e-03, 7.74810789e-04, 1.26999093e-03, 2.01381277e-03,\n",
       "       2.37977179e-03, 1.71016937e-03, 3.80764785e-03, 1.44708727e-03,\n",
       "       2.55077891e-03, 1.15914294e-03, 4.61535150e-04, 1.22873788e-03,\n",
       "       2.39544478e-03, 1.22172688e-03, 1.56729121e-03, 3.79607384e-03,\n",
       "       2.68869475e-03, 1.98001997e-03, 2.09933077e-03, 2.98443437e-03,\n",
       "       2.02605501e-03, 2.39044824e-03, 1.06269540e-03, 2.16325559e-03,\n",
       "       3.09197139e-03, 1.20555935e-03, 1.30415859e-03, 2.42691999e-03,\n",
       "       1.40911515e-03, 1.89007609e-03, 3.24058323e-03, 2.02118582e-03,\n",
       "       1.62605045e-03, 7.96310254e-04, 3.49889905e-03, 2.38229288e-03,\n",
       "       1.72356388e-03, 2.00916640e-03, 1.46066560e-03, 1.77321094e-03,\n",
       "       1.88944547e-03, 2.06050929e-03, 4.19538177e-04, 2.59810081e-03,\n",
       "       4.73910855e-04, 1.79906131e-03, 2.26131617e-03, 2.15655169e-03,\n",
       "       2.53127655e-03, 1.91470399e-03, 3.45115154e-03, 5.60382323e-04,\n",
       "       1.52267446e-03, 1.34133757e-03, 1.31154968e-03, 2.04052497e-03,\n",
       "       1.02812727e-03, 1.77503238e-03, 6.74429059e-04, 7.17124029e-04,\n",
       "       1.92709500e-03, 1.16995862e-03, 1.38292357e-03, 2.54267757e-03,\n",
       "       2.34647305e-03, 1.60911866e-03, 2.41674995e-03, 2.72553833e-03,\n",
       "       2.73078959e-03, 3.71425273e-03, 2.31825095e-03, 3.18677048e-03,\n",
       "       1.17936882e-03, 1.69120182e-03, 2.69464753e-03, 5.12871612e-03,\n",
       "       1.93463580e-03, 1.24950777e-04, 1.84333604e-03, 6.70971640e-04,\n",
       "       2.26130360e-03, 3.85667430e-03, 7.88101286e-04, 4.20938246e-04,\n",
       "       1.93498319e-03, 3.42767290e-03, 1.68657722e-03, 2.43487209e-03,\n",
       "       2.55306996e-03, 1.56050036e-03, 3.32475873e-03, 2.23873486e-03,\n",
       "       2.12049391e-03, 1.57910120e-03, 2.28102063e-03, 1.14805275e-03,\n",
       "       2.86009046e-03, 2.91621382e-03, 1.70241157e-03, 1.74681121e-03,\n",
       "       1.04278757e-03, 2.38818419e-03, 4.69886698e-04, 1.27602182e-03,\n",
       "       1.71217707e-03, 2.47986696e-04, 4.82937461e-03, 1.49003207e-03,\n",
       "       6.11056224e-04, 1.33874163e-03, 1.47456583e-03, 1.57305738e-03,\n",
       "       1.08601130e-03, 2.04256456e-03, 2.77727400e-03, 2.49176100e-03,\n",
       "       9.63742496e-04, 2.35666710e-04, 2.67781923e-03, 1.57707825e-03,\n",
       "       3.43644666e-03, 1.98199530e-03, 3.09995026e-03, 1.57411280e-03,\n",
       "       2.80580577e-03, 1.41954131e-03, 2.27633235e-03, 1.48717582e-03,\n",
       "       1.06900779e-03, 1.92875857e-03, 3.38908215e-03, 7.21893855e-04,\n",
       "       1.76189363e-03, 1.05809828e-03, 1.74881308e-03, 9.05712601e-04,\n",
       "       3.30914208e-03, 1.26561848e-03, 1.63514051e-03, 2.55169201e-04,\n",
       "       6.18482765e-04, 1.61848066e-03, 4.99608787e-03, 7.16275070e-04,\n",
       "       2.79998500e-03, 2.22663558e-03, 2.38988618e-03, 1.41897064e-03,\n",
       "       1.70783990e-03, 1.75144942e-03, 3.22794775e-03, 1.79855444e-03,\n",
       "       1.35982048e-03, 6.56761113e-04, 8.14420287e-04, 1.79536710e-03,\n",
       "       4.33208048e-03, 2.39698892e-03, 2.17840960e-03, 2.23252084e-03,\n",
       "       1.91953208e-03, 2.42921989e-03, 1.58199691e-03, 7.93280313e-04,\n",
       "       3.00285290e-03, 8.38951266e-04, 3.64107057e-03, 1.49192475e-03,\n",
       "       1.08027959e-03, 1.53419445e-03, 1.60502316e-03, 2.52008904e-03,\n",
       "       2.44682981e-03, 5.97962178e-04, 3.29100690e-03, 3.85636324e-03,\n",
       "       1.92462676e-03, 2.22618808e-03, 8.59259628e-04, 2.99298251e-03,\n",
       "       1.15059048e-03, 4.78030037e-04, 2.44443258e-03, 1.57093373e-03,\n",
       "       1.70514348e-03, 1.37434748e-03, 5.89459378e-04, 1.41556095e-03,\n",
       "       5.92059630e-04, 2.05071457e-03, 6.37691235e-04, 2.77672173e-03,\n",
       "       6.50723872e-04, 7.87313737e-04, 1.73768704e-03, 2.61146785e-03,\n",
       "       2.41605588e-03, 2.92092445e-03, 2.03673961e-03, 2.33692443e-03,\n",
       "       1.10300863e-03, 4.23335191e-03, 1.79956190e-03, 2.73348967e-04,\n",
       "       2.66549736e-03, 2.73281871e-03, 8.87692964e-04, 1.82316499e-03,\n",
       "       1.92283734e-03, 1.67652662e-03, 8.52680532e-04, 1.57165108e-03,\n",
       "       3.49916308e-03, 2.20370549e-03, 1.59868121e-03, 1.09202624e-03,\n",
       "       1.95795810e-03, 1.75371324e-03, 1.32064824e-03, 2.07836810e-03,\n",
       "       2.96889897e-03, 1.04028150e-03, 1.31875172e-03, 1.90518599e-03,\n",
       "       1.44214067e-03, 1.79347210e-03, 4.04552324e-03, 1.76769600e-03,\n",
       "       3.27623961e-03, 2.80173006e-03, 2.30291486e-03, 2.18894146e-03,\n",
       "       4.29753819e-03, 3.96389718e-04, 2.80684815e-03, 1.72841002e-03,\n",
       "       8.32079328e-04, 1.23499031e-03, 8.70833523e-04, 2.68168957e-03,\n",
       "       2.90034083e-03, 1.55486353e-03, 9.15927463e-04, 2.03278428e-03,\n",
       "       2.91162916e-03, 1.35879754e-03, 1.80400850e-03, 2.40794802e-03,\n",
       "       2.38292804e-03, 1.20574713e-03, 2.92453938e-03, 2.08525942e-03,\n",
       "       1.59221725e-03, 3.18986853e-03, 1.64728554e-03, 1.08403096e-03,\n",
       "       4.04944090e-04, 1.90798088e-03, 1.06106780e-03, 1.93714129e-03,\n",
       "       2.29740934e-03, 1.69379869e-03, 2.86242459e-03, 1.64335105e-03,\n",
       "       1.66605366e-03, 1.69518369e-03, 6.31072966e-04, 1.73345394e-03,\n",
       "       2.28923257e-03, 5.95116871e-04, 2.49189488e-03, 1.45506591e-03,\n",
       "       4.74221166e-03, 3.64696002e-03, 1.79535663e-03, 1.25663565e-03,\n",
       "       2.56798812e-03, 2.48256419e-03, 3.92089644e-03, 4.73346590e-04,\n",
       "       1.13272341e-03, 9.20392689e-04, 1.33722415e-03, 2.82181613e-03,\n",
       "       6.00517495e-04, 3.98704782e-04, 1.33181049e-03, 2.32045786e-04,\n",
       "       2.46483041e-03, 3.49986251e-03, 1.63948815e-03, 1.25702797e-03,\n",
       "       5.48246666e-04, 2.65123718e-03, 3.69326794e-04, 1.77412538e-03,\n",
       "       2.02213530e-03, 2.22979486e-03, 1.76435674e-03, 5.97793958e-04,\n",
       "       1.75029528e-03, 2.85709772e-04, 2.13284069e-03, 4.32362023e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(pred,axis=0)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0431564 , 0.04969775, 0.03855895, 0.0473083 , 0.04825364,\n",
       "       0.04153984, 0.03876032, 0.02742871, 0.05394102, 0.03834876,\n",
       "       0.05311058, 0.04025944, 0.04402751, 0.0490651 , 0.04529456,\n",
       "       0.05169499, 0.05430209, 0.03668012, 0.05224204, 0.03989543,\n",
       "       0.05179229, 0.05054558, 0.04952174, 0.05267722, 0.03513261,\n",
       "       0.05475615, 0.0515241 , 0.03853419, 0.02725641, 0.05868765,\n",
       "       0.05132379, 0.06161826, 0.044791  , 0.05282198, 0.05968352,\n",
       "       0.036227  , 0.04257078, 0.04440696, 0.050751  , 0.05052431,\n",
       "       0.03119921, 0.05248038, 0.05077071, 0.05365586, 0.04209185,\n",
       "       0.04676313, 0.04421702, 0.04579834, 0.04491324, 0.05062904,\n",
       "       0.01135702, 0.05096268, 0.0548835 , 0.04697408, 0.04883337,\n",
       "       0.02582511, 0.03095761, 0.03505554, 0.05255293, 0.04615692,\n",
       "       0.04236671, 0.04940849, 0.05350411, 0.05224761, 0.04388189,\n",
       "       0.04533507, 0.0102862 , 0.04446835, 0.04741205, 0.05951528,\n",
       "       0.05657636, 0.02142219, 0.03366445, 0.04343371, 0.05729761,\n",
       "       0.0252766 , 0.02903133, 0.05575779, 0.05617744, 0.03257521,\n",
       "       0.02247935, 0.05404596, 0.04371475, 0.02709154, 0.05677767,\n",
       "       0.04833161, 0.04828109, 0.04940727, 0.04948597, 0.03609862,\n",
       "       0.05955745, 0.04296338, 0.04552327, 0.03543995, 0.03283273,\n",
       "       0.04931949, 0.04865997, 0.04068929, 0.04790096, 0.03912064,\n",
       "       0.0398428 , 0.02473088, 0.0553575 , 0.04226862, 0.04777663,\n",
       "       0.0419123 , 0.03380325, 0.03178661, 0.03283432, 0.04540551,\n",
       "       0.03196609, 0.02913073, 0.04191255, 0.03920836, 0.05312479,\n",
       "       0.05010604, 0.05794572, 0.05960811, 0.0478517 , 0.03952923,\n",
       "       0.02792164, 0.03002935, 0.03173809, 0.0426522 , 0.0399608 ,\n",
       "       0.04132742, 0.04984227, 0.04796317, 0.04463214, 0.0515863 ,\n",
       "       0.04796929, 0.04078643, 0.02529024, 0.04857281, 0.03348636,\n",
       "       0.05062439, 0.04521937, 0.04384649, 0.04825238, 0.04329121,\n",
       "       0.04137245, 0.05389285, 0.03577598, 0.04470078, 0.00553215,\n",
       "       0.03398529, 0.03917249, 0.0413027 , 0.04619595, 0.04303227,\n",
       "       0.06260688, 0.03073607, 0.04960412, 0.04583958, 0.04585851,\n",
       "       0.01829362, 0.0218051 , 0.05015438, 0.03272357, 0.03669926,\n",
       "       0.04878677, 0.0508391 , 0.03996155, 0.05604449, 0.05110639,\n",
       "       0.04692018, 0.04738482, 0.04222366, 0.05050602, 0.05464261,\n",
       "       0.03319599, 0.0319128 , 0.04914628, 0.0364556 , 0.04125248,\n",
       "       0.04305835, 0.0378779 , 0.03844839, 0.04437673, 0.06047864,\n",
       "       0.03894733, 0.03540257, 0.03492536, 0.01547244, 0.04132587,\n",
       "       0.02861336, 0.05560821, 0.01183897, 0.04610988, 0.03244675,\n",
       "       0.04746086, 0.04299951, 0.04531232, 0.04769249, 0.04575795,\n",
       "       0.02248874, 0.03023947, 0.04670418, 0.05975338, 0.0473024 ,\n",
       "       0.02338439, 0.03805327, 0.04121935, 0.04858759, 0.01108842,\n",
       "       0.00899426, 0.05761997, 0.05853718, 0.04760728, 0.05635706,\n",
       "       0.04396947, 0.02266472, 0.01297426, 0.03808793, 0.04404166,\n",
       "       0.02396549, 0.04617022, 0.04287783, 0.05418656, 0.04238559,\n",
       "       0.04195327, 0.05710537, 0.0509432 , 0.03638472, 0.02617969,\n",
       "       0.03634106, 0.0397693 , 0.05406874, 0.06041196, 0.01295193,\n",
       "       0.03074803, 0.05457698, 0.05852643, 0.03632618, 0.05282684,\n",
       "       0.03595927, 0.04053573, 0.05078574, 0.03500913, 0.05497805,\n",
       "       0.04956181, 0.02845237, 0.0407478 , 0.04602344, 0.05498571,\n",
       "       0.05072803, 0.02578902, 0.0464456 , 0.04435587, 0.04677314,\n",
       "       0.05173348, 0.01977258, 0.05182871, 0.03645828, 0.0279105 ,\n",
       "       0.04235893, 0.05076715, 0.05087312, 0.05424659, 0.06855181,\n",
       "       0.04422294, 0.04404384, 0.0480559 , 0.0303419 , 0.04007422,\n",
       "       0.04775951, 0.04673964, 0.04583165, 0.04843107, 0.03807228,\n",
       "       0.05448342, 0.04760987, 0.0643728 , 0.05089721, 0.05054045,\n",
       "       0.0332551 , 0.02468076, 0.03762875, 0.03988898, 0.05757271,\n",
       "       0.03790466, 0.04298424, 0.05868273, 0.03670799, 0.03756181,\n",
       "       0.05214033, 0.04472421, 0.05696785, 0.04494815, 0.03450862,\n",
       "       0.04704181, 0.02334997, 0.02986762, 0.05161224, 0.05620141,\n",
       "       0.05222939, 0.04467931, 0.0489997 , 0.0495365 , 0.04621155,\n",
       "       0.04337701, 0.05346574, 0.0497233 , 0.0382026 , 0.03084383,\n",
       "       0.04334359, 0.0467745 , 0.04811399, 0.05540412, 0.05319657,\n",
       "       0.04676824, 0.04576552, 0.0439999 , 0.04333228, 0.02536245,\n",
       "       0.03591536, 0.02295679, 0.02831741, 0.05429457, 0.03872026,\n",
       "       0.0488098 , 0.06217891, 0.04496687, 0.03768866, 0.03325614,\n",
       "       0.04705235, 0.03717739, 0.04767556, 0.04042361, 0.03711223,\n",
       "       0.04779476, 0.0424524 , 0.05121006, 0.04502894, 0.05721328,\n",
       "       0.04967029, 0.04543351, 0.04812967, 0.05915443, 0.04353216,\n",
       "       0.01751068, 0.03379069, 0.05766879, 0.03767007, 0.05351223,\n",
       "       0.03349093, 0.04992897, 0.04517258, 0.04069404, 0.04205312,\n",
       "       0.04795181, 0.05169808, 0.0374911 , 0.03150341, 0.04778771,\n",
       "       0.04311692, 0.04088245, 0.05157347, 0.0443037 , 0.04732099,\n",
       "       0.03154638, 0.03420469, 0.04484023, 0.04555113, 0.04694647,\n",
       "       0.05122086, 0.04627543, 0.04252163, 0.04474524, 0.03122801,\n",
       "       0.01934869, 0.04319685, 0.04142588, 0.03456147, 0.05068298,\n",
       "       0.04869697, 0.05239601, 0.04956362, 0.05983573, 0.02153687,\n",
       "       0.03550952, 0.0464869 , 0.04462811, 0.02719247, 0.05773027,\n",
       "       0.05161191, 0.05878594, 0.00208639, 0.04201858, 0.05184774,\n",
       "       0.04426338, 0.05117898, 0.05073817, 0.05351448, 0.05402535,\n",
       "       0.04577507, 0.05611808, 0.05914985, 0.04466726, 0.02322593,\n",
       "       0.03792042, 0.05123575, 0.05932598, 0.04795756, 0.03675182,\n",
       "       0.02751456, 0.03912448, 0.04538932, 0.04836195, 0.05051495,\n",
       "       0.02866215, 0.0432008 , 0.0395794 , 0.04443843, 0.04305637,\n",
       "       0.02278794, 0.03924836, 0.04040878, 0.04731345, 0.04340834,\n",
       "       0.04934145, 0.04595838, 0.03995411, 0.0458116 , 0.05424259,\n",
       "       0.04345906, 0.05392049, 0.05280305, 0.04987638, 0.05991869,\n",
       "       0.04251751, 0.01701319, 0.03208505, 0.03382802, 0.03842177,\n",
       "       0.04426273, 0.03181393, 0.05166826, 0.05148867, 0.05092975,\n",
       "       0.04996853, 0.03707433, 0.06044374, 0.05923851, 0.05260835,\n",
       "       0.05374192, 0.04310514, 0.04090143, 0.04931208, 0.03354262,\n",
       "       0.0534268 , 0.02893621, 0.04165035, 0.01263647, 0.04555151,\n",
       "       0.0496933 , 0.03698523, 0.0215331 , 0.05607457, 0.05355519,\n",
       "       0.05140696, 0.03206646, 0.03988279, 0.036453  , 0.03297009,\n",
       "       0.0348795 , 0.04881245, 0.04895946, 0.05432425, 0.04270628,\n",
       "       0.04606979, 0.0502889 , 0.04384214, 0.0488308 , 0.04669532,\n",
       "       0.05552838, 0.02549283, 0.03876086, 0.0468627 , 0.03680868,\n",
       "       0.05540834, 0.05230523, 0.03453429, 0.04735253, 0.04590225,\n",
       "       0.03527714, 0.06442207, 0.06024581, 0.03972135, 0.06082097,\n",
       "       0.04986886, 0.04036294, 0.06468292, 0.0387648 , 0.03472407,\n",
       "       0.0514809 , 0.05490259, 0.04915803, 0.03878867, 0.01789977,\n",
       "       0.04601246, 0.04451512, 0.04584667, 0.04884089, 0.03854724,\n",
       "       0.04955594, 0.0526979 , 0.05040755, 0.04639824, 0.04437584,\n",
       "       0.04295623, 0.03993121, 0.04750422, 0.02933983, 0.02585732,\n",
       "       0.04410898, 0.03092979, 0.03141052, 0.03897015, 0.0482182 ,\n",
       "       0.02901143, 0.05422446, 0.03952684, 0.04564613, 0.04711892,\n",
       "       0.03233393, 0.04865127, 0.03627895, 0.02992235, 0.04804363,\n",
       "       0.0424225 , 0.05205099, 0.04085217, 0.02840899, 0.0529225 ,\n",
       "       0.04402075, 0.05336086, 0.04481269, 0.05014012, 0.04583066,\n",
       "       0.02093211, 0.04739963, 0.05219042, 0.06478318, 0.03670907,\n",
       "       0.04896019, 0.04752251, 0.04456022, 0.03795807, 0.05671144,\n",
       "       0.05001243, 0.0604811 , 0.05632909, 0.02849118, 0.03465679,\n",
       "       0.03920597, 0.05085422, 0.01072793, 0.04588363, 0.04768172,\n",
       "       0.05697957, 0.05190749, 0.04833633, 0.03543329, 0.04501884,\n",
       "       0.04620076, 0.05203689, 0.04144426, 0.04268379, 0.05118324,\n",
       "       0.0332527 , 0.04738205, 0.04793518, 0.04461449, 0.0531837 ,\n",
       "       0.04319623, 0.0275572 , 0.05102248, 0.02397395, 0.0504021 ,\n",
       "       0.05231318, 0.04537564, 0.0503415 , 0.04259925, 0.06160574,\n",
       "       0.0385827 , 0.03800093, 0.04660355, 0.05130472, 0.04923915,\n",
       "       0.03871639, 0.05621924, 0.05584785, 0.05582978, 0.02387996,\n",
       "       0.03707092, 0.04378351, 0.05194674, 0.0367941 , 0.05086438,\n",
       "       0.03688093, 0.05547107, 0.04079812, 0.03236021, 0.0596715 ,\n",
       "       0.05761537, 0.04471928, 0.03935561, 0.03797151, 0.00491344,\n",
       "       0.05747378, 0.05665964, 0.05117974, 0.03691301, 0.04684566,\n",
       "       0.05490552, 0.05408742, 0.03729419, 0.05286231, 0.0446961 ,\n",
       "       0.04492245, 0.05165253, 0.04999978, 0.03604245, 0.02893076,\n",
       "       0.04414347, 0.04159592, 0.05409823, 0.04844014, 0.04897505,\n",
       "       0.0358022 , 0.04549398, 0.04101447, 0.03454245, 0.04628202,\n",
       "       0.04905359, 0.04307108, 0.04865096, 0.05248322, 0.05162679,\n",
       "       0.03635557, 0.04834627, 0.02862114, 0.0514857 , 0.05604624,\n",
       "       0.00903052, 0.03630405, 0.04454258, 0.04693101, 0.04122026,\n",
       "       0.05529515, 0.04692445, 0.05449084, 0.05318404, 0.02596167,\n",
       "       0.01598353, 0.0339018 , 0.04020503, 0.05067425, 0.04427661,\n",
       "       0.04704639, 0.02912931, 0.04127528, 0.04623241, 0.03777985,\n",
       "       0.04046646, 0.06120367, 0.04187296, 0.04217079, 0.03200058,\n",
       "       0.04416937, 0.05507066, 0.03939224, 0.05223152, 0.04660848,\n",
       "       0.05053348, 0.03803681, 0.05856829, 0.0586145 , 0.04756217,\n",
       "       0.05281258, 0.02936116, 0.02512968, 0.05184307, 0.00583403,\n",
       "       0.04638182, 0.05496627, 0.05181607, 0.03382037, 0.036472  ,\n",
       "       0.03894766, 0.05291431, 0.05263909, 0.03871649, 0.02008855,\n",
       "       0.03742577, 0.03652504, 0.04624172, 0.05160008, 0.0394726 ,\n",
       "       0.04693228, 0.04296284, 0.04321641, 0.04667478, 0.04267451,\n",
       "       0.05683322, 0.02137484, 0.06009914, 0.0521871 , 0.05210885,\n",
       "       0.04924632, 0.03797345, 0.03081307, 0.04244936, 0.04084   ,\n",
       "       0.05117676, 0.04718108, 0.03371119, 0.05242704, 0.04160222,\n",
       "       0.03977337, 0.04113978, 0.04135996, 0.03801076, 0.05208359,\n",
       "       0.04333476, 0.0321356 , 0.04338376, 0.05315742, 0.04249507,\n",
       "       0.04115209, 0.04130749, 0.05070592, 0.04181172, 0.05216123,\n",
       "       0.0475699 , 0.04495125, 0.03563345, 0.0153156 , 0.03967575,\n",
       "       0.01959453, 0.04885145, 0.03480746, 0.04112178, 0.03917743,\n",
       "       0.04569783, 0.05321724, 0.0278904 , 0.03396115, 0.04288093,\n",
       "       0.05715845, 0.03533743, 0.0573675 , 0.03842473, 0.02291801,\n",
       "       0.02009462, 0.0510104 , 0.04701311, 0.05281296, 0.04570992,\n",
       "       0.0432368 , 0.04882639, 0.04182541, 0.03703927, 0.0589011 ,\n",
       "       0.03868565, 0.03374499, 0.04759512, 0.05734651, 0.04082036,\n",
       "       0.03661986, 0.03050522, 0.05243628, 0.00870461, 0.04794522,\n",
       "       0.0601099 , 0.04254165, 0.03879817, 0.03595383, 0.01887754,\n",
       "       0.03505214, 0.04169795, 0.04547094, 0.04132206, 0.04651923,\n",
       "       0.05282566, 0.03614138, 0.04683549, 0.04033929, 0.05108856,\n",
       "       0.05728945, 0.04551208, 0.03755203, 0.04467101, 0.05238394,\n",
       "       0.05030576, 0.05652277, 0.04902952, 0.01314064, 0.04551708,\n",
       "       0.04272278, 0.03208169, 0.03773933, 0.04890849, 0.03336762,\n",
       "       0.03956208, 0.04038452, 0.04795751, 0.05864462, 0.04052725,\n",
       "       0.04799246, 0.02506623, 0.03901212, 0.01674858, 0.03531852,\n",
       "       0.04831307, 0.03027716, 0.04542707, 0.03212763, 0.04412797,\n",
       "       0.03838617, 0.05047495, 0.02597453, 0.04377535, 0.05311875,\n",
       "       0.04770272, 0.01813829, 0.05366884, 0.05089856, 0.04999666,\n",
       "       0.03692191, 0.04125882, 0.03399874, 0.04823855, 0.04180595,\n",
       "       0.01384645, 0.03008925, 0.06773498, 0.03648056, 0.02900701,\n",
       "       0.04743493, 0.04753299, 0.04616208, 0.05190375, 0.05501391,\n",
       "       0.0377571 , 0.03310622, 0.04373847, 0.05625969, 0.05263944,\n",
       "       0.04349661, 0.03904655, 0.05204038, 0.03623304, 0.04264774,\n",
       "       0.02727477, 0.04721693, 0.04046593, 0.05237054, 0.06261152,\n",
       "       0.03732206, 0.02791207, 0.06140615, 0.04162817, 0.05455867,\n",
       "       0.04465888, 0.04566441, 0.03714058, 0.04608715, 0.02182602,\n",
       "       0.05516002, 0.0354646 , 0.05267248, 0.04224135, 0.05110759,\n",
       "       0.03995465, 0.06226022, 0.05897995, 0.03383978, 0.04448675,\n",
       "       0.04931883, 0.04723144, 0.01562987, 0.05191312, 0.01761934,\n",
       "       0.04543916, 0.0254223 , 0.05054941, 0.03089499, 0.03907562,\n",
       "       0.04634814, 0.05098956, 0.04463116, 0.04619052, 0.04918528,\n",
       "       0.0441306 , 0.05276152, 0.0631049 , 0.04874514, 0.00470982,\n",
       "       0.03722102, 0.05213908, 0.03612228, 0.03219277, 0.04747765,\n",
       "       0.05438197, 0.0345904 , 0.04763506, 0.04215852, 0.02503304,\n",
       "       0.05062003, 0.04353555, 0.04449359, 0.04028874, 0.05056358,\n",
       "       0.04002769, 0.04242025, 0.04889746, 0.05962991, 0.05163167,\n",
       "       0.04285509, 0.03871188, 0.05027641, 0.05616127, 0.04119465,\n",
       "       0.04595536, 0.04234467, 0.05207283, 0.0445023 , 0.01923697,\n",
       "       0.04703914, 0.04971581, 0.02650872, 0.05229105, 0.03547359,\n",
       "       0.03865658, 0.03312261, 0.04616978, 0.06255078, 0.04761579,\n",
       "       0.03512173, 0.05373797, 0.04241197, 0.03878929, 0.04625438,\n",
       "       0.0461191 , 0.04417004, 0.05762719, 0.04221475, 0.03239971,\n",
       "       0.03499205, 0.0359676 , 0.0224487 , 0.04781169, 0.04289159,\n",
       "       0.04509062, 0.04844044, 0.04427891, 0.03372817, 0.04161375,\n",
       "       0.0518408 , 0.05131002, 0.01376301, 0.05445385, 0.05453737,\n",
       "       0.0318143 , 0.03955467, 0.0496581 , 0.03642612, 0.03510289,\n",
       "       0.03385714, 0.0566457 , 0.04413895, 0.05621924, 0.04471542,\n",
       "       0.03270414, 0.03613307, 0.0488335 , 0.03770294, 0.03776751,\n",
       "       0.03085239, 0.01106474, 0.04356145, 0.01224084, 0.04480129,\n",
       "       0.02657368, 0.05087915, 0.05762611, 0.02196817, 0.03343868,\n",
       "       0.00982634, 0.04802483, 0.04446217, 0.04408449, 0.05916734,\n",
       "       0.04707127, 0.02250707, 0.01567128, 0.03592784, 0.03627275],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_pred=np.mean(pred,axis=0)\n",
    "mc_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "save_obj(mc_pred, \"../pred_upd_loss_MC_Xx.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
